% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  11pt,
  man,floatsintext]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
\usepackage{fontspec}

%% Pandoc can only set 10, 11, 12 pt
%% uncomment below to set fontsize
%\usepackage[fontsize=13pt]{scrextend}

%\setmainfont{Calibri} % Set main font for latin characters

\setlength{\parskip}{0.15cm} % Set space between paragraphs
%\linespread{1.2}\selectfont % Set line height
%\usepackage[doublespacing]{setspace} % Use double space without changing footnotes line height

%% Special font for IPA
%% Make sure "Doulos SIL" is installed on your computer
%% For other typefaces supporting IPA symbols, see
%% https://en.wikipedia.org/wiki/International_Phonetic_Alphabet#Typefaces
\newfontfamily\ipa{Doulos SIL} % Font for IPA symbols
\DeclareTextFontCommand{\ipatext}{\ipa}



%%%         Section for CJK Characters                   %%%
%%%   You may want to uncomment the code below if        %%%
%%%   you're writing this document with CJK characters   %%%

%\usepackage{xeCJK}  % Uncomment for using CJK characters
%% Set main font for CJK characters
%% Make sure your system has the font set
%\setCJKmainfont[
%	BoldFont={HanWangHeiHeavy}  % Set font for CJK boldface
%    ]{標楷體}    % Set font for normal CJK
%% Some Traditional Chinese fonts: AR PL KaitiM Big5, PingFang TC, Noto Sans CJK TC
%\XeTeXlinebreaklocale "zh"
%\XeTeXlinebreakskip = 0pt plus 1pt

%% Added by Florian to make biblatex and its \refsection work since pandoc does not parse 
%% content within latex environments. \refsection in turn is required to allow authors
%% to have multiple separate bibliographies (here: one for main text and one for SI).
%%
%% For some reasons it does seem to be necessary to reload the bibpackage here since
%% ---at least for my system---adding the following to the YAML header instead does
%% not seem to work:
%%
%%    biblio-style: apa
%%    biblatexoptions: [backend=biber,maxbibnames=999,style=apa]
%% 
%% curiously removing the following from the YAML header:
%%
%%     citation_package: biblatex
%%
%% makes knitr (or pandoc?) revert back to the default bibtex treatment, which does 
%% not allow multiple bibliographies.
\usepackage[backend=biber,maxbibnames=999,style=apa]{biblatex}
\addbibresource{latex-stuff/library.bib}
\newcommand{\brefsection}{\begin{refsection}}
\newcommand{\erefsection}{\end{refsection}}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{speech perception; computational model; accent adaptation; perceptual recalibration}
\usepackage{lineno}

\linenumbers
\usepackage{csquotes}
\usepackage{animate}
\usepackage{amsmath}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{soul}
\usepackage{tabto}
\usepackage{xcolor}
\usepackage{placeins}
\setstcolor{red}
\usepackage{sectsty}
\sectionfont{\color{black}}
\subsectionfont{\color{black}}
\subsubsectionfont{\color{black}}
\usepackage{setspace}\doublespacing
\usepackage{subfig}
\usepackage{hyperref}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{biblatex}
\addbibresource{latex-stuff/library.bib}
\addbibresource{latex-stuff/r-references.bib}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={What we do (not) know about the mechanisms underlying adaptive speech perception: A computational framework and review},
  pdfauthor={Xin Xie1,2, T. Florian Jaeger2,3, \& Chigusa Kurumada2},
  pdflang={en-EN},
  pdfkeywords={speech perception; computational model; accent adaptation; perceptual recalibration},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{What we do (not) know about the mechanisms underlying adaptive speech perception: A computational framework and review}
\author{Xin Xie\textsuperscript{1,2}, T. Florian Jaeger\textsuperscript{2,3}, \& Chigusa Kurumada\textsuperscript{2}}
\date{April 07, 2023}


\shorttitle{Exposure effects in speech perception}

\authornote{

We are grateful to \#\#\# ommitted for review \#\#\#

Correspondence concerning this article should be addressed to Xin Xie, 3151 Social Science Plaza B, University of California, Irvine CA 92697--5100. E-mail: \href{mailto:xxie14@uci.edu}{\nolinkurl{xxie14@uci.edu}}

}

\affiliation{\vspace{0.5cm}\textsuperscript{1} Language Science, University of California, Irvine\\\textsuperscript{2} Brain and Cognitive Sciences, University of Rochester\\\textsuperscript{3} Computer Science, University of Rochester}

\abstract{%
Speech from unfamiliar talkers can be difficult to comprehend initially. These difficulties tend to dissipate with exposure, sometimes within minutes or less. Adaptivity in response to unfamiliar input is now considered a fundamental property of speech perception, and research over the past two decades has made substantial progress in identifying its characteristics. The \emph{mechanisms} underlying adaptive speech perception, however, remain unknown. Past work has attributed facilitatory effects of exposure to any one of three qualitatively different hypothesized mechanisms: (1) low-level, pre-linguistic, signal normalization, (2) changes in/selection of linguistic representations, or (3) changes in post-perceptual decision-making. Direct comparisons of these hypotheses, or combinations thereof, have been lacking. We describe a general computational framework that---for the first time---implements all three mechanisms. We demonstrate how the framework can be used to derive predictions for experiments on perception from the acoustic properties of the stimuli. Using this approach, we find that---at the level of data analysis presently employed by most studies in the field---the signature results of common experimental paradigms do not distinguish between the three mechanisms. This highlights the need for a change in research practices, so that future experiments provide more informative results. We recommend specific changes to experimental paradigms and data analysis. All data and code for this study are shared via OSF, including the R markdown document that this article is generated from, and an R library that implements the models we present.
}



\begin{document}
\maketitle

\setcounter{secnumdepth}{5}

\brefsection

\setcounter{page}{1}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

How human listeners are able to infer meaning from speech is a central question in cognitive and neurosciences. The computational complexity of spoken language understanding becomes most apparent when a talker's pronunciations---and thus the mapping of acoustic input to linguistic categories and meaning---strongly deviate from listeners' expectations. This might occur, for example, when listening to a talker with an unfamiliar regional or non-native accent, or a patient with apraxia or dysarthria. The same fundamental challenge is, however, present even during seemingly effortless comprehension. Even among talkers who share similar language backgrounds, the mapping between the acoustic input and linguistic categories can vary substantially between talkers due to both physiology (e.g., vocal tract size and shape) and socio-cultural factors (e.g., social identity and language background). As a consequence, one talker's pronunciation of, say, the sound category {[}s{]} (as in \emph{sip}) can be acoustically more similar to another talker's production of \([\ipatext{ʃ}]\) \autocite[as in \emph{ship},][]{newman2001}. How we manage to understand each other despite such cross-talker differences has remained one of the perennial puzzles in research on speech perception \autocite[where it is known as part of the infamous \emph{lack of invariance} problem,][]{liberman1967}.

This article presents a general computational framework---which we refer to as \emph{ASP} for \emph{adaptive speech perception}---that helps address this question. ASP grew out of our long-term goal to contribute to the development of stronger theories, affording decisive comparisons between alternative hypotheses about the mechanisms underlying speech perception \autocite[in the tradition of ``strong inference'' approaches to scientific inquiry,][]{platt1964}. We identify the three most influential hypotheses about the mechanisms that allow listeners to overcome cross-talker variability (Figure \ref{fig:overview}). These three hypotheses---described below---entail fundamentally different cognitive abilities and neural architectures, with far-reaching consequences for theories of speech perception, linguistics, and the malleability of neural representations more generally. ASP is the first to formalize and implement all three of these mechanisms in a common computational framework. This responds to a need identified in recent reviews to better characterize the mechanisms of adaptive speech perception \autocites[e.g.,][]{baeseberk2020,kurumada-roettger2021,johnson-sjerps2021,quam-creel2021,stilp2020,weatherholtz-jaeger2016}.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\columnwidth]{../figures/diagrams/overview-of-three-mechanisms.png}
\caption{Listeners' recognition of speech categories are typically assumed to involve at least three types of mechanisms: 1) the acoustic input is transformed by low-level normalization processes into the perceptual cues that form the input to categorization, 2) category representations describe the mapping between these perceptual and linguistic categories (here $C_1$ and $C_2$), and 3) decision-making mechanisms allow additional, stimulus-independent, biases to affect recognition. Any of these three mechanisms can in theory be affected by recent experience. It is unknown which (combination) of the three mechanisms underlie these changes and how the engagement of different mechanisms depends on, for instance, stimulus properties, task demands, and individual differences between listeners.}\label{fig:overview}
\end{center}
\end{figure}

While the three hypotheses in Figure \ref{fig:overview} have largely been pursued in separate lines of research, they are not mutually exclusive. This raises questions as to whether adaptive speech perception is the result of \emph{combinations} of these mechanisms, and (if so) how the relative engagement of these mechanisms depends on, for instance, stimulus properties, task demands, or individual differences between listeners. ASP can be used to address these questions. Researchers can, for example, use ASP to predict adaptive changes in speech perception while `switching off' individual change mechanisms---making it possible to test whether any of the three mechanisms is \emph{sufficient} to explain a given result---or to predict changes from weighted combinations of all three mechanisms. Future work could use ASP, for example, to investigate whether specific instances of impaired adaptation \autocites[as seen, e.g., in children with dyslexia,][]{gabay2021,ozernov-palchik2021} arise from auditory, linguistic, or cognitive sources---an important prerequisite for devising more effective interventions/treatments.

For the present article, we have three more immediate goals. The first part of this article introduces the ASP framework, along with visual demonstrations and animations. We take a deliberately tutorial-like approach so to help researchers apply ASP to their own experiments. All data and code for this article can be downloaded from the Open Science Framework at \url{https://osf.io/q7gjp/}. The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software \autocites{R}[see supplementary information for detailed software requirements]{RStudio}. Readers can also revisit any of the assumptions we make---e.g., changing parameterizations of our models, or substituting alternative models \autocite[see also][]{tan2021}. We welcome questions about the code, including the accompanying \texttt{R} library.

Second, we demonstrate one way in which computational frameworks like ASP can advance theories of speech perception. We simulate human recognition for two influential experimental paradigms under each of the three hypothesized mechanisms. The first paradigm---\emph{perceptual recalibration} \autocites[e.g.,][]{kraljic-samuel2006,reinisch-holt2013,samuel2016,vroomen-baart2009}---tends to employ a small set of exposure and test stimuli that are phonetically manipulated to form a single perceptual continuum. This property makes such experiments comparatively easy to understand and model, and thus suitable as an initial case study. The second paradigm---\emph{adaptation to second language (L2) accents} \autocites[e.g.,][]{bradlow-bent2008,hernandez2019,tzeng2016,sidaras2009,xie2016jep,zheng-samuel2020}---introduces a much greater degree of complexity for both the listener and the researcher. Stimuli for these experiments tend to exhibit the full range of naturally occurring phonetic variability, introducing listeners to novel phonetic cues, differences in cue weightings, and other complex changes in the mapping from acoustics to linguistic categories.

We find that, contrary to common assumptions, the signature findings of both paradigms are qualitatively compatible with \emph{any} of the three hypothesized change mechanisms. Beyond the immediate consequences for research on perceptual recalibration and accent adaptation, this demonstrates how ASP can inform and revise intuitions about the mechanisms underlying adaptive speech perception. The R code for our case studies further serves as a template for experimenters interested in understanding the theoretical implications of their results while reducing the need for ad-hoc reasoning.

Finally, we discuss how computationally-guided behavioral and neuroimaging research can advance future work. We show that the empirical indeterminacy of many existing findings is due to the level of analysis chosen in those studies. We propose new standards of experimental design and data analysis that will more effectively distinguish between competing hypotheses about the relative engagement of different mechanisms. But first some background.

\hypertarget{the-state-of-the-fields}{%
\subsection{The state of the field(s)}\label{the-state-of-the-fields}}

Research over the past decades has identified adaptive changes in speech perception to be a key component in listeners' ability to overcome cross-talker differences. Although speech perception can initially be slower and/or less accurate when listeners encounter an unfamiliar talker with unexpected pronunciations, these processing difficulties tend to reduce with exposure \autocites[e.g.,][]{bradlow-bent2008,nygaard1994,Perrachione2016,sidaras2009,wade2007,weil2001a,xie2021jep}. Remarkably, substantial improvements---such as reduced processing times or increased accuracy of recognition---can occur within minutes or less \autocite{clarke-garrett2004,munro-derwing1995,xie2018jasa}. Even a context as brief as a single utterance can be sufficient to change perception and segmentation of ambiguous speech, blurring the distinction between processing and adaptation \autocites*[e.g., categorizing a Dutch ambiguous /m?t/ embedded in an utterance with fast or slow speech as either ``mat'' or ``maat'' Bosker et al.,][]{bosker2017}[see also][]{kluender1988,newman-sawusch1996,sawusch-newman2000,sjerps2011}. Yet, adaptive changes after only a few words of exposure can sometimes last for days, if not longer \autocites{eisner-mcqueen2006,samuel2021,xie2016}[see also][]{goldinger1996}.

In short, listeners' ability to adapt based on recent input is now considered a central part of human speech perception, and substantial progress has been made in identifying its empirical properties and constraints \autocites[for comprehensive reviews, see][]{baeseberk2020,johnson-sjerps2021,tzeng2021,quam-creel2021,stilp2020}. Despite these substantial advances, however, \textbf{it remains unclear \emph{how}---through what mechanisms---recent exposure comes to facilitate spoken language understanding.} This is not due to a lack of theoretical proposals. Across cognitive sciences and neurosciences, there are now dozens of competing perspectives. However, contrastive comparisons between proposals have largely been lacking. Two inter-related factors seem to contribute to this: (1) many proposals---including some of our own past work---remain under-specified, constituting informal hypotheses rather than theories or models that lend themselves to strong contrastive tests; (2) there is a tendency, particularly in behavioral research, to focus on characterizing properties of adaptive speech perception rather than on identifying the underlying cognitive and neural architectures \autocites[but see][]{apfelbaum-mcmurray2015,chodroff-wilson2020,hoffmanbion-escudero2007,kleinschmidt-jaeger2015,kiefte-nearey2019,lehet-holt2020,mcmurray-jongman2011,xie2021cognition}. As a consequence, different hypotheses are often \emph{assumed} rather than tested, with different lines of research making different (implicit) assumptions, sometimes co-existing for decades without targeted attempts to contrast the predictions that would follow from those assumptions.

Neuroimaging research has, on the whole, been more invested into contrastive comparisons \autocites[e.g.,][]{bonte2017,erb2013brain,guediche2015evidence,myers-mesite2014}: questions about the involvement of different types of information processing---operationalized as differential activation of brain areas and networks that are associated with different functionality---are the bread and butter of neuroimaging. This approach has identified a wide range of brain regions as involved in different aspects of adaptive speech perception, ranging from subcortical areas \autocites[e.g.,][]{skoe2021auditory,guediche2015evidence} to networks associated with post-perceptual decision-making \autocites[e.g.,][]{myers-mesite2014,erb2013brain}. This leaves open, however, what types of computations underlie the observed differential activations, or why different types of exposure lead to different types of behavioral changes. In light of these gaps in our understanding, recent reviews continue to emphasize the need to better characterize the nature of the mechanisms underlying adaptive speech perception \autocites[from, e.g.,][]{samuel-kraljic2009}[to][]{weatherholtz-jaeger2016}[to][]{baeseberk2020}. This includes questions about how the task demands of different types of experimental paradigms affect the engagement of different mechanisms \autocite{baeseberk2018,zheng-samuel2020}, and thus also which paradigms are most likely to shed light on the mechanisms affording flexible perception in everyday life.

The ASP framework we present in Section \ref{sec:framework} is designed to help address these questions. We group existing proposals for adaptive speech perception into three types of theoretical perspectives (Figure \ref{fig:overview}). The three proposals share with each other, and with all major theories in speech perception, general assumptions about how the acoustic input supports perception of a speech category. The process begins with (A) the extraction and normalization of acoustic/phonetic cues. These cues are (B) mapped onto, and activate, linguistic categories (such as phonemes, syllables, and/or words). Finally, (C) decision processes integrate the resulting category activations with contextual support (e.g., from lexical or sentential context) and/or meta-reasoning (e.g., about the task), and recognition takes place. Existing proposals differ, however, in which of (A)-(C) explains \emph{changes} in speech perception as a function of recent exposure. As we discuss next, it is these differences that entail fundamentally different cognitive and neural architectures.

The majority of recent behavioral research on talker adaptation has focused on the middle layer, \textbf{changes in the mapping from acoustic or phonetic cues onto phonological categories (for brevity: changes in category representations)}. This includes proposals that attribute exposure effects to ``boundary re-tuning/shift'' \autocites[e.g.,][]{norris2003,reinisch2013}, ``perceptual/category recalibration'' \autocites[e.g.,][]{kraljic-samuel2006,reinisch-holt2013,samuel2016,vroomen-baart2009}, ``perceptual retuning'' \autocite{jesse-mcqueen2011,mcqueen2006,mitterer2013}, ``category shift'' \autocite{lindsay2022,sawusch-pisoni1976}; ``category expansion'' \autocite{schmale2012}, or ``dimension-based statistical learning'' \autocite{idemaru-holt2011,lehet-holt2020,liu-holt2015}. While these proposals are often not further formally specified or modeled \autocites[for notable exceptions, see][]{apfelbaum-mcmurray2015,clayards2008,hitczenko-feldman2016,kleinschmidt-jaeger2015,lancia-winter2013,xie2021cognition}, they all describe types of changes in category representations. For example, ``category shift'' refers to a change in the mean of the cue distribution corresponding to a category, and ``category expansion'' refers to increases in the variance of that distribution.

One reason that the idea of representational changes has received so much attention is that it is of high theoretical relevance. Changes in category representations as a function of recent exposure are predicted by influential exemplar \autocite{apfelbaum-mcmurray2015,johnson2006}, episodic \autocite{goldinger1998}, Bayesian inference \autocite{kleinschmidt-jaeger2015}, and neural network models \autocite{lancia-winter2013}. All of these theories predict that listeners can learn and store talker- or accent-specific representations \autocites[see also][]{bradlow-bent2008,baeseberk2013,tzeng2016}. Perhaps unsurprisingly, this idea---continued implicit learning of new representations throughout their adult life---has been influential beyond speech perception. The possibility that listeners learn and maintain category representations for multiple types of talkers has influenced psycholinguistic research on lexical, sentence, and semantic/pragmatic processing \autocites{chang2012,fine2013,kaschak2004,pogue2016,prasad2021,ryskin2019,schuster-degen2020}[ for review, see][]{brown-schmidt2015} and research on second language learning \autocites[for reviews, see][]{kaan-chun2018,pajak2016}. It has shaped linguistic theories \autocites[e.g.,][]{bybee2001,goldinger-azuma2004,hay2019,magnuson-nusbaum2007,pierrehumbert2001} as well as theories about the interface between social and linguistic cognition \autocites[e.g.,][]{babel2019,creel-bregman2011,foulkes-hay2015,hanulikova2012,sumner2014}. Further illustrating the influence of this idea, several recent reviews have gone as far as to discuss \emph{what types} of representational changes underlie the effects of recent exposure (e.g., ``category expansion'' vs.~``category shifts''), rather than \emph{whether} representational changes are indeed necessary to explain adaptive speech perception \autocites[e.g.,][]{baeseberk2020,bent-baeseberk2021,schertz-clare2020}.

In the absence of contrastive tests, however, the same behavioral results that have been interpreted as arguing for representational changes could in principle be explained by mechanisms that are both computationally and representationally more parsimonious than changes in category representations.\footnote{We use the term \emph{computational parsimony} to refer to the number of parameters that listeners and/or researchers need to estimate (the degrees of freedom of a model; indicated as DF in Figure \ref{fig:overview}), and \emph{representational parsimony} to refer to the amount of information listeners need to keep in memory (for details, see Section \ref{sec:framework}). We submit that our claims about the relative parsimony of the different mechanisms hold regardless of the specific models chosen to implement the mechanisms, \emph{as long as one compares like with like}. For example, in some parts of the field, talker-specificity of adaptation is taken to implicate changes in representations---so strongly, in fact, that one reviewer initially assumed evidence of talker-specificity to necessarily be evidence against alternative mechanisms. However, it is equally possible for talker-specificity to be due any of the other adaptive mechanisms. Indeed, at least for normalization, talker-specificity has been proposed \autocite{barreda2012,magnuson2021talker}. Vice versa, changes in category representations do not \emph{have} to be talker-specific {[}e.g., learning of category-specific statistics over a moving time window; see discussion in \textcite{kleinschmidt2015}, p.~173{]}. Once such implicit association between mechanism and hypothesized properties of the mechanism (e.g., talker-specificity, longevity, generalizability, etc.) are separated, changes in category representations are the least parsimonious of the three mechanisms considered here.} For instance, proposals dating back to at least the 1970s hold that adaptive speech perception is due to low-level, automatic (involuntary) \textbf{normalization} during the early stages of auditory processing (bottom of Figure \ref{fig:overview}). These processes are thought to be pre-linguistic in that they do not refer to mapping from cues to categories but rather refer to the \emph{overall} distribution of acoustic or phonetic cues, regardless of the category \autocites[for reviews,][]{johnson-sjerps2021,stilp2020}. Compared to theories of representational changes, this makes weaker assumptions about listeners' ability to learn talker-specific information: listeners only need to learn, e.g., the overall mean of a phonetic cue, rather than the category-specific cue means, for a given talker. Evidence from more recent neuroimaging studies has lent support for low-level normalization mechanisms by showing engagement of subcortical structures in sensory adaptation \autocites[e.g., the brain stem,][]{skoe2021auditory}[and cerebellum,][]{guediche2015evidence}[for review, see][]{guediche2014}. Studies using intracranial electrocorticography have found evidence of talker-normalized (spectral) cues in the auditory cortex before they are mapped onto categories \autocites[e.g.,][]{sjerps2019,Tang2017}. This leaves open, however, whether normalization can indeed explain talker-specific adaptation that has been attributed to changes in representations. If it can, this would remove the necessity for the computational complexity and cognitive abilities implied by theories of representational changes.\footnote{It would also indicate a need to revisit the interpretation of findings that speech perception can be affected by the (inferred) social identity of a talker \autocites[e.g., regional origin,][]{hay-drager2010,niedzielski1999}[sex,][]{johnson1999,strand1999}[age,][]{skoogwaller2015,walker-hay2011}[and individual identity,][]{nygaard1994,remez2018}. In sociophonetics and related fields, these findings seem to be routinely attributed to talker-specific storage of \emph{category representations}, without considering more parsimonious explanations in terms of talker-specific storage of \emph{marginal cue distributions} (normalization). While early normalization accounts that were limited to correction for physiology were rather convincingly rejected by cross-linguistic comparisons \autocite{johnson2006}, modern theories of normalization \autocites[see also][]{magnuson-nusbaum2007,pisoni1997some} like the ones we discuss in Section \ref{sec:framework} are not subject to those arguments.}

Another explanation for adaptive speech perception that does not refer to changes in representations are post-perceptual---`cognitive'--- mechanisms of decision-making (top of Figure \ref{fig:overview}). Just like normalization, this explanation is computationally and representationally more parsimonious than changes in representations. And just like normalization is broadly accepted to be part of speech perception, there is little doubt that \textbf{changes in decision and response biases} can affect listeners' interpretation of speech input, or at least the responses they give within experiments. For instance, Clarke-Davidson et al. \autocite*[p.605]{clarkedavidson2008} define a response bias as ``the increased likelihood to give a particular response---such as /s/---given any acoustic input, or the need for less evidence for a particular response'' that ``would help participants make faster word decisions in {[}\ldots{]} ambiguous cases''. In behavioral research, such biases continue to be rarely considered when interpreting the effects of recent exposure. \emph{When} response biases have been considered \autocite*[e.g., as ``response equilibration'' in Vroomen and Baart][]{vroomen-baart2009}, this explanation tends to be dismissed. One reason for this might be that particular result patterns---such as boundary shifts in perceptual recalibration or improved categorization or transcription accuracy in accent adaptation---are taken to rule out changes in response biases (or normalization, for that matter). However, to anticipate one take-home point from the case studies we present below, we find that response biases can explain more complex changes in behavior than has previously been assumed, including the very signature results that are often assumed to be incompatible with changes in response bias.

In contrast to behavioral research, changes in decision-making have received substantial attention in neuroimaging research. Recent studies have investigated whether adaptive changes in speech perception primarily recruit areas associated with decision-making \autocite{erb2013brain,myers-mesite2014}---such as prefrontal areas \autocites[e.g.,][]{binder2004neural,thompson1997role} as well as the insula and parietal cortex \autocites[e.g.,][]{furl2011parietal,keuken2014}---as opposed to cortical areas associated with phonetic representations \autocite{sohoglu-davis2016,bonte2017,luthra2020a}. For instance, \textcite{myers-mesite2014} found that sensitivity to category shifts between {[}s{]} and \([\ipatext{ʃ}]\) emerged in right frontal and middle temporal regions, implicating adjustments of decision-related or attentional criteria downstream from functions performed by primary auditory cortex. A recent examination of the same data using multivoxel pattern analysis, however, suggests the activity of relatively early (temporoparietal) brain regions, potentially suggesting multiple mechanisms underlying talker adaptation and not just decision-level ones \autocite{luthra2020a}.

In summary, current results from both behavioral and neuroimaging studies are mixed. And comparisons between competing hypotheses about the types of computations that underlie adaptive speech perception---within or across paradigms---remain lacking. As a result, we often do not know whether the results of a given paradigm clearly support one mechanism over another, or whether the results of different paradigms all point to the same (combination of) mechanisms. The computational framework of adaptive speech perception (ASP for brevity) we present below provides a way for future work to address these questions.

\hypertarget{sec:framework}{%
\section{Modeling adaptive changes in speech perception}\label{sec:framework}}

We assume conceptual familiarity with reasoning about distributions and contemporary theories of speech perception---all of which expect that listeners have implicit representations of the mapping between phonetic cues and linguistic categories that encompass (in one form or other) knowledge of the distribution of cues that correspond to a linguistic category. This assumption is motivated by observations made as early as \textcite{abramson1973voice} or \textcite{nearey-hogan1986}, and illustrated in Figure \ref{fig:demonstrate-production-perception-link}: listeners' categorization responses for two phonetic categories (/d/ and /t/ in Figure \ref{fig:demonstrate-production-perception-link}B) can be predicted by considering the phonetic properties of the input \emph{relative to the distribution of phonetic cues} that listeners have previously experienced (Figure \ref{fig:demonstrate-production-perception-link}A). Recent introductions to, and discussions of, this perspective are provided in \textcite{bent-baeseberk2021}, \textcite{kurumada-roettger2021}, \textcite{quam-creel2021}, and \textcite{schertz-clare2020}.



\begin{figure}

{\centering \includegraphics{../figures/knitted/demonstrate-production-perception-link-1} 

}

\caption{Speech production realizes phonetic categories as distributions in a multi-dimensional acoustic or phonetic cue space. Implicit knowledge of these distributions is known to mediate listeners' interpretation of speech inputs \autocite[for review, see][]{schertz-clare2020}. \textbf{Panel A:} voice onset time (VOT) and fundamental frequency (f0) of 6,518 word-initial /d/ and /t/ from 50 male L1 speakers of US English \autocite[data from][]{chodroff-wilson2018}. VOT and f0 were centered relative to their overall mean (see SI \ref{sec:SI-chodroff}). \textbf{Panel B:} responses to a 2AFC (``da'' vs.~``ta'') categorization task by 10 L1 listeners of US English, depending on VOT and f0 \autocite[data from Experiment 1 in][]{burchill-jaeger2022}.}\label{fig:demonstrate-production-perception-link}
\end{figure}

ASP is not meant to be a new model of speech perception but rather a conceptual and computational framework that i) integrates the core insights shared by present-day models of speech perception, ii) extends these models to specify how recent exposure can come to affect any of the three mechanistic levels, and iii) stays as simple and conceptually transparent as possible. Any of the components we present below can be exchanged and compared to alternatives: researchers might, for example, substitute a different approach to normalization or use exemplar models of category representations rather than the Bayesian model we employ in this study.

For the present purpose, we focus on modeling offline responses, which remain one of the most commonly employed measures in research on speech perception. This includes categorization/identification responses in two-forced-choice (2AFC) tasks that continue to be the standard approach used to assess the effects of recent exposure on subsequent speech perception \autocites[e.g.,][]{norris2003,kraljic-samuel2006,vroomen2007,reinisch-holt2013,drouin2016,liu-jaeger2018,zheng-samuel2020,clayards2008,maye2008,kleinschmidt2015,xie2017}. The general framework we develop here can, however, be extended to offline and online tasks with two or more categorical outcomes, including discrimination \autocites[e.g.,][]{feldman2009,richter2017}, transcription \autocite[e.g.,][]{bradlow-bent2008}, accuracy in cross-modal priming \autocites[e.g.,][]{eisner2013,sjerps-mcqueen2010}, spoken repetition \autocite[e.g.,][]{bieber2021}, or fixations in the visual world paradigm \autocites[e.g.,][]{hanulikova-weber2012,nixon2016}.

Figure \ref{fig:overview-change} provides an overview of how ASP can be used to model adaptive changes in speech perception. This is the approach we employ in the two case studies in Sections \ref{sec:PR} and \ref{sec:AA}. ASP integrates a \emph{processing model} with three different \emph{change models}---one each to describe changes in normalization, representations, and decision-making, respectively. The processing model---for the present study, a \emph{categorization model}---describes the mechanisms that allow listeners to infer `\emph{what} is being said'.\footnote{The present study focuses on the recognition of phonetic categories---be it a phoneme, syllable, or word---\emph{out of context}. We do not aim to model the well-documented effects of, e.g., phonotactic, prosodic, lexical/semantic, visual and sentential/discourse contexts \autocite[for a great concise review, see][]{winn2018}. The results we present below are, however, expected to generalize to situations in which such context is taken into account (in ASP, context simply changes the overall response bias). Critically, models of spoken word recognition that capture contextual effects share the general assumptions encoded in ASP's processing model \autocites[DIANA,][]{bosch2015}[NAM,][]{luce-pisoni1998}[TRACE,][]{mcclelland-elman1986}[EARSHOT,][]{magnuson2020}. Additionally, we note that the current form of ASP focuses on the outcome of computation and hence is NOT a processing model. Although we emphasize that the three mechanisms are not meant to be discrete, information-encapsulated processes, ASP remains agnostic as to whether and how the three mechanisms may interact with, or provide feedback to, each other during processing.} This model is closely related to psychometric models that are used for data analysis in many areas of the cognitive sciences \autocite[for an introduction, see][]{wichmann-hill2001} but remain rare in research on speech perception---in particular, in research on the effects of recent experience \autocites[but see][]{clayards2008,kleinschmidt-jaeger2016cogsci}. It is also conceptually related to general process models of online perceptual decision-making \autocite[e.g., the drift diffusion model,][]{ratcliff2011}. The change models describe listeners' inferences about `how things are being said'. These change models extend existing models for normalization \autocite[adding \emph{inference} of talker's means to C-CuRE,][]{mcmurray-jongman2011} and representational changes \autocite[extending ideal adaptors to multivariate acoustic inputs,][]{kleinschmidt-jaeger2015}, and add a novel model for changes in decision-making.

\begin{figure}[h]
\begin{center}
\includegraphics[width=.99\columnwidth]{../figures/diagrams/overview-of-changes.png}
\caption{Overview of the approach for the present study. Experiments on the effect of recent exposure on subsequent speech perception tend to involve two phases. An exposure phase manipulates the statistics of the speech input, typically between participants. A test phase---typically identical across participants---assesses the effects of those manipulations on the subsequent interpretation of speech input. We seek to understand what type of exposure effects each of the three mechanisms in Figure \ref{fig:overview} can explain. To this end, we specify both a) a {\em processing model}---specifically, a {\em categorization model}---that describes the interpretation of speech input at any given moment (vertical information flow) and b) {\em change models} for all three mechanisms that describe how these parts of the categorization model change as a function of exposure (horizontal information flow). We then use the different change models to compare the predicted consequences of changes to normalization, representations, or response biases.}\label{fig:overview-change}
\end{center}
\end{figure}

\hypertarget{the-categorization-model-processing}{%
\subsection{The categorization model (`processing')}\label{the-categorization-model-processing}}

For the present purpose, we can think of the mapping from acoustic inputs to categorization responses as involving at least the three processes illustrated in Figure \ref{fig:model-perceptual-decision-making} (by presenting these processes as three different steps, we do not mean to imply that they are discrete, information encapsulated processes).

\begin{figure}[h]
\begin{center}
\includegraphics[width=.9\columnwidth]{../figures/diagrams/graphical-model.png}
  \caption{Graphical model of a simplified general {\em categorization model} for $J$-AFC alternative-choice tasks (e.g., vowel categorization, word transcription, etc.) over a $K$-dimensional phonetic input. The left-hand side links the components and parameters in the model to the three mechanism introduced in Figure \ref{fig:overview}. Filled gray circles represent variables the researcher can observe. Empty circles represent latent variables that are not observable. Diamonds represent variable-free processes. Only variables that we consider as being potentially affected by recent exposure are shown (see text for additional detail). Parentheses describe the number of degrees of freedom (DF) introduced by each parameter. In practice, many of these DFs can be fixed based on phonetic databases and/or previous perception experiments (see text for details). Squares are annotated with the distributions resulting at that level of the model: $\mathcal{N}$(ormal), Multi(nomial), and $\mathcal{M}$(ixture) distributions. For 2AFC tasks, all multinomial distributions simplify to Bin(omial) distributions.} \label{fig:model-perceptual-decision-making}
\end{center}
\end{figure}

\hypertarget{sec:normalization}{%
\subsubsection{`Pre-linguistic' auditory processes: signal transformations and normalization}\label{sec:normalization}}

The first step in our categorization model maps the acoustic input onto perceptual features. These perceptual features are the outcome of low-level signal transformations and normalization processes. Experiments on the perception of stimulus similarity suggest that the human brain represents frequency information logarithmically \autocite{greenwood1961}. This is captured by, for example, the Mel \autocite{stevens1937}, Bark \autocite{zwicker1961}, and ERB \autocite{moore-glasberg1983} transformations. Such logarithm-transformed spectral cues have been found to provide a better fit against human categorization responses than features based on raw frequencies \autocite{hoffmanbion-escudero2007,kleinschmidt2019,richter2017}. The case studies we present below model perception for a phonological contrast that depends on both temporal and spectral cues. For the spectral cue, we use Mel-transformed frequencies.

Many accounts of normalization assume further transformation of the acoustic input based either on other concurrently occurring acoustic properties (intrinsic normalization) or on the overall distribution of the acoustic input itself \autocite[extrinsice normalization, for review, see][]{weatherholtz-jaeger2016}. For example, some of the most influential normalization accounts for vowel formants either center \autocite{nearey1989} or both center and standardize formants based on the talker-specific mean and standard deviation \autocite{lobanov1971,gerstman1968}. These types of accounts have been found to remove a substantial amount of cross-talker variability in the realization of vowels \autocites[e.g.,][]{hoffmanbion-escudero2007,nearey1989,persson-jaeger2023}[for critical review, see][]{barreda2020}. In particular, accounts that assume simple centering seem to provide a good fit against vowel categorization by human listeners \autocites[e.g.,][]{barreda2021,persson-jaeger2022,richter2017}.

The case studies we present below employ a general model of normalization that can be applied to any type of phonological contrast \autocites[C-CuRE,][]{cole2010,mcmurray-jongman2011}. C-CuRE stands for ``computing cues relative to expectations'' and centers cues by subtracting the (expected) mean for that cue in the current context (\(\mu\) in Figure \ref{fig:model-perceptual-decision-making}). C-CuRE has been found effective in accounting for cross-talker differences as well as effects of surrounding phonological context, improving the ability to predict human responses compared to a model without any normalization \autocites{mcmurray-jongman2011}[see also][]{apfelbaum-mcmurray2015,crinnion2020,kleinschmidt2020,mcmurray-jongman2016,xie2021cognition}. Figure \ref{fig:demonstrate-normalization} visualizes the effects of C-CuRE normalization on the marginal distributions of f0 and VOT to word-initial stop voicing in US English (e.g., /b/ vs.~/p/ in \emph{bin} vs.~\emph{pin}). The specific procedure is described in the SI (\ref{sec:SI-chodroff}). We use these normalized data throughout the remainder of this study, including the two case studies. That is, all studies presented below assume that listeners' \emph{long-term} expectations are based on talker-normalized cues, while leaving open how normalization affects perception during the initial exposure to an unfamiliar talker. The qualitative findings of our studies do not, however, depend on this assumption.



\begin{figure}

{\centering \includegraphics{../figures/knitted/demonstrate-normalization-1} 

}

\caption{Effects of applying C-CuRE normalization to productions of word-initial stop voicing in US English (e.g., \emph{bin} vs.~\emph{pin}). \textbf{Panel A:} unnormalized VOT and f0 of 40,459 stop productions from \textcite{chodroff-wilson2018}. VOT is the primary cue to this voicing contrast for L1 US English, and f0 is known to be a secondary cue. The data exhibit clear evidence of multimodality along both VOT and f0. \textbf{Panel B:} the same productions but after C-CuRE normalization has been applied to remove talker-specific variability. Compared to unnormalized data, the C-CuRE normalized data exhibits reduced variability and reduced evidence of multimodality (the remaining low-f0 outliers in the Panel B are the result of creaky voice and measurement errors like pitch-halving, see SI \ref{sec:SI-chodroff}).}\label{fig:demonstrate-normalization}
\end{figure}

Beyond signal transformations and normalization, the stimulus observed by a listener is also affected by perceptual noise (not shown in Figure \ref{fig:demonstrate-normalization}). This means that even the same exact acoustic stimulus does not necessarily result in the same percept \autocites{leonard2016,schuerman2022,feldman2009}[p.~148]{nearey-hogan1986}. The integration of noise into models of speech perception has been shown to explain otherwise puzzling differences in the perception and recognition of different types of phonological contrasts \autocite[e.g.,][]{kronrod2016}. Although not critical for the present purpose, we incorporate perceptual noise into ASP also because it results in more human-like (less steep) categorization functions. This noise is held constant across all case studies presented below, set to the values obtained by Kronrod and colleagues \autocite*[\(\sigma^2_{noise, VOT}=80 msec^2\), \(\sigma^2_{noise, spectral}=878 Mel^2\)]{kronrod2016}.

\hypertarget{sec:representations}{%
\subsubsection{Category representations: from perceptual features to linguistic categories}\label{sec:representations}}

The output of normalization is the input to the mapping onto linguistic categories. All major theories of speech perception agree that this involves implicit knowledge of the distributional realization of linguistic categories---i.e., the distribution of acoustic or phonetic cues that are observed across instances of the category. In analytic models, this mapping is typically described by the \emph{category likelihood} that is learned from previous inputs \autocites[e.g., in the neighborhood activation model,][]{luce-pisoni1998}[shortlist B,][]{norris-mcqueen2008}[and other Bayesian inference models,][]{clayards2008,feldman2009,kleinschmidt-jaeger2015}. In exemplar models and related theories, the same mapping is achieved by storing previously experienced inputs as exemplars \autocites[e.g.,][]{johnson2006,pierrehumbert2001,wedel2006} or episodic traces \autocite{goldinger1996} that subsequent inputs are compared to during recognition \autocite[e.g., by means of \(k\)-nearest neighbor algorithms,][]{fix-hodges1989}. In connectionist and deep neural network models, the same probabilistic mapping is achieved through latent structure in the network that is learned from previous input \autocite{mcclelland-elman1986,magnuson2020}. Although the specific nature of these representations continues to be a matter of debate, all of these theories share the assumption that there is a probabilistic mapping between perceptual cues and linguistic categories \autocite[see also][ on the close computational relation between exemplar and Bayesian inference models]{shi2010} and that perception involves implicit knowledge of this probabilistic mapping.

Here we employ an analytic characterization of category likelihoods. Specifically, we adopt a simplifying assumption commonly made in research on speech perception \autocite{clayards2008,feldman2009,kleinschmidt-jaeger2015,norris-mcqueen2008} and automatic speech recognition \autocite{jurafsky-martin2000} that the cue distributions for each category follow a multivariate Gaussian distribution. In Figure \ref{fig:model-perceptual-decision-making}, this is indicated through the two parameters that are sufficient to specify each multivariate Gaussian (the category means \(\mu_c\) and the category covariance matrices \(\Sigma_c\)).\footnote{\label{fn:alternative-representational-changes} This assumption strikes a compromise between two common alternatives. A representationally less complex proposal introduces an independence assumption and describes linguistic categories as a combination of \emph{independent uni}variate Gaussians for each cue dimension. The likelihoods from the univariate Gaussians is then combined through a cue integration model. This substantially reduces the degrees of freedom that need to be estimated---both for learners/listeners and the researcher \autocite[see][]{toscano-mcmurray2010}. A representationally more complex alternative dispenses with the Gaussian assumption and instead assumes that listeners store all previously experienced exemplars \autocite[or some pruned set of exemplars,][]{pierrehumbert2001}. This allows for more accurate (non-parametric) representations of previous experience but also introduces many additional degrees of freedom--both for learners/listeners and the researcher \autocite[for discussion, see][]{apfelbaum-mcmurray2015}. We expect the big picture conclusions we draw below to be largely independent of the specific model of category representations.} Bivariate Gaussian categories fit to the data from \textcite{chodroff-wilson2018} are shown in Figure \ref{fig:show-representations-plots}, along with the categorization functions of the f0-VOT space that would result from these categories prior to considering other effects on decision-making that we discuss in the next section.



\begin{figure}

{\centering \subfloat[Distributions for /b/ and /p/\label{fig:show-representations-plots-1}]{\includegraphics[width=0.33\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.density.panel_1} }\subfloat[Distributions for /d/ and /t/\label{fig:show-representations-plots-2}]{\includegraphics[width=0.33\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.density.panel_2} }\subfloat[Distributions for /g/ and /k/\label{fig:show-representations-plots-3}]{\includegraphics[width=0.33\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.density.panel_3} }\newline\subfloat[Categorization for /b/-/p/\label{fig:show-representations-plots-4}]{\includegraphics[width=0.33\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.categorization.panel_1} }\subfloat[Categorization for /d/-/t/\label{fig:show-representations-plots-5}]{\includegraphics[width=0.33\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.categorization.panel_2} }\subfloat[Categorization for /g/-/k/\label{fig:show-representations-plots-6}]{\includegraphics[width=0.33\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.categorization.panel_3} }

}

\caption{Illustrating listeners' implicit category representations. \textbf{Top row:} Bivariate Gaussian category likelihoods learned from (fit to) the by-talker normalized cue distributions in Figure \ref{fig:demonstrate-normalization}B. Ellipses show 95\% probability mass. \textbf{Bottom row:} Categorization functions that would result from these category likelihoods prior to taking into account other effects on decision-making (see Figure \ref{fig:show-lapse-bias-demonstration-plots} in the next section).}\label{fig:show-representations-plots}
\end{figure}

\hypertarget{post-perceptual-decision-making-incorporating-priors-response-biases-and-attentional-lapses}{%
\subsubsection{Post-perceptual decision-making: incorporating priors, response biases, and attentional lapses}\label{post-perceptual-decision-making-incorporating-priors-response-biases-and-attentional-lapses}}

The third and final step in our categorization model takes the output of the second step and derives a decision/recognition.\footnote{This step can be split further into \emph{recognition} of the category and post-recognition processes that affect the behavioral \emph{response}. For the present purpose, we group these processes together. Similarly, some models of perceptual decision-making distinguish between \emph{decision thresholds} (the amount of evidence necessary before a decision is being made) and \emph{decision biases} \autocites[stimulus-independent effects on the activation or probability of a response option,][]{clarkedavidson2008,venezia2012}. For the present purpose, these two have essentially identical effects.} This includes the integration of information about the prior probability of a category \(c\) in the current context, \(p(c | context)\). In Bayesian ideal observer models, this integration takes place according to Bayes theorem, yielding an estimate of each category's posterior probability \autocites[e.g.,][]{luce-pisoni1998,norris-mcqueen2008}. Alternative computational approaches can introduce additional degrees of freedom, for example, by allowing non-optimal weighting of priors and category likelihoods. As our goal here is to arrive at the simplest possible model, we follow the approach taken in ideal observers:

\begin{equation}\label{eq:posterior-probability}
\begin{split}
p(c | input) & = \frac{p(input | c) p(c)}{\Sigma_i p(input | c_i) p(c_i)} \\
                    & = \frac{\mathcal{N}\!(input | \mu_c, \Sigma_c) p(c)}{\Sigma_i \mathcal{N}\!(input | \mu_{c_i}, \Sigma_{c_i}) p(c_i)}
\end{split}
\end{equation}

where the second row substitutes the Gaussian category likelihoods assumed in the previous section into the equation. In Bayesian models, \(p(c_i)\) is generally assumed to reflect a rational estimate based on either the relative frequency of the category in this type of context in listeners' longterm experience or an estimate based on the expectations about the present context. The latter allows for the integration of \emph{response biases} that go beyond capturing the relative frequency of categories in previous experience. In an experiment with a 2AFC task, for example, participants might expect both response options to occur about equally often. This might lead participants to adjust response biases based on the sequence of most recently observed categories. The simplified model in Figure \ref{fig:model-perceptual-decision-making} captures these response biases---regardless of whether they reflect priors based on the relative frequency of categories, meta-reasoning about the structure of experiments, or other factors---through the parameters \(\pi_{c}\).
For a \(J\)-way categorical outcome, this introduces \(J-1\) degrees of freedom (since \(\Sigma_i \pi_{c_i} = 1\)) that cannot be independently estimated from phonetically annotated databases. These parameters can, however, often be set based on assumptions about the current task as well as on the extent to which prior expectations about natural language transfer to this task. For example, standard experimental designs provide participants with ample evidence that prior expectations based on the relative frequency of lexical items in natural language use do \emph{not} transfer to the experiment \autocite[cf.][]{jaeger2010}. In such contexts, participants might quickly come to adjust their expectations and employ uniform response biases.

In addition to response biases, the categorization model in Figure \ref{fig:model-perceptual-decision-making} includes the possibility that listeners sometimes respond \emph{in}dependent of the stimulus. As we will show below, this is a critical addition that affects what types of behavioral changes can be explained by changes in decision biases. Stimulus-independent responses can occur, for example, because of attentional lapses. On such occasions, the response is only influenced by the response biases. Lapsing models \emph{without} consideration of response biases have previously been used in some analyses of exposure effect on speech perception \autocite{clayards2008,mcmurray-jongman2011,kleinschmidt-jaeger2016cogsci}. For the present purpose, we further assume that these response biases on lapsing trials are identical to the response biases on non-lapsing trials. Assuming equal priors for all alternative categories, this yields the following simplified model of the joint effect of attentional lapses and response biases, where \(\lambda\) is lapse rate---the probability of lapsing:\footnote{Readers familiar with psychometric models might recognize the close relation between Equation \eqref{eq:posterior-probability-lapse} and the standard psychometric model in \textcite{wichmann-hill2001}: \(\gamma + (1-\gamma-\lambda) F(stimulus | \alpha, \beta)\). In the Wichmann and Hill model, \(\gamma\) describes the floor and \(1-\lambda\) describes the ceiling probability. In Equation \eqref{eq:posterior-probability-lapse}, \(\lambda\) is the lapse rate and \(\pi_c\) determines how the lapses (and non-lapses) are affected by response biases, resulting in a floor of \(\lambda \pi_c\) and a ceiling of \(1 - \lambda(1 - \pi_c)\). For the special case of two Gaussian categories with identical variance along a unidimensional cue continuum, Equation \eqref{eq:posterior-probability-lapse} can be described by Wichmann and Hill's psychometric model if the perceptual model \(F\) is set to be a logistic function with appropriate choice of its threshold \(\alpha\) and slope \(\beta\) \autocite[cf.][p.~200]{kleinschmidt-jaeger2015}. For unequal variances and/or additional cue dimensions, additional \(\beta\)s are required.}

\begin{equation}\label{eq:posterior-probability-lapse}
p(c | input) = (1 - \lambda) \frac{\mathcal{N}\!\left( input | \mu_c, \Sigma_c \right) \pi_c}{\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i} \right) \pi_{c_i}} + \lambda \frac{\pi_c}{\Sigma_i \pi_{c_i}}
\end{equation}

In Figure \ref{fig:model-perceptual-decision-making}, the fact that response bias affects listeners' responses in both lapsing and non-lapsing trials is indicated by the two arrows leaving from \(\pi_c\). Figure \ref{fig:show-lapse-bias-demonstration-plots} visualizes the effects of the two parameters \(\lambda\) and \(\pi_c\) for a 2AFC categorization task.



\begin{figure}

{\centering \subfloat[$\lambda$ = 0, $\pi_{/d/}=.5$\label{fig:show-lapse-bias-demonstration-plots-1}]{\includegraphics[width=0.49\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.categorization.panel_lambda=1} }\subfloat[$\lambda$ = .2,  $\pi_{/d/}=.5$\label{fig:show-lapse-bias-demonstration-plots-2}]{\includegraphics[width=0.49\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.categorization.panel_lambda=2} }\newline\subfloat[$\lambda$ = .2, $\pi_{/d/}=.1$\label{fig:show-lapse-bias-demonstration-plots-3}]{\includegraphics[width=0.49\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.categorization.panel_pi=1} }\subfloat[$\lambda$ = .2, $\pi_{/d/}=.9$\label{fig:show-lapse-bias-demonstration-plots-4}]{\includegraphics[width=0.49\linewidth,height=0.49\textheight]{../figures/plotly//p.3d.categorization.panel_pi=2} }

}

\caption{Illustrating the effects of \(\lambda\) and \(\pi_c\) on the posterior probability of /d/, using the two bivariate Gaussian categories of /d/ and /t/ shown in Figure \ref{fig:show-representations-plots}b. The colored planes indicate the ceiling and flooring levels of posterior probability of /d/. Panel a) here is identical to Panel e) in Figure \ref{fig:show-representations-plots}，with \(\lambda\) = 0 and \(\pi_{/d/}=.5\). \textbf{Top panel:} Differences in lapse rate \(\lambda\) for a uniform bias of \(\pi_{/d/}=.5\). As the lapse rate increases, the ceiling and flooring responses at the end points change while the categorization slope remains the same. \textbf{Bottom panel:} Differences in bias \(\pi_{/d/}\) when the lapse rate \(\lambda\) = .2. As the bias towards /d/ changes from 0.1 to 0.9, the categorization surface shifts upwards and to the right, resulting in more /d/ responses across the phonetic space.}\label{fig:show-lapse-bias-demonstration-plots}
\end{figure}

The final part of ASP's categorization model is the decision rule that takes the posterior in Equation \eqref{eq:posterior-probability-lapse} as input and returns a response. Here, we follow a common assumption in research in speech perception and employ Luce's choice rule \autocites{luce1959}[for a comparison of decision rules, see][]{massaro-friedman1990}. Under this decision rule, the predicted distribution of responses is identical to the posterior in Equation \eqref{eq:posterior-probability-lapse}. For example, in a 2AFC categorization task, a posterior probability of .3 for /d/ and .7 for /t/ would result in a /d/-response with .3 probability or a /t/-response with .7 probability.

\hypertarget{the-change-models-adaptationlearning}{%
\subsection{The change models (`adaptation'/`learning')}\label{the-change-models-adaptationlearning}}

This completes the description of the categorization model in Figure \ref{fig:model-perceptual-decision-making}. For any parameterization of \(\lambda\), \(\pi_c\), \(\mu_c\), \(\Sigma_c\), and \(\mu\), this model takes acoustic stimuli as input and returns predictions about the expected response distribution (e.g., 30\% /d/-responses and 70\% /t/-responses). In practice, all but one of these parameters (the lapse rate \(\lambda\)) can either be independently estimated from phonetically annotated databases (\(\mu_c\), \(\Sigma_c\), and \(\mu\)) or assumed to have certain values (e.g., uniform response biases \(\pi_c\)), leaving a single parameter that must be estimated from perceptual data.

Next, we extend this model to capture \emph{changes} at any of the three mechanistic levels. That is, we specify \emph{change models} that describe how \(\pi_c\), \(\mu_c\), \(\Sigma_c\), and \(\mu\) can change with exposure. It is these change models that make it possible to use ASP to derive predictions for human responses following exposure---for example, during the test phase of an experiment on perceptual recalibration or accent adaptation (see Figure \ref{fig:overview-change}). Together, the three change models introduce a total of four new parameters that must be estimated from perceptual data, all of which can be understood as related to \emph{learning rates}.

\hypertarget{sec:change-normalization}{%
\subsubsection{\texorpdfstring{Modeling \emph{changes} in normalization}{Modeling changes in normalization}}\label{sec:change-normalization}}

\begin{figure}
\centering
  \tikz{ %
    \node[obs] (x_in) {x} ; %
    \node[det, above=of x_in] (x_prime) {} ; %
    \node[above=of x_prime] (dots) {$\vdots$} ; %
    \node[det, right=of x_prime] (mu_n) {$\mu_n$} ; %
    \node[obs, right=of mu_n, yshift=-.75cm] (xbar) {$\bar{x}$} ; %
    \node[right=of xbar] (xbar_description) {{\em cue mean observed from unfamiliar talker}} ;
    \node[obs, below=of xbar, yshift=.5cm] (n) {$N$} ; %
    \node[right=of n] (n_description) {{\em number of observations from unfamiliar talker}} ;
    \node[latent, right=of mu_n, yshift=.75cm] (mu_0) {$\mu_0$} ; %
    \node[right=of mu_0] (mu_0_description) {{\em prior beliefs about cue mean} ($k$ DF)} ;
    \node[latent, above=of mu_0, yshift=-.5cm] (kappa_0) {$\kappa_0$} ; %
    \node[right=of kappa_0] (kappa_0_description) {{\em strength of prior beliefs about cue mean} ($1$ DF)} ;
    \edge {x_in} {x_prime} ; %
    \edge {x_prime} {dots} ; %
    \edge {mu_n} {x_prime} ; %
    \edge {mu_0} {mu_n} ; %
    \edge {kappa_0} {mu_n} ; %
    \edge {xbar} {mu_n} ; %
    \edge {n} {mu_n} ; %
}
\caption{Graphical representation of Equation \ref{eq:normalization-change}, describing how normalization changes with exposure. $\mu_n$ here substitutes $\mu$ in Figure \ref{fig:model-perceptual-decision-making} (removing those degrees of freedom from the categorization model). While we mark the prior cue mean $\mu_0$ as a latent variable, it can be estimated from phonetic databases (as we do in  Case Studies 1 and 2), leaving $\kappa_0$ as the only degree of freedom for this change model.\label{fig:graphical-model-changes-normalization}}
\end{figure}

C-CuRE and most other models of normalization assume that the theoretical quantities employed in normalization---for C-CuRE, the means of all cues---are \emph{known} to the listeners \autocites[e.g.,][]{apfelbaum-mcmurray2015,mcmurray-jongman2011}. However, listeners must somehow \emph{infer} these quantities from the observed inputs \autocite[see also Section 3.3 in][]{weatherholtz-jaeger2016}. For example, in the types of experiments we consider below, listeners need to \emph{infer} the unfamiliar talker's cue means from the inputs observed during exposure. At the beginning of exposure, the best estimates of the cue means for the unfamiliar talker are the mean expected based on listeners' longterm experience with various different talkers (\(\mu_0\)). With increasing exposure to the unfamiliar talker, listeners can update their estimates of the talker's cue means based on the input. ASP thus expands C-CuRE with a linking hypothesis that describes how listeners update their estimates of the unknown mean.

Some proposals for such functions include moving-window algorithms that estimate the mean as the sample mean over a finite number of the most recent observations \autocite{lee2002,zhang-peng2021}. Here, we model listeners' estimate of the talker-specific cue mean \(\mu_n\) after \(N\) observations from that talker as simple linear interpolation between the cue mean observed in previous longterm experience (\(\mu_0\)) and the mean observed in the sample observed from the unfamiliar talker (\(\bar{x}\)). This approach has the advantage that listeners can draw on prior experience when they do not yet have much (or any) data from an unfamiliar talker. They can then update the inferred cue mean as they observe further data from the talker \autocite[see discussion of the flexibility-stability trade-off in][p.~178-182]{kleinschmidt-jaeger2015}:

\begin{equation}\label{eq:normalization-change}
\mu_n = \frac{1}{\kappa_0 + N} \left( \kappa_0 \mu_0 + N \bar{x} \right) = \frac{\kappa_0}{\kappa_0 + N} \mu_0 + \frac{N}{\kappa_0 + N}\bar{x}
\end{equation}

Figure \ref{fig:graphical-model-changes-normalization} expresses Equation \eqref{eq:normalization-change} as a graphical model. The two parameters describing the input from the unfamiliar talker, \(N\) and \(\bar{x}\), are determined by the exposure data used in the experiment (the estimation of the cue means, \(\bar{x}\) requires that the exposure stimuli are phonetically annotated). \(\mu_0\) can be estimated from sufficiently large phonetically annotated databases of speech, essentially assuming that listeners have learned the overall cue means across talkers from previously experienced speech input. This is what we did to create Figure \ref{fig:demonstrate-normalization}, and it is what we assume in the remainder of this study. This leaves only one degree of freedom that is not independently determined: the relative weight of previous experience compared to the input received from the talker so far (\(\kappa_0\)). This is the parameter we vary in the case studies presented below to illustrate the types of results that can be explained through changes in normalization due to recent exposure.

One important characteristic of normalization that we will return to in the general discussion is that it applies to cues \emph{regardless of the category they originate from/map into}. On the one hand, this makes normalization potentially very efficient, by considering \emph{all} available information about a cue, rather than just observations of a specific category \autocite{apfelbaum-mcmurray2015}. On the other hand, this limits what types of changes in the mapping from cues to categories normalization can account for.

\hypertarget{sec:ideal-adaptor}{%
\subsubsection{\texorpdfstring{Modeling \emph{changes} in category representations}{Modeling changes in category representations}}\label{sec:ideal-adaptor}}

Just like normalization requires inference of \emph{overall} cue statistics (e.g., the overall mean of a cue), learning of category representations requires inferences of \emph{category-specific} cue statistics (e.g., the category mean and covariance). Changes in representations can thus be understood as the process of inferring the talker's category likelihood function, the distribution of phonetic cues conditional on the category. A theory of this inference process---the ideal adaptor---is introduced in \textcite{kleinschmidt-jaeger2015}. Bayesian belief-updating models based on the ideal adaptor framework have been shown to provide a good qualitative and quantitative fit against human responses in experiments on perceptual recalibration \autocites{kleinschmidt-jaeger2011,kleinschmidt-jaeger2012}[for closely related models, see][]{xie2021cognition}, selective adaptation \autocite{kleinschmidt-jaeger2016pbr}, unsupervised or semi-supervised distributional learning \autocites[e.g.,][]{kim2020,kleinschmidt-jaeger2016cogsci,theodore-monto2019}[for closely related model, see][]{bejjanki2011,clayards2008}, and accent adaptation \autocites{hitczenko-feldman2016}[for a closely related model, see][]{tan2021}. In a MEG experiment, \textcite{sohoglu-davis2016} found that their predictive coding implementation of Bayesian belief-updating provided a good qualitative fit against changes in activation in the superior temporal gyrus following exposure to degraded speech \autocite[see also][]{davis-sohoglu2020}. To date, however, an explicit comparison with alternative types of change models---like normalization or changes in decision-making---continue to be lacking.

The belief-updating model describes the inference of category means and category variances in ways that are conceptually similar to the type of interpolation we described in Equation \eqref{eq:normalization-change}, by combining knowledge based on previously experienced speech input \(\mathcal{D}_0\) with the observations made from the unfamiliar talker. The specific instance of the model we use here---belief-updating over a Normal-Inverse-Wishart (\(\mathcal{NW^{-1}}\)) prior \autocite{kleinschmidt-jaeger2015}---describes the uncertainty listeners have about the category means \(\mu_c\) and category covariance matrices \(\Sigma_c\) prior to any observations from an unfamiliar talker as a function of four variables \autocite[p.~132-3]{murphy2012}:

\begin{equation}\label{eq:niw-updating}
\begin{split}
p\left( \mu_c, \Sigma_c | \mathcal{D}_0 \right) & = \mathcal{NW}^{-1} \left( \mu_c, \Sigma_c | \mathrm{m}_{c,0}, \kappa_{c,0}, \nu_{c,0}, \mathrm{S}_{c,0} \right) \\
& = \mathcal{N}\left( \mu_c | \mathrm{m}_{c,0}, \frac{1}{\kappa_{c,0}} \Sigma_{c} \right) \times \mathcal{W}^{-1}\left( \Sigma_c | \mathrm{S}_{c,0}, \nu_{c,0} \right)
\end{split}
\end{equation}

The Normal part of the \(\mathcal{NW^{-1}}\) model describes the uncertainty about the category mean \(\mu_c\), the Inverse-Wishart part describes the uncertainty about the category covariance \(\Sigma_c\). For the former, \(\mathrm{m}_{c,0}\) is the mean of the normal distribution describing the uncertainty about the category mean \(\mu_c\), and \(\kappa_{c,0}\) indicates the extent to which listeners transfer their prior beliefs about the category mean to the present input. The larger \(\kappa_{c,0}\) is, the more certain listeners are about the category mean even prior to any observation, and the less their inferences about the talker's category mean will be influenced by observations from the talker. Put differently, larger \(\kappa_{c,0}\) predicts slower learning of changes in the category mean. Similarly, \(\mathrm{S}_{c,0}\) is the scale matrix of the Inverse-Wishart---having a conceptually similar function to the mean \(\mathrm{m}_{c,0}\) of the Normal distribution---and \(\nu_{c,0}\) indicates extent to which listeners transfer their prior beliefs about the category covariance to the present input. Just as larger \(\kappa_{c,0}\) predicts slower learning of changes in the category mean, larger \(\nu_{c,0}\) predicts slower learning of changes in the category covariances.

In practice, researchers can estimate the \(\mathrm{m}_{c,0}\)s and the \(\mathrm{S}_{c,0}\)s such that they yield the distribution of category means and covariances that would be expected given listeners' previous long-term experience (see SI, \ref{sec:SI-models-changes-in-representations}). This leaves two degrees of freedom to model changes in each category's likelihood, \(\kappa_{c,0}\) and \(\nu_{c,0}\). We further follow previous work to make the simplifying assumption that all categories have the same prior \(\kappa_{c,0}\) and \(\nu_{c,0}\) \autocite{kleinschmidt-jaeger2015,kleinschmidt-jaeger2016cogsci}, leaving just two degrees of freedom \emph{across} all categories to model changes in representations. These are the two parameters we vary in the case studies presented below to illustrate the types of results that can be explained through changes in representations. Figure \ref{fig:demonstrate-niw-prior-mu-sigma} demonstrates how \(\kappa_{c,0}\) and \(\nu_{c,0}\) affect the uncertainty about the category likelihoods for the simple case of univariate Gaussian categories along a single cue dimension (VOT). The case studies we present below employ bivariate Gaussian categories along f0 and VOT.



\begin{figure}

{\centering \includegraphics{../figures/knitted/demonstrate-niw-prior-mu-sigma-1} 

}

\caption{Illustrating the effects of \(\kappa_{c,0}\) and \(\nu_{c,0}\) on the uncertainty about the category means \(\mu_c\) and variances \(\Sigma_c\) for univariate /d/ and /t/ categories along VOT. The four priors have identical expected category means (\(\mathbf{E}(\mu_{/d/}), \mathbf{E}(\mu_{/t/})\)) and variances (\(\mathbf{E}(\sigma_{/d/}), \mathbf{E}(\sigma_{/t/})\))---set to match the average of the C-CuRE normalized category means and covariance matrices obtained from the data in \textcite{chodroff-wilson2018}, and indicated by black points. The four priors differ, however, in their uncertainty about the category means and variances and thus in the changes they predict to occur when exposed to input from an unfamiliar talker. The more uncertain a listener is about the category means and variances of an unfamiliar talker (smaller \(\kappa_{c,0}\) and \(\nu_{c,0}\)), the quicker that listener will adjust their expectations based on the inputs observed from that talker (see Figures \ref{fig:demonstrate-changes-in-representations} and \ref{fig:demonstrate-changes-in-representations-b}). Density lines are drawn at \(10^{-3}\) to \(10^{-10}\) at powers of 10.}\label{fig:demonstrate-niw-prior-mu-sigma}
\end{figure}

\begin{figure}
\centering
  \tikz{ %
    \node (x_in) {$\vdots$} ; %
    \factor[above=of x_in] {x_prime} {left:$\mathcal{N}$} {} {}; %
    \node[above=of x_prime] (dots) {$\vdots$} ; %
    \node[det, right=of x_prime, yshift=2cm] (mu_n) {$\mu_{c,n}$} ; %
    \node[det, right=of x_prime, yshift=-2cm] (sigma_n) {$\Sigma_{c,n}$} ; %
    % Change model for mean
    \node[latent, right=of mu_n, yshift=.75cm] (m_0) {${\rm m}_{c,0}$} ; %
    \node[right=of m_0] (m_0_description) {{\em prior beliefs about category mean} ($jk$ DF)} ;
    \node[latent, above=of m_0, yshift=-.5cm] (kappa_0) {$\kappa_0$} ; %
    \node[right=of kappa_0] (kappa_0_description) {{\em strength of prior beliefs about category mean} (up to $j$ DF)} ;
    \node[obs, right=of mu_n, yshift=-.75cm] (xbar) {$\bar{x_c}$} ; %
    \node[right=of xbar] (xbar_description) {{\em category mean observed from unfamiliar talker}} ;
    \node[obs, below=of xbar, yshift=.5cm] (n) {$N_c$} ; %
    \node[right=of n] (n_description) {{\em number of observations for category from unfamiliar talker}} ;
    % Change model for covariance
    \node[latent, right=of sigma_n, yshift=-.75cm] (s_0) {${\rm S}_{c,0}$} ; %
    \node[right=of s_0] (sigma_0_description) {{\em prior beliefs about category covariance matrix} (up to $\frac{j}{2}(k^2+k)$ DF)} ;
    \node[latent, below=of s_0, yshift=.5cm] (nu_0) {$\nu_0$} ; %
    \node[right=of nu_0] (nu_0_description) {{\em strength of prior belief about category covariance} (up to $j$ DF)} ;
    \node[obs, right=of sigma_n, yshift=.75cm] (sbar) {${\rm S}_c$} ; %
    \node[right=of sbar] (sbar_description) {{\em category sums of squares observed from unfamiliar talker}} ;
    % edges
    \edge {x_in} {x_prime} ; %
    \edge {x_prime} {dots} ; %
    \edge {mu_n} {x_prime} ; %
    \edge {sigma_n} {x_prime} ; %
    \edge {m_0} {mu_n} ; %
    \edge {kappa_0} {mu_n} ; %
    \edge {xbar} {mu_n} ; %
    \edge {sbar} {sigma_n} ; %
    \edge {s_0} {sigma_n} ; %
    \edge {nu_0} {sigma_n} ; %
    \edge {n} {mu_n} ; %
    \edge {n} {sigma_n} ; %
}
\caption{Simplified graphical representation of Equation \ref{eq:niw-updating}, describing how category representations change with exposure. $\mu_{c,n}$ and $\Sigma_{c,n}$ here substitutes $\mu_c$ and $\Sigma_c$ in Figure \ref{fig:model-perceptual-decision-making} (removing those degrees of freedom from the categorization model). While we mark the prior ${\rm m}_{c,0}$ and ${\rm S}_{c,0}$ as latent variables, they can be estimated from phonetic databases (as we do in  Case Studies 1 and 2), leaving $\kappa_{c,0}$ and $\nu_{c,0}$ as the only degrees of freedom for this change model. A more detailed graphical model is provided in the SI (\ref{sec:SI-models-changes-in-representations}). \label{fig:graphical-model-changes-representations}}
\end{figure}

As listeners observe additional information from the unfamiliar talker, they update their beliefs (and thus uncertainty) about the distribution of that talker's category means and covariances by integrating their prior beliefs with the observed input. The updating formula for each parameter is provided in the SI (\ref{sec:SI-models-changes-in-representations}). Figures \ref{fig:demonstrate-changes-in-representations} and \ref{fig:demonstrate-changes-in-representations-b} illustrate how speech input from a talker with unexpected pronunciations changes listeners' beliefs about the category likelihoods and, consequently, their categorization functions. We exposed the four models from Figure \ref{fig:demonstrate-niw-prior-mu-sigma}---each of which reflects the expected category means and variances derived from Chodroff and Wilson (2018), but with different certainty---to input from a talker with unexpected pronunciations of /d/ and /t/. Specifically, the talker's /d/ and /t/ categories were shifted by +25 msecs VOT compared to a (C-CuRE normalized) talker in Chodroff and Wilson' (2018) data. Additionally, the talker's /t/-category exhibited half the variance found in Chodroff and Wilson' data. Figure \ref{fig:demonstrate-changes-in-representations} shows how the expected category likelihoods of the four models change as a function of input from the new talker. Models with weak prior beliefs about category means (\(\kappa_{c,0}=4\)) accommodate the unfamiliar speech input by changing beliefs about the category mean---shifting categories `horizontally'. Models with weak prior beliefs about category variance (\(\nu_{c,0}=4\)) accommodate the unfamiliar speech input by changing beliefs about the category variance---expanding the category. This is particularly apparent for the /d/ category. Figure \ref{fig:demonstrate-changes-in-representations-b} demonstrates the consequences of these changes for the expected categorization function.



\begin{figure}

{\centering \animategraphics[,controls,loop]{5}{../figures/knitted/demonstrate-changes-in-representations-}{1}{1}

}

\caption[Illustrating the effects of \(\kappa_{c,0}\) and \(\nu_{c,0}\) on changes in the expected category likelihoods, assuming a binary phonetic contrast between two Gaussian categories (/d/-/t/) along a unidimensional continuum (VOT). Updating is shown for the data points in the rug along the x-axis: 20 observations each of /d/and/t/ from a talker who realized both categories with a shifted mean of 25 msec, and exhibits typical variance for /d/ but only half of the typical variance for /t/. We set the lapse rate \(\lambda = 0\) and response biases to uniform (\(\pi_{c,0}=.5\)). Animation controls require Acrobat PDF reader.]{Illustrating the effects of \(\kappa_{c,0}\) and \(\nu_{c,0}\) on changes in the expected category likelihoods, assuming a binary phonetic contrast between two Gaussian categories (/d/-/t/) along a unidimensional continuum (VOT). Updating is shown for the data points in the rug along the x-axis: 20 observations each of /d/and/t/ from a talker who realized both categories with a shifted mean of 25 msec, and exhibits typical variance for /d/ but only half of the typical variance for /t/. We set the lapse rate \(\lambda = 0\) and response biases to uniform (\(\pi_{c,0}=.5\)). Animation controls require Acrobat PDF reader.}\label{fig:demonstrate-changes-in-representations}
\end{figure}



\begin{figure}

{\centering \animategraphics[,controls,loop]{5}{../figures/knitted/demonstrate-changes-in-representations-b-}{1}{1}

}

\caption[Changes in expected categorization functions resulting from the changes in the expected category likelihoods in Figure \ref{fig:demonstrate-changes-in-representations}. We set the lapse rate \(\lambda = 0\), the prior lapse biases to uniform (\(\pi_{c,0}=.5\)). Neither normalization, nor response biases changed as a function of the input. Animation controls require Acrobat PDF reader.]{Changes in expected categorization functions resulting from the changes in the expected category likelihoods in Figure \ref{fig:demonstrate-changes-in-representations}. We set the lapse rate \(\lambda = 0\), the prior lapse biases to uniform (\(\pi_{c,0}=.5\)). Neither normalization, nor response biases changed as a function of the input. Animation controls require Acrobat PDF reader.}\label{fig:demonstrate-changes-in-representations-b}
\end{figure}

Unlike normalization, changes in category representations can capture \emph{category-specific} changes in cue distributions. As we lay out in more detail in the general discussion, this makes representational changes computationally less parsimonious than normalization but also more expressive: there are ways in which talkers might differ from each other that can be better captured by changes in representations than by normalization (which of these two change mechanisms better describes \emph{listeners'} abilities is, however, an open question).

\hypertarget{sec:change-bias}{%
\subsubsection{\texorpdfstring{Modeling \emph{changes} in decision-making}{Modeling changes in decision-making}}\label{sec:change-bias}}

Finally, we specify a linking hypothesis for changes in response biases. To the best of our knowledge, this problem has not previously been formalized for speech perception. The change model we propose is general and can be applied to changes in decision-making beyond speech perception.

Consider a typical experiment on accent adaptation. A listener unfamiliar with the L2 accent will initially miscategorize the L2-accented inputs. Typically, these errors will not be random but rather exhibit directionality. For example, in experiments on Mandarin-accented English, L1-English participants initially mishear voiced syllable-final stops as voiceless \autocites[e.g., hearing \emph{lid} as \emph{lit},][]{flege1992,xie2016jep}. Critically, experiments of this type tend to employ exposure stimuli that effectively label the intended category of the stimulus (e.g., \emph{lemona\_e}). Participants thus receive an error signal, indicating how much their expectations based on the acoustic input mismatched the intended (labeled) category. Under the view that talker adaptation reflects changes at a post-perceptual stage of speech processing, participants can use this prediction error---operationalized here as the surprisal \(I(c_i|x_i)\) of the category label given the acoustic input---to adapt the biases for all categories.\footnote{All three change models discussed here are sensitive to prediction errors \autocite[e.g., for Bayesian belief-updating, sensitivity to the prediction error is a side-effect of optimal information integration, as demonstrated e.g., in][]{jaeger2019}. The three change models differ, however, in what \emph{type} of prediction error drives adaptation. For changes in normalization, the relevant prediction error is based on expectations about the overall cue distributions. For changes in category representations, the relevant prediction error is based on expectations about the category-specific cue distribution. For changes in decision-making, the relevant prediction error is based on expectations about the category given the cues.} For the present example, participants would increase the bias for the labeled category (/d/ in this example) and decrease the bias for all other categories, thereby improving their recognition accuracy for the L2-accented speech.\footnote{We thank Zach Burchill for bringing this possibility to our attention.} With each exposure observation, the log-odds for the intended category increase by an amount jointly determined by the prediction error and the change/learning rate (\(\beta_{\pi}\)), with larger prediction errors and larger \(\beta_{\pi}\)s resulting in larger changes (for details, see SI, \ref{sec:SI-models-changes-in-decision-making}). We note that this makes our change model for decision-making sensitive to the order of exposure, unlike the two change models presented so far. We return to this issue in our case studies. Figure \ref{fig:graphical-model-changes-decision-making} expresses the change model for decision-making as a graphical model.

\begin{figure}
\centering
  \tikz{ %
    \node (x_in) {$\vdots$} ; %
    \factor[above=of x_in, xshift=1cm] {multi} {left: Multi} {} {}; %
    \node[above=of x_in] (dots) {$\vdots$} ; %
    \node[det, right=of multi] (pi_n) {$\pi_{c,n}$} ; %
    % Change model for response biases
    \node[latent, right=of pi_n, yshift=1cm] (beta) {$\beta_{\pi}$} ; %
    \node[right=of beta] (beta_description) {{\em learning rate for changes in response biases} ($1$ DF)} ;
    \node[det, below=of beta, yshift=.5cm] (surprisal_i) {$I_{c_{i} | x_{i}}$} ; %    
    \node[obs, right=of surprisal_i, yshift=.5cm] (x_i) {$x_{i}$} ; %
    \node[right=of x_i, xshift=-.5cm] (x_i_description) {{\em acoustic inputs previously observed from unfamiliar talker}} ;
    \node[obs, right=of surprisal_i, yshift=-.5cm] (c_i) {$c_{i}$} ; %
    \node[right=of c_i, xshift=-.5cm] (c_i_description) {{\em category labels of previously observed from unfamiliar talker}} ;
    % edges
    \edge {x_in} {multi} ; %
    \edge {multi} {dots} ; %
    \edge {pi_n} {multi} ; %
    \edge {beta} {pi_n} ; %
    \edge {surprisal_i} {pi_n} ;
    \edge {x_i} {surprisal_i} ; %
    \edge {c_i} {surprisal_i} ; %
}
\caption{Graphical representation of change model for decision-making. $\pi_{c,n}$ here substitutes $\pi_c$ in Figure \ref{fig:model-perceptual-decision-making} (removing those degrees of freedom from the categorization model). $I$ refers to the surprisal experienced when the acoustic input $x_i$ is recognized (from context) to be of category $c_i$.\label{fig:graphical-model-changes-decision-making}}
\end{figure}

Figure \ref{fig:demonstrate-lapse-bias-change} illustrates the proposed change model for three different values of \(\beta_{\pi}\) when the lapse rate \(\lambda=0\). Here and in the case studies in Sections \ref{sec:PR} and \ref{sec:AA}, we assume uniform initial biases across all categories, leaving \(\beta_{\pi}\) as the only degree of freedom. The right panel highlights an interesting limitation in the types of results that can be explained through changes in response biases: in the absence of lapses (\(\lambda=0\)) or when all responses are lapses (\(\lambda=1\)), changes in response biases can only explain \emph{additive} effects on the log-odds of the categories, but not changes in the \emph{slope} of the categorization function. This property does not depend on the specific change model assumed here (for derivation, see SI, \ref{sec:consequences-of-lambda}). Critically, this strong constraint on changes in decision-making only follows if \(\lambda \in \{0,1\}\). For \(\lambda \not\in \{0,1\}\), changes in response biases lead to non-additive changes in the posterior log-odds. This is illustrated in Figure \ref{fig:demonstrate-lapse-bias-change-nonzero-lapse}. Once the possibility of non-zero lapse rates is considered, which applies to most experimental scenarios, changes in decision-biases alone may be able to account for complex accent adaptation patterns beyond a mere boundary shift \autocite[e.g.,][]{xie2016jep}. This does not mean that changes in decision-making can explain arbitrary types of adaptive behavior, and we will return to this point in the general discussion (for further visualization, see also SI \ref{sec:consequences-of-lambda}). It does, however, mean that changes in decision-making can explain a wider range of behavior than is typically considered, and we will observe examples of that in our case studies.



\begin{figure}

{\centering \animategraphics[,controls,loop]{5}{../figures/knitted/demonstrate-lapse-bias-change-}{1}{1}

}

\caption[Illustrating the effects of \(\beta_{\pi}\) on changes in the categorization function (\textbf{Left:} posterior probability, \textbf{Right:} posterior log-odds). The model has the same uncertain prior beliefs about category mean and variances as the model in Figure \ref{fig:demonstrate-niw-prior-mu-sigma}D (\(\kappa_{c,0} = \nu_{c,0}=0\)). Neither normalization nor prior beliefs about the categories changes as a function of exposure. We set the lapse rate \(\lambda = 0\), the prior lapse biases to uniform (\(\pi_{c,0}=.5\)). The input to this change model is identical to what was used in Figures \ref{fig:demonstrate-changes-in-representations} and \ref{fig:demonstrate-changes-in-representations-b}. In the left panel, orange arrows indicate changes in the category boundary (point of subjective equality) for \(\beta_{\pi} = 1\). In the right panel, orange arrows indicate the distance between the posterior log-odds resulting from the two most extreme \(\beta_{\pi}\)s. This highlights the fact that changes to response biases have additive effects on the log-odds of /d/ responses when \(\lambda = 0\). Animation controls require Acrobat PDF reader.]{Illustrating the effects of \(\beta_{\pi}\) on changes in the categorization function (\textbf{Left:} posterior probability, \textbf{Right:} posterior log-odds). The model has the same uncertain prior beliefs about category mean and variances as the model in Figure \ref{fig:demonstrate-niw-prior-mu-sigma}D (\(\kappa_{c,0} = \nu_{c,0}=0\)). Neither normalization nor prior beliefs about the categories changes as a function of exposure. We set the lapse rate \(\lambda = 0\), the prior lapse biases to uniform (\(\pi_{c,0}=.5\)). The input to this change model is identical to what was used in Figures \ref{fig:demonstrate-changes-in-representations} and \ref{fig:demonstrate-changes-in-representations-b}. In the left panel, orange arrows indicate changes in the category boundary (point of subjective equality) for \(\beta_{\pi} = 1\). In the right panel, orange arrows indicate the distance between the posterior log-odds resulting from the two most extreme \(\beta_{\pi}\)s. This highlights the fact that changes to response biases have additive effects on the log-odds of /d/ responses when \(\lambda = 0\). Animation controls require Acrobat PDF reader.}\label{fig:demonstrate-lapse-bias-change}
\end{figure}



\begin{figure}

{\centering \animategraphics[,controls,loop]{5}{../figures/knitted/demonstrate-lapse-bias-change-nonzero-lapse-}{1}{1}

}

\caption[Same as Figure \ref{fig:demonstrate-lapse-bias-change} but for a lapse rate \(\lambda=\) 0.05. For such non-zero lapse rates, changes to the response bias have \emph{non}-additive effects on the posterior log-odds. This means that, even in log-odds, changes in response biases can have effects that go beyond purely additive changes in the categorization functions (`shifts in category boundaries'). Animation controls require Acrobat PDF reader.]{Same as Figure \ref{fig:demonstrate-lapse-bias-change} but for a lapse rate \(\lambda=\) 0.05. For such non-zero lapse rates, changes to the response bias have \emph{non}-additive effects on the posterior log-odds. This means that, even in log-odds, changes in response biases can have effects that go beyond purely additive changes in the categorization functions (`shifts in category boundaries'). Animation controls require Acrobat PDF reader.}\label{fig:demonstrate-lapse-bias-change-nonzero-lapse}
\end{figure}

\hypertarget{next-in-this-theater-overview-of-case-studies}{%
\section{Next in this theater: overview of case studies}\label{next-in-this-theater-overview-of-case-studies}}

Next, we present two case studies that illustrate the predictions of the three change models for two types of paradigms that have been particularly influential: perceptual recalibration (Case Study 1) and accent adaptation (Case Study 2). Figure \ref{fig:ASP-predictions} summarizes how we will use ASP in these two case studies to derive predictions about listeners' behavior.

\begin{figure}[h]
\begin{center}
\includegraphics[width=.99\columnwidth]{../figures/diagrams/predictive-power-simulations.png}
\caption{The ASP framework can be used to derive predictions, or to fit to human behavior. ASP's change models describe how ASP's categorization model is predicted to change based on the inputs experienced during exposure. The updated categorization model can then be used to predict listeners' response distributions for any test token (as we do in Case Study 1), and/or to predict listeners' recognition accuracy (as we do in Case Study 2). As described in the text, researchers can choose to set some of ASP's parameters based on phonetic databases. By doing so, researchers commit to the assumptions (i) that listeners learn and store some statistics of previously experienced speech input, and (ii) that the database is a sufficiently good approximation of those statistics for an average listener in the experiment. Setting parameters based on phonetic databases reduces the degrees of freedom and functional flexibility of ASP's change models, leading to stronger (more easily falsifiable) predictions. This is the approach we employ in our case studies.}\label{fig:ASP-predictions}
\end{center}
\end{figure}

ASP can be used to predict changes in perception from \emph{weighted combinations} of all three change mechanisms. In the general discussion, we outline why this ability to make predictions based on combinations of mechanisms will likely be critical in moving the field forward. In that context, we discuss how future work can use ASP to address more advanced questions about the factors that determine the relative engagement of the three mechanisms---by investigating how the ASP parameters (\(\kappa_0\), \(\kappa_{c,0}\), \(\nu_{c,0}\), and \(\beta_{\pi}\)) that best explain human behavior depend on factors such as stimulus properties, task demands, and individual differences between listeners.

For our two case studies, however, we deliberately consider ASP models that employ one of the three change mechanisms at a time. This allows us to assess which of the three mechanisms are \emph{sufficient} and which ones are \emph{insufficient}, to explain the signature results of the two paradigms. Of particular theoretical interest is whether it is indeed the case that the computationally least parsimonious change model, changes in category representations, is the only model that can explain the signature results of both perceptual recalibration and accent adaptation paradigms (as seems to be often assumed).

Beyond this specific question, we use the two case studies to (i) illustrate how intuitions about the types of results each mechanism can(not) account for can be misleading, and (ii) show how ASP can be used to inform researchers' intuitions, by predicting the effects of recent exposure on subsequent speech perception. For these reasons, both case studies focus on \emph{qualitative comparison} of ASP's predictions against the signature results from each paradigm, documented in previously published perception experiments. In the general discussion, we show how ASP can be used to compare change models in terms of their \emph{quantitative fit} against listeners' behavior.

The two case studies build on each other, each introducing new insights. Experiments on perceptual recalibration often use comparatively simple stimulus designs, intended to elicit simple boundary shifts along a uni-dimensional perceptual continuum. And while the specific tasks employed during exposure and test can vary across studies, the general stimulus design is remarkably similar across experiments. This allows us to use Case Study 1 to introduce many of the relevant concepts while keeping the paradigm comparatively simple. Case Study 2 then introduces additional complexities that come with modeling \emph{natural} accents, which can deviate from listeners' expectations in heterogeneous ways across multiple acoustic-phonetic dimensions. This additional complexity has caused researchers to question whether the same mechanisms that underlie perceptual recalibration also underlie adaptation to natural accents \autocites[see recent reviews and discussions,][]{baeseberk2018,bent-baeseberk2021,zheng-samuel2020}. Case Study 2 sheds new light on this question by asking whether even simple changes in decision-making or normalization can suffice to explain accent adaptation. Additionally, Case Study 2 allows us to illustrate an important consideration for future work: while all three change mechanisms predict that exposure should facilitate---or at least not hinder---subsequent processing of the same accent, the exact effects of exposure are predicted to depend on the specific accent properties and how they relate to listeners' prior expectations. In the general discussion, we elaborate on this point and how future work can draw on it.

\hypertarget{sec:PR}{%
\section{Case Study 1: perceptual recalibration}\label{sec:PR}}

In a typical perceptual recalibration experiment, participants are randomly assigned to one of two exposure groups that subsequently perform the same test. During exposure, participants hear stimuli that contain a critical phonetic contrast \autocite[e.g., syllable-intial /d/ vs.~/t/, as in \emph{croco\emph{d}ile} or \emph{cafe\emph{t}eria},][]{kraljic-samuel2006}, mixed with many fillers that do not contain that phonetic contrast. Between participants, critical exposure stimuli are manipulated with the goal to shift the category boundary along the phonetic contrast into different directions. This is achieved by exposing listeners to \emph{typical} pronunciations from one category and pronunciations of the other category that are acoustically \emph{shifted} towards the point of maximal perceptual ambiguity between the two categories. Which of the two categories is shifted is manipulated between the two exposure groups. To ensure that the shifted stimuli are still recognized as instances of the intended category, exposure stimuli are typically lexically or visually \emph{labeled} (e.g., hearing the shifted sound /?dt/ embedded in the lexical context of ``croco\_ile'' would label the shifted sound as a /d/). In a subsequent test phase, both groups of participants then categorize the same test stimuli along an artificially generated acoustic continuum between the two categories. The goal of this test phase is to estimate participants' categorization function for the critical phonetic contrast, and so test stimuli are \emph{unlabeled}. This is achieved by presenting the manipulated continuum in the contexts of a lexical minimal pair (e.g., \emph{dip}-\emph{tip}) or a nonce-word pair (e.g., \emph{idi}-\emph{iti}).

For example, an influential study by \textcite{kraljic-samuel2006} exposed participants to either shifted /d/ and typical /t/ or shifted /t/ and typical /d/. Participants heard 20 typical and 20 shifted tokens (mixed with 160 fillers). Following exposure, participants in both conditions categorized non-word tokens (/\ipatext{ɪ}d\ipatext{ɪ}/-/\ipatext{ɪ}t\ipatext{ɪ}/) tokens along a six-step /d/-/t/ continuum. Figure \ref{fig:kraljic-samuel-2007-replotted} shows the results obtained by Kraljic and Samuel (2006).\footnote{We estimated the VOT range in Figure \ref{fig:kraljic-samuel-2007-replotted} based on the observation that post-burst aspiration accounts for about \(2/3^{rd}\)s of the stop durations reported in Kraljic and Samuel (2007, p.~6). We also averaged over the two talkers employed in the study, as Kraljic and Samuel did not report results relative to talker-specific VOT ranges.} Although relatively small, a significant perceptual recalibration effect was observed: participants in the /d/-shifted exposure condition were more likely to give ``d''-responses, resulting a categorization function that is shifted rightwards relative to participants in the /t/-shifted condition. This shift in the category boundary is the hallmark of perceptual recalibration experiments.



\begin{figure}

{\centering \includegraphics{../figures/knitted/kraljic-samuel-2007-replotted-1} 

}

\caption{Categorization functions observed during the test phase of the perceptual recalibration experiment presented in Kraljic and Samuel (2006; replotted from Kraljic \& Samuel, 2007, Figure 1). No error bars were provided in the original, and the original data are no longer available (Samuel, p.c.).}\label{fig:kraljic-samuel-2007-replotted}
\end{figure}

Perceptual recalibration continues to be one of the most frequently used paradigm in research on adaptive speech perception. A recent review cites over 200 studies using variants of this paradigm \autocite{theodore2021}. Boundary shifts like those in Figure \ref{fig:kraljic-samuel-2007-replotted} (though often larger) have been demonstrated for an increasing number of phonetic contrasts and languages \autocites[e.g.,][]{hanulikova-weber2012,kraljic-samuel2005,kraljic-samuel2006,reinisch2013,sumner2011,vroomen2007}. We now know that boundary shifts can occur for both isolated and connected speech \autocites[e.g.,][]{eisner-mcqueen2005,reinisch-holt2013}, even after fewer shifted tokens \autocites[e.g., as few as four,][]{liu-jaeger2018,liu-jaeger2019} or under cognitive load \autocites{baart-vroomen2010,zhang-samuel2014}[but see][]{samuel2016}, and that they can persist over hours and days \autocite{eisner-mcqueen2006,vroomen-baart2009,saltzman-myers2021} though they eventually decay \autocite{samuel2021,zheng-samuel2023}. More recent work has begun to identify the brain regions involved in perceptual recalibration, which range from primary auditory cortex and superior temporal cortices to more frontal and parietal areas \autocites{bonte2017,kilianhutten2011,myers-mesite2014,ullas2020,luthra2020a}[for review, see][]{guediche2014}.

While the signature boundary shift observed in perceptual recalibration experiments is often considered the consequence of representational changes, it is also possible that this finding is compatible with explanations in terms of either of the two alternative change mechanisms. We use ASP to assess this possibility. Specifically, we predict changes in categorization following the type of exposure employed in experiments on perceptual recalibration, while switching on and off the three different change models of ASP.

\hypertarget{data}{%
\subsection{Data}\label{data}}

Most published experiments on perceptual recalibration do \emph{not} measure and report the acoustic properties of their stimuli. We therefore generate data based on the same procedure that experimenters use to create and select the stimuli for perceptual recalibration experiments. The details of our stimulus generation approach, which closely follows the procedure described in \textcite{kraljic-samuel2006}, are described in the SI (\ref{sec:SI-PR}). These details do not affect the general results we present below. They are merely meant to provide a sufficiently concrete example. Readers can revisit any of the assumptions we made by downloading the \href{https://osf.io/q7gjp/}{R markdown document that this article is generated from}.

\hypertarget{exposure-phase}{%
\subsubsection{Exposure phase}\label{exposure-phase}}

Figure \ref{fig:PR-exposure-test-plot} shows the stimuli for the exposure and test phases of the experiment. During /d/-shifted exposure, listeners hear \texttt{20} lexically-labeled word recordings containing /d/ shifted towards /t/ and \texttt{20} lexically-labeled word recordings containing typical /t/, mixed in with word and non-word fillers for a total of 200 recordings. As is typical for perceptual recalibration experiments, these fillers are assumed not to contain any information about the VOT distributions of /d/ and /t/, and thus do not affect listeners' beliefs about those distributions. During /t/-shifted exposure, participants hear lexically-labeled \texttt{20} word recordings containing /t/ shifted toward /d/ and \texttt{20} lexically-labeled word recordings containing typical /d/. Shifted recordings are selected to be perceptually half-way between /d/ and /t/.



\begin{figure}

{\centering \includegraphics{../figures/knitted/PR-exposure-test-plot-1} 

}

\caption{Distribution of the exposure and test of the perceptual recalibration experiment. \textbf{Panel A - Exposure:} 20 tokens each of shifted and typical /d/ and /t/, respectively. Arrows indicate how the shifted tokens created by the experimenter differ from typical tokens of the same category (arrows originate from the mean of typical tokens and end at the mean of shifted tokens). As would be expected given the procedures researchers typically employ to generate stimuli for this type of experiment, the exposure distributions for /d/ and /t/ in both conditions differ primarily along VOT but, notably, \emph{also} differ along f0. \textbf{Panel B - Test (identical for both exposure groups):} Numbers indicate the item ID of the test tokens used in subsequent plots, with IDs in increasing order corresponding to 85\%, 65\%, 55\%, 45\%, 35\%, 15\% expected /d/-responses \emph{prior to exposure} (e.g., in a test-only norming experiment).}\label{fig:PR-exposure-test-plot}
\end{figure}

\hypertarget{test-phase}{%
\subsubsection{Test phase}\label{test-phase}}

The test phase consists of the six stimuli shown in Figure \ref{fig:PR-exposure-test-plot}B, chosen to yield 15\%, 35\%, 45\%, 55\%, 65\%, 85\% expected /d/-responses in a norming experiment without any prior exposure to the talker's speech. This closely resembles the test stimulus placement typical for perceptual recalibration experiments, which tend to examine tokens at the two ends of the continuum as well as those placed near the category boundary.

\hypertarget{results}{%
\subsection{Results}\label{results}}

Next, we use ASP to illustrate the extent to which changes in representations, response biases, or normalization can explain the typical behavioral signature of perceptual recalibration experiments: a shift in the category boundary in the same direction as the shifted tokens during exposure (optionally accompanied by a change in the slope of the categorization functions). The goal of these computational demonstrations is to illustrate the general type of pattern that the three different hypotheses can account for.

All three change models start from the same prior state. Specifically, we assume that listeners have acquired category representations with expected category means (\(\mathbf{E}(\mu_{/d/}), \mathbf{E}(\mu_{/t/})\)) and variances (\(\mathbf{E}(\sigma_{/d/}), \mathbf{E}(\sigma_{/t/})\)) that reflect the talker-normalized distributions in Chodroff and Wilson (2018; see Figure \ref{fig:show-representations-plots}). Prior to any exposure, decision-making is assumed to employ uniform response biases (\(\pi_{/d/}=\pi_{/t/}=.5\)). Here and throughout this article, we marginalize over the expected consequences of perceptual noise, attentional lapses, and decision-making (both for adaptation during exposure and for categorization during test). This can be seen as predicting the state of an `average' listener after exposure to a specific set of stimuli.\footnote{For any particular experiment, however, researcher have to \emph{estimate}, e.g., participants' categorization functions from their categorization responses. This is important to keep in mind, for example, when using ASP to estimate the statistical power of an experiment. To support predictive power simulations, the ASP functions provided on OSF also allow simulations that \emph{sample} the effects of perceptual noise, attentional lapses, and decision-making for adaptation and categorization. This allows simulations of trial-level behavior for individual participants.} The findings we present below do not qualitatively depend on these specific assumptions.

Finally, we note that our simulations predict categorization at the \emph{beginning} of the test phase, prior to repeated testing. This should be kept in mind when comparing our results against previous work, including the results of \textcite{kraljic-samuel2007} in Figure \ref{fig:kraljic-samuel-2007-replotted}. As is typical for experiments on perceptual recalibration, participants in Kraljic and Samuel's experiment heard the same six test tokens many times. In their visualizations and analyses, Kraljic and Samuel averaged participants' responses across these repetitions. While this approach continues to be the norm, it is now known to underestimate the true boundary shift, as repeated testing \emph{reduces} the effects of exposure \autocites{mitterer2011-perceptual-recalibration,liu-jaeger2018,liu-jaeger2019,scharenborg-janse2013,zheng-samuel2023}[for early discussion, see][p.~11]{norris2003}.

\hypertarget{changes-in-representations}{%
\subsubsection{Changes in representations}\label{changes-in-representations}}

We begin by modeling exposure-driven changes in the mapping from VOT and f0 to the /d/ and /t/ categories. This is arguably the mechanism that is most commonly assumed to underlie the boundary shift observed in perceptual recalibration experiments. Specifically, we use the \(\mathcal{NW}^{-1}\) ideal adaptor model described in Section \ref{sec:ideal-adaptor} while varying the \(\kappa_{c,0}\) and \(\nu_{c,0}\) parameters. We set the other two parameters of the \(\mathcal{NW}^{-1}\) ideal adaptor (\(\mathrm{m}\) and \(\mathrm{S}\)) so that they match the expected mean and covariance of the data in \textcite{chodroff-wilson2018} (as we did in Section \ref{sec:ideal-adaptor}). Normalization and response biases did not change based on exposure (\(\kappa_0 = \infty\); \(\beta_{\pi}=0\)), and attentional lapses were assumed to be zero (\(\lambda = 0\)).

Figure \ref{fig:PR-result-changes-in-representations} shows the predicted categorization functions after /d/-shifted and /t/-shifted exposure, depending on the strength of the prior beliefs for the category means and variances. We show the results for \(\kappa_{c,0}\)s and \(\nu_{c,0}\)s ranging from 1024 (very slow learning) to 4 (very fast learning). These values are best understood in the context of the number of critical exposure stimuli. Given 20 exposure tokens for each category, a \(\kappa_{c,0}=20\) would mean that the listeners' beliefs about the distribution of the category mean after exposure is a 50/50 mix of their beliefs prior to exposure and the mean of the exposure stimuli (see Equation \eqref{eq:niw-updating}). For \(\kappa_{c,0}=4\), listeners' beliefs about the category mean are thus primarily determined by the mean of the exposure stimuli, whereas a \(\kappa_{c,0}=1024\) essentially means that exposure does not affect listeners' belief about the category mean at all (and mutatis mutandis, for \(\nu_{c,0}\)).



\begin{figure}

{\centering \includegraphics{../figures/knitted/PR-result-changes-in-representations-1} 

}

\caption{Predictions of a learning model that derives perceptual recalibration as changes in category representations. Predicted categorization responses are shown for the 6 test tokens after /d/- and/t/-shifted exposure, depending on the strength of the prior beliefs in categories means (\(\kappa_{c,0}\)) and covariances (\(\nu_{c,0}\)). Smaller \(\kappa_{c,0}\) and \(\nu_{c,0}\) indicate \emph{faster} learning, weighting previous long-term experience less during the integration with the observations made during the exposure phase of the experiment (see Equation \eqref{eq:niw-updating}). The highlighted panel corresponds to the best-fitting \(\kappa_{c,0}\) and \(\nu_{c,0}\) observed in previous work within other types of paradigms \autocite{kleinschmidt-jaeger2016cogsci,kleinschmidt2020}.}\label{fig:PR-result-changes-in-representations}
\end{figure}

Figure \ref{fig:PR-result-changes-in-representations} demonstrates that distributional learning can account for the typical result of perceptual recalibration experiments. Of note, the range of parameterizations that best fit human responses in a series of distributional learning experiments on /b/ and /p/ \autocite[gray panel in Figure \ref{fig:PR-result-changes-in-representations}, \(\kappa_{/b/,0}=\kappa_{/p/,0}=160\) (95\% CI: 75-780) and \(\nu_{/b/,0}=\nu_{/p/,0}=510\) (95\% CI: 160-1000),][]{kleinschmidt2020} also provides a good qualitative fit against the /d/-/t/ perceptual recalibration results by Kraljic and Samuel \autocite*{kraljic-samuel2007}.

Figure \ref{fig:PR-result-changes-in-representations} further shows that boundary shifts can result from either changes in the beliefs about category means (small \(\kappa_{c,0}s\), e.g.~right column of Figure \ref{fig:PR-result-changes-in-representations}) or changes in the beliefs about category covariances (small \(\nu_{c,0}s\), e.g., bottom row in Figure \ref{fig:PR-result-changes-in-representations}). This replicates an observation made by Kleinschmidt and Jaeger \autocites*[p.~168]{kleinschmidt-jaeger2015}[see also][]{hitczenko-feldman2016,theodore-monto2019}, and highlights that ``category expansion'' and ``category shifts'' can both be seen as consequences of distributional learning \autocites[see also][]{bent-baeseberk2021,schertz-clare2020}. Figure \ref{fig:PR-result-changes-in-representations-categories} serves to further clarify how the different parameter settings for \(\kappa_{c,0}\) and \(\nu_{c,0}s\) affect the expected category likelihoods for a subset of the panels of Figure \ref{fig:PR-result-changes-in-representations}, leading to category shifts, expansion, shrinkage, or rotation, depending on both the model parameters and the input during exposure.



\begin{figure}

{\centering \includegraphics{../figures/knitted/PR-result-changes-in-representations-categories-1} 

}

\caption{Expected category likelihoods (based on expected category mean and covariance matrices) after exposure, depending on the exposure condition and the strength of the prior beliefs in categories means (\(\kappa_{c,0}\)) and covariances (\(\nu_{c,0}\)). Only a illustrative subset of the \(\kappa_{c,0}\) and \(\nu_{c,0}\) values from Figure \ref{fig:PR-result-changes-in-representations} are shown. Specifically, we compare scenarios with very slow changes in representations (first column), fast changes in beliefs about the category means (leading `category shifts', second and last column), and fast changes in beliefs about the category covariances (leading to `category expansions', `category shrinkage', and/or `category rotation', third and last column).}\label{fig:PR-result-changes-in-representations-categories}
\end{figure}

Finally, Figure \ref{fig:PR-result-changes-in-representations} shows that distributional learning not only predicts shifts in the category boundary but can also predict changes in the slope of those boundaries. This is evident, for example, in the top-right panel. Although such changes in slopes are often not discussed, they are present in many perceptual recalibration experiments \autocites[e.g.,][]{drouin2016,liu-jaeger2018,liu-jaeger2019,myers-mesite2014}.

\hypertarget{changes-in-decision-making}{%
\subsubsection{Changes in decision-making}\label{changes-in-decision-making}}

Next, we model the effects of changes in decision-making. Such changes have rarely been considered as an explanation for boundary shifts \autocite[but see][]{clarkedavidson2008}. In addition to varying the rate at which response biases change \(\beta_{\pi}\), we also vary the lapse rate parameter \(\lambda\). Normalization and representations did not change based on exposure (\(\kappa_0 = \kappa_{c,0} = \nu_{c,0} = \infty\)). Since the change model for decision-making is sensitive to the order of stimuli during exposure (see Section \ref{sec:change-bias}), each panel of the figure averages over 50 simulations, each of them randomizing the order of exposure. We confirmed that this number of simulations was more than sufficient to achieve reproducible estimates for the range of \(\beta_{\pi}\)s considered here (the order-sensitivity of the change model increases for faster change rates, i.e., larger \(\beta_{\pi}\)).



\begin{figure}

{\centering \includegraphics{../figures/knitted/PR-result-changes-in-decision-making-1} 

}

\caption{Predictions of a model that derives perceptual recalibration as changes in decision-making. Predicted categorization responses are shown for the 6 test locations after /d/- and/t/-shifted exposure, depending on the rate at which response biases change (\(\beta_{\pi}\)) and the rate of attentional lapses (\(\lambda\)). The response biases towards /d/ that results from \(\beta_{\pi}\) are shown in the bottom row for each of the two exposure conditions, and are identical within each column (when all trials are lapses, as in the bottom row, the response distribution is identical to the response biases).}\label{fig:PR-result-changes-in-decision-making}
\end{figure}

Figure \ref{fig:PR-result-changes-in-decision-making} shows the effects of changes in response biases for lapse rates ranging from negligible lapse rates (\(\lambda = .0005\), top row) to a scenario in which listeners always lapse (\(\lambda = 1\)). In the latter---rather implausible---edge case, listeners' responses only reflect response biases and are completely stimulus-independent (bottom row). For reference, lapse rates for perception experiments like the ones modeled here typically fall between 1-10\% \autocites[e.g.,][ constrained lapse rates to be \(<5\)\%]{clayards2008}[ report a best-fitting value of 5\%]{kleinschmidt-jaeger2016cogsci}.

The primary insight from Figure \ref{fig:PR-result-changes-in-decision-making} is that changes in response biases can account for the type of boundary shift that is observed in perceptual recalibration experiments. This shows that the signature result of perceptual recalibration experiments is ambiguous between two explanations that evoke very different cognitive architectures. For this case study, perhaps the closest match to the findings from Kraljic and Samuel (2006) is observed for plausible lapse rates up to 5\% and small changes in response bias (second or third column, top rows).

Figure \ref{fig:PR-result-changes-in-decision-making} also shows that changes in response biases do not necessarily result in effects that are symmetric around the uniform response biases assumed to hold prior to the experiment (\(\pi_{/d/}=\pi_{/t/}=.5\)). For example for \(\beta_{\pi} = .8\), the /d/-shifted condition results in a /d/-bias of \(\pi_{/d/}=.97\), whereas /t/-shifted condition results in \(\pi_{/d/}=.16\) (\(\neq 1 - .97\)). This type of asymmetry is a consequence of the assumption that listeners change their response biases only to the extent that their categorization is in conflict with the category label indicated in the input (e.g., when the listener hears a /t/ but the lexical context is \emph{lemona\_e}). This means that the degree to which listeners change their response biases depends on the degree to which the acoustic properties of the exposure stimuli are in conflict with the lexical contexts in which they appear.

Furthermore, Figure \ref{fig:PR-result-changes-in-decision-making} illustrates the effects of lapse rates. The higher the lapse rate, the more strongly response biases, rather than stimulus-dependent aspects, determine the overall response pattern. This also means that it is important to keep lapse rates in mind when analyzing behavioral data. Consider, for example, a scenario in which listeners' lapse rates differ after /d/-shifted and /t/-shifted exposure---e.g., because one exposure condition resulted in a more difficult or less engaging task, or in less plausible stimuli. Compare, say, the blue line for \(\beta_{\pi} = .05\), \(\lambda = .005\) against the red line for \(\beta_{\pi} = .05\), \(\lambda = .5\). Analyses that fail to take into account the difference in lapse rates would wrongly conclude that the category representations have changed since the slopes of the categorization functions resulting from the two exposure conditions seem to differ.

\hypertarget{changes-in-normalization}{%
\subsubsection{Changes in normalization}\label{changes-in-normalization}}

Finally, we compare a model that normalizes test tokens based on the phonetic inputs experienced during exposure against a model that continues to apply normalization based on previous long-term experience. Essentially, the model that changes normalization based on the exposure tokens subtracts the cue means experienced during exposure---which varies based on the exposure condition---from that of each of the test tokens. We implement this by adjusting the cue values of the test tokens by the difference between the prior mean of cues based on previously experienced input (estimated in the SI, \ref{sec:SI-chodroff}) and the mean of cues experienced during exposure. All other parameters have the same setting as in the previous sections, except that no changes in representations or response biases are considered.

To illustrate the consequences of normalization, we vary the only parameter of our change model for normalization, \(\kappa_0\) (i.e., strength of prior beliefs about cue mean). Figure \ref{fig:PR-test-normalization} illustrates the effect of normalization on the cue values of the test tokens. Figure \ref{fig:PR-result-changes-in-normalization} shows the results of different speed in the changes of normalization. Paralleling the results for changes in representations and response biases, changes in normalization can account for the signature boundary shift of perceptual recalibration experiments.



\begin{figure}

{\centering \includegraphics{../figures/knitted/PR-test-normalization-1} 

}

\caption{Effects of changes in normalization on the perception of test locations, depending on the relative weighting of previous experience (\(\kappa_0\)) during the inference of the cue mean during exposure. Normalization based on exposure results in a shift in the perception of the test stimuli. The magnitude of that shift depends on \(\kappa_0\), with larger shifts (more learning) for smaller \(\kappa_0\) (see Equation \eqref{eq:normalization-change}).}\label{fig:PR-test-normalization}
\end{figure}



\begin{figure}

{\centering \includegraphics{../figures/knitted/PR-result-changes-in-normalization-1} 

}

\caption{Predictions of a model that derives perceptual recalibration from changes in normalization. Predicted categorization responses are shown for the 6 test locations after /d/- and/t/-shifted exposure, depending on the relative weighting of previous experience (\(\kappa_0\)) during the inference of the cue mean during exposure. Smaller \(\kappa_0\) indicate faster learning.}\label{fig:PR-result-changes-in-normalization}
\end{figure}

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

We applied the three change models developed in Section \ref{sec:framework} to a (simulated) perceptual recalibration experiment. We found that all three change models can qualitatively predict the type of boundary shift characteristic of such experiments. To us, and we imagine to many other researchers, this finding is surprising. On the one hand, all three change models were designed to explain adaptive speech perception. As such, it is not surprising that all three models predict changes in the `right' direction, compared to the absence of an adaptive response---i.e., that all models change the categorization function in a way that is meant to accommodate the acoustic properties of the exposure stimuli.\footnote{That the predicted changes in categorization functions indeed \emph{improve} recognition accuracy will become more evident in Case Study 2.} On the other hand, the three change mechanisms assume very different cognitive architectures, and each of the three change models is subject to different computational limitations. Specifically, the two simpler change models for normalization and decision-making involve fewer degrees of freedom, and have access to less information, compared to the change model for category representations. Recall, for example, that normalization affects cue values regardless of category membership, or that changes in decision-making can only predict additive changes in the log-odds of categorization responses (when listeners have lapse rates of 0 or 1).

Given these differences between the models, it seemed plausible that the simpler models would have insufficient functional flexibility to account for the types of boundary shifts associated with perceptual recalibration. Case Study 1 suggests that this is not the case. At least at the level of analysis that is typically applied in these experiments, the signature result of perceptual recalibration experiments therefore does \emph{not} constitute decisive evidence for changes in representations. Rather, boundary shifts could result from any of the three types of change mechanisms. This does not, of course, mean that perceptual recalibration experiments are \emph{necessarily} uninformative about the mechanisms underlying adaptive speech perception. To anticipate the general discussion, if adequate approaches to data analysis are chosen, it is possible to distinguish between the three different change mechanisms (and even combinations of them). Before we discuss what would be required to achieve this goal, we present the second case study using another common experimental paradigm.

\hypertarget{sec:AA}{%
\section{Case Study 2: accent adaptation}\label{sec:AA}}

The second paradigm we consider focuses on \emph{naturally} accented speech---e.g., dialectal \autocite{smith2014}, varietal \autocite{shaw2018}, or second language (L2) accents \autocite{bradlow-bent2008,eisner2013,sidaras2009,weil2001a}. Typically, exposure to an unfamiliar accent is compared to a control condition in which listeners are exposed to a familiar accent, most often the `standard' variety of listeners' first language (L1). Following exposure, listeners in either group are tested on the unfamiliar accent. In an influential study, \textcite{bradlow-bent2008} had listeners transcribe a total of 160 sentences of either Mandarin-accented US English or L1-accented US English, distributed over two sessions on two separate days. In a subsequent test phase, both groups transcribed Mandarin-accented sentences. Participants who were first exposed to Mandarin-accented US English were significantly more accurate during test (over 90\% accuracy compared to about 80\%). This finding has since been replicated and extended \autocite[for review, see][]{baeseberk2020}. We now know that substantially shorter exposure can lead to similarly large improvements in accuracy \autocite[e.g., 80 sentences in a single session, about 2-5 minutes of speech,][]{xie2021jep}, that these can persist over hours and days \autocite{witteman2015,xie2018lcn}, and that accent adaptation can sometimes generalize across talkers of the same or similar accents \autocites[e.g.,][]{baeseberk2013,tzeng2016,xie2021jep}.

As in the case of perceptual recalibration, accent adaptation is often attributed to changes in category representations \autocites[e.g.,][]{bent-baeseberk2021,sidaras2009,eisner2013,sumner2009,tzeng2016,xie2016jep}. This might in part be due to the intuition that the other change mechanisms are not sufficiently flexible to explain adaptation to complex differences between accents. For example, a common assumption seems to be that changes in decision/response biases can only explain \emph{trade-offs} in accuracy: as the accuracy for one category improves, it has to inevitably decrease for all other categories. Under this trade-off assumption (which we show below to be false), changes in decision-making could not possibly explain \emph{overall} improvements in recognition accuracy. Similarly, pre-linguistic normalization is rarely considered a plausible mechanism for accent adaptation. However, previous work has never actually put these intuitions to a test. Case Study 2 provides this test.

Compared to perceptual recalibration paradigms, experiments on accent adaptation exhibit more heterogeneity in their designs and tasks. Case Study 2 focuses on experiments in which treatment exposure employs a single talker with an unfamiliar accent, and the test phase heard by both groups of participants employs previously unheard stimuli from the same accented talker \autocites[e.g.,][]{eisner2013,schertz2015,xie2017}.\footnote{The approach we take here can also be applied to cross-talker generalization---i.e., paradigms in which speech from different talkers of the same (or different) accent as the exposure talker is used during test \autocite{baeseberk2013,bradlow-bent2008,sidaras2009}.} For example, in an exposure-test experiment, \textcite{xie2016jep} exposed two groups of L1-US English listeners to Mandarin-accented US English speech. In exposure, participants in the treatment group heard 30 words ending in /d/ (e.g., \emph{overload}) together with 60 word fillers and 90 nonword fillers. Participants in the control group did not hear any words that contain /d/. After exposure, participants in both conditions categorized recordings of 60 minimal /d/-/t/ pairs (e.g., \emph{seed} vs.~\emph{seat}) spoken by the same Mandarin-accented talker. Participants who heard /d/-final words during exposure were more likely to categorize /d/-final words correctly during test, compared to the control condition. At the same time, a non-significant numerical \emph{de}crease in accuracy was observed for /t/-final words \autocite[see also][]{xie2018lcn}. Related paradigms have yielded similar results for other contrasts and other L1-L2 pairs \autocites[e.g.,][]{zheng-samuel2020,eisner2013}.

For Case Study 2, we construct a hypothetical experiment that closely follows this type of design. For the sake of continuity, we focus on the same syllable-initial /d/-/t/ contrast used in Case Study 1, simulating exposure to Korean-accented L2 English. Like in Case Study 1, we analyze the data following the conventions of the field. For experiments on accent adaptation, this typically means that the facilitatory effects of L2-accented exposure are assessed through improvements in categorization accuracy during test. As in Case Study 1, we ask whether any of the three change mechanisms can qualitatively explain the signature results found in \textcite{xie2016jep} and other similar work. As in any experiment on accent adaptation, the results of our simulated experiment are expected to depend on the acoustic-phonetic characteristics of both the L1 and the L2 accent. In the discussion of Case Study 2, we thus consider other common differences between L1 and L2 accents, and show how these accent properties are expected to affect the three different change models.

\hypertarget{data-1}{%
\subsection{Data}\label{data-1}}

The stimulus generation procedure is described in detail in the SI (\ref{sec:SI-AA}).

\hypertarget{exposure-phase-1}{%
\subsubsection{Exposure phase}\label{exposure-phase-1}}

Figure \ref{fig:study-AA-exposure-test-plot}A shows the stimuli for the exposure and test phases of the experiment. In the L2-accented exposure condition, listeners hear word recordings containing L2-accented initial /d/ and /t/ (30 tokens per category). In the control condition with L1-accented exposure, listeners hear the same words but from an L1-accented talker.\footnote{The use of L1-accented exposure, rather than L2-accented exposure without /d/, as control follows a typical design in accent adaptation studies \autocite[e.g.,][]{bradlow-bent2008}, rather than \textcite{xie2016jep}. Xie and colleagues instead employed L2-accented exposure without /d/, using the same L2-accented talker as during test \autocite[see also][]{eisner2013}. For the present purpose, both types of control conditions lead to identical predictions (as long as the L2-accented control exposure successfully avoids conveying non-negligible amount of information about the categories considered during the test phase) since the current implementation of the three change models do not consider talker-switch costs.} For L1-accented exposure, the category likelihoods were set to match those observed in \textcite{chodroff-wilson2018}, after C-CuRE normalization (i.e., the distributions shown in Figure \ref{fig:demonstrate-normalization}B). This follows the same approach we took for the typical tokens in Case Study 1. For L2-accented exposure, /d/ and /t/ categories were created following the VOT and f0 statistics of Korean-accented US English reported in \textcite{schertz2015}. The /d/ and /t/ categories in this L2 accent differ from L1-accented US English in both category means and variances. First, the VOTs of both stop categories are longer in the L2 accent, making the /d/s more similar to /t/s for native-English listeners who rely on VOT as a primary cue to voicing distinction. As a result, the recognition accuracy is expected to be lower for /d/ than for /t/ for the L1-accented exposure condition. Second, the L2-accented categories exhibit the same ordering along f0 as in L1-accented US English (lower f0 for /d/ than for /t/) but this difference along f0 is enhanced in Korean-accented US English. This makes syllable-initial stop voicing in Korean-accented US English a representative case of cue reweighting, a common difference between L2 and L1 accents that has received much attention in research on accent adaptation \autocites[e.g.,][]{escudero2009,kim2020,schertz2016}[see also][]{idemaru-holt2011,idemaru-holt2020,harmon2019}.

\hypertarget{test-phase-1}{%
\subsubsection{Test phase}\label{test-phase-1}}

During test, listeners from both exposure conditions hear the same recordings from the same L2-accented talker used in the L2-accented exposure condition. 60 test tokens are randomly sampled from the distribution of each L2-accented category, half from the /d/ category and half from the /t/ category (Figure \ref{fig:study-AA-exposure-test-plot}B).



\begin{figure}

{\centering \includegraphics{../figures/knitted/study-AA-exposure-test-plot-1} 

}

\caption{\textbf{Panel A - Exposure:} Distribution of the stimuli used during the exposure phase of the accent adaptation experiment (30 tokens each of L1-accented and L2-accented /d/ and /t/, respectively). \textbf{Panel B - Test:} Distribution of the stimuli used during the test phase of the accent adaptation experiment. The test tokens come from L2-accented speech (60 tokens per category) and are identical for the two exposure conditions. Ellipses show the 95\% probability mass for the two categories in L2-accented exposure speech.}\label{fig:study-AA-exposure-test-plot}
\end{figure}

\hypertarget{results-1}{%
\subsection{Results}\label{results-1}}

\begin{table}

\caption{\label{tab:AA-table-models-parameter-setting}Parameter range considered for each change model}
\centering
\begin{tabular}[t]{lll}
\toprule
Model & Parameters & Range\\
\midrule
Changes in decision-making & $\lambda$ & 0 to 1\\
\cmidrule{1-3}
 & $\beta_\pi$ & 0.001 to 100\\
\cmidrule{1-3}
Changes in representations & $\kappa_{c,0}$ & 1 to 10000\\
\cmidrule{1-3}
 & $\nu_{c,0}$ & 4 to 10000\\
\cmidrule{1-3}
Changes in normalization & $\kappa_0$ & 1 to 10000\\
\bottomrule
\end{tabular}
\end{table}

Paralleling Case Study 1, we implement the three change models to predict L1 listeners' response behavior in this simulated accent adaptation experiment. Specifically, we evaluate for each change model whether it can explain the two types of signature results observed in, for example, Xie et al. \autocite*{xie2016jep}: (1) with L1-accented exposure, /d/ test tokens are recognized with lower accuracy than /t/ test tokens, since the categories are shifted towards /t/ along the primary cue VOT, creating a deviation from L1 listeners' expectations; and (2) L2-accented exposure should lead to a significant increase in recognition accuracy for /d/ test tokens without a corresponding decrease in the accuracy for /t/ test tokens, resulting in an overall increase in accuracy.

As in Case Study 1, we first model changes in representations, and then compare them against changes in decision-making and normalization. We consider a wider range of parameterizations than in Case Study 1, because the models with the highest accuracy for L2-accented exposure (henceforth, the best-performing parameterization) in some cases fell outside of the parameter ranges considered in Case Study 1. These best-performing parameterizations were determined by bounded quasi-Newton optimization of the recognition accuracy during test after L2-accented exposure \autocite[implemented in function \texttt{optim()} in R]{byrd1995}, and are indicated in all result plots we present below. Table \ref{tab:AA-table-models-parameter-setting} summarizes the parameter ranges considered by the optimization algorithm \autocite[\(\nu_{c,0}\) must be larger than the number of cues \(k\) + 1,][p.~134]{murphy2012}.\footnote{In Case Study 1, there was no measure like accuracy for which performance can unambiguously be interpreted as `better' or `worse'.} We emphasize that we use the term ``best-performing'' to refer to the parameterization that achieves the highest recognition accuracy for L2-accented test tokens, rather than the best fit against human responses in a perception experiment. In the general discussion, we describe how the same approach can be used to fit change models to the results of perception experiments.

\hypertarget{changes-in-representations-1}{%
\subsubsection{Changes in representations}\label{changes-in-representations-1}}



\begin{figure}

{\centering \includegraphics{../figures/knitted/AA-result-changes-in-representations-1} 

}

\caption{Predictions of a learning model that derives accent adaptation as changes in category representations. Predicted categorization accuracies for the L2-accented test tokens after L1-accented and L2-accented exposure, as a function of the strength of the prior beliefs in category means (\(\kappa_{c,0}\)) and covariances (\(\nu_{c,0}\)). The average accuracy across /d/ and /t/ is shown above the bars for each exposure condition. Error bars show 95\% bootstrapped confidence intervals. The highlighted panel is the one closest to the best-performing parameterization (\(\kappa_{0,c} = 1\); \(\nu_{c,0}=4\); overall accuracy \(=0.93\)).}\label{fig:AA-result-changes-in-representations}
\end{figure}

Figure \ref{fig:AA-result-changes-in-representations} shows the predicted categorization accuracy after exposure to L1- or L2-accented speech, as a function of the strength of the prior beliefs for the category means and variances. First consider the top left panel, where both \(\kappa_{c,0}\)s and \(\nu_{c,0}\) have very high values. This simulates a very slow learner who has strong prior beliefs for both category means and variances such that the small number of tokens heard during exposure phase have minimal influence on subsequent perception of the L2-accented test tokens. Therefore, the performance from both L1- and L2-accented exposure conditions are more or less identical, reflecting how a listener would categorize based on their long-term prior experience predominantly.

As \(\kappa_{c,0}\)s and \(\nu_{c,0}\) get smaller (indicating weaker prior beliefs), the effects of exposure become more noticeable and both of the signature results of experiments on accent adaptation begin to emerge. First, for L1-accented exposure, the accuracy is always predicted to be lower for /d/ than for /t/, indicating that /d/ is often misheard as /t/. Second, L2-accented exposure is predicted to improve recognition accuracy of /d/ compared to L1-accented exposure, and this improvement occurs without corresponding decreases in the recognition accuracy of /t/. Looking across the range of \(\kappa_{c,0}\)s and \(\nu_{c,0}\)s, we make two more observations. First, for the scenario studied here, faster learning of category means improves recognition accuracy. When listeners' beliefs about category covariances are held constant (i.e., looking within each row), smaller \(\kappa_{c,0}\) values yield higher categorization accuracy. Second, faster updating of beliefs about the \emph{(co)variance} also leads to accuracy improvements, although these improvements are less pronounced. Both results follow from the way that the Korean-accented stimuli differ from the L1 accented stimuli: as shown in Figure \ref{fig:study-AA-exposure-test-plot}, the category means of Korean-accented /d/ and /t/ are shifted relative to the L1-accented stimuli. In comparison, differences in category (co)variances are relatively subtle between the two sets of stimuli. For the present exposure stimuli, adapting to the L2 accent thus primarily hinges on learning the new category means. Taken together, these results demonstrate how changes in representation can explain the types of results that are considered the signature of accent adaptation.

\hypertarget{changes-in-decision-making-1}{%
\subsubsection{Changes in decision-making}\label{changes-in-decision-making-1}}



\begin{figure}

{\centering \includegraphics{../figures/knitted/AA-result-changes-in-decision-making-1} 

}

\caption{Predictions of a model that derives accent adaptation as changes in decision-making. Predicted categorization responses for the test tokens after L1-accented and L2-accented exposure, depending on the rate at which response biases change (\(\beta_{\pi}\)) and the rate of attentional lapses (\(\lambda\)). The highlighted panel is the one closest to the best-performing parameterization (\(\beta_{\pi} = 0.34\); \(\lambda=1.9e-30\); overall accuracy\(=0.91\)).}\label{fig:AA-result-changes-in-decision-making}
\end{figure}

Figure \ref{fig:AA-result-changes-in-decision-making} shows the effects of changes in decision-making as a function of the rate at which response biases change \(\beta_{\pi}\) and the lapse rate \(\lambda\), averaged across all simulations for each parameterization. As in Case Study 1, the figure shows the average across 50 repeated simulations, each time randomizing the order of exposure. This was sufficient to achieve reproducible estimates even for the largest \(\beta_{\pi}\)s considered here (SEs of the mean accuracy after L2-accented exposure across repeated simulations were smaller than \(0.002\), meaning that the 95\% CI across the simulations spanned less than 1\% in recognition accuracy).

For L1-accented exposure, we again see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens, regardless of the specific value of (\(\beta_{\pi}\)). The second signature result---improved accuracy after L2-accented exposure for /d/---is obtained for a sufficiently fast learning rate \(\beta_{\pi}\). As \(\beta_{\pi}\) increases, the overall recognition accuracy first increases and then plateaus. The largest improvements are found for the smallest lapse rates and relatively fast learning rates (gray panel in Figure \ref{fig:AA-result-changes-in-decision-making}).

Importantly, Figure \ref{fig:AA-result-changes-in-decision-making} shows that changes in response biases do not necessarily result in the type of zero-sum trade-off that is sometimes attributed to them---accuracy increasing by some degree for one category while decreasing by the same degree for the other category. Rather, L2-accented exposure can predict improvements in the \emph{overall} recognition accuracy across both categories (e.g., top right panel). Zero-sum trade-offs are only expected under the highly implausible assumptions that responses are entirely independent of stimulus properties (bottom row of Figure \ref{fig:AA-result-changes-in-decision-making}). Indeed, for the specific L1-L2 accent combination considered here, the largest improvements observed for changes in decision-making are similar in magnitude to those observed for the representational change model, despite the fact that the former change model is computationally simpler.

\hypertarget{changes-in-normalization-1}{%
\subsubsection{Changes in normalization}\label{changes-in-normalization-1}}

Finally, we compare models that normalize test tokens based on the phonetic input experienced during exposure against models that continue to apply normalization based on prior long-term L1 experience. Figure \ref{fig:AA-result-changes-in-normalization} shows the predicted categorization accuracy following changes in normalization in the L1- and L2-accented exposure conditions. We again obtain both of the signature results of experiments on accent adaptation. For L1-accented exposure, we continue to see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens. For L2-accented exposure, recognition accuracy is improved for /d/ test tokens at no cost to recognition accuracy for /t/. That is, like the other two change models, normalization can predict \emph{overall} improvements in recognition accuracy after L2-accented exposure. Indeed, for the specific L1-L2 accent combination considered here, the computationally simple model for changes in normalization yields improvements similar to those predicted by changes in category representations---similar to the results for changes in decision-making.



\begin{figure}

{\centering \includegraphics{../figures/knitted/AA-result-changes-in-normalization-1} 

}

\caption{Predictions of a model that derives accent adaptation from changes in normalization. Predicted categorization responses for the test tokens after L1-accented and L2-accented exposure, depending on the relative weighting of previous experience (\(\kappa_0\)) during the inference of the cue mean during exposure. The highlighted panel is the one closest to the best-performing parameterization (\(\kappa_0 = 1\); overall accuracy \(=0.92\)).}\label{fig:AA-result-changes-in-normalization}
\end{figure}

\hypertarget{summary-1}{%
\subsection{Summary}\label{summary-1}}

Paralleling Case Study 1 on perceptual recalibration, we find that any of the three change mechanisms can qualitatively explain the signature results of experiments on accent adaptation. Even computationally simple, non-representational mechanisms---normalization and decision-making---can explain improvements in the perception of L2 accents that differ in complex ways from L1 listeners' prior expectations. And, contrary to common assumptions, changes in decision-making can lead to \emph{overall} improvements in recognition accuracy, not just zero-sum trade-offs. Thus, overall improvements in accuracy after exposure to an unfamiliar accent are thus \emph{not} diagnostic of listeners' adaptation to L2-specific category realizations. This supports the conclusion of Case Study 1: at the level of analysis that is commonly applied in previous work (and thus here), experiments on accent adaptation do not necessarily rule out any of the three mechanisms as driving the benefits of exposure.

For the current hypothetical L2 accent considered in Case Study 2, the three models are not only similar in their qualitative predictions, but they also yield highly comparable quantitative results in terms of the maximal benefits of L2-accented exposure (compare the highlighted panels across Figure \ref{fig:AA-result-changes-in-representations}-\ref{fig:AA-result-changes-in-normalization}). This is not always expected to be the case. A critical insight of ASP is that the specific predicted benefit of L2-accented exposure under each mechanism depends on the statistics of the exposure and test stimuli, relative to L1 listeners' prior expectations. For experiments using naturally accented exposure and test tokens, this also means that the results are predicted to depend on the specific ways in which the acoustic-phonetic distributions of an L2 accent deviate from those of the L1 accent.

To further illustrate this point, we simulated three additional scenarios of adaptation to an L2 accent (Panel A in Figure \ref{fig:AA-plot-results-additional-cases}). Each of these scenarios is representative of commonly attested differences between L1- and L2-accents. The top row of Figure \ref{fig:AA-plot-results-additional-cases} shows an example of \emph{contrast reduction}, in which the L2 accent shows a greater category overlap than the L1 accent. Here it is simulated by shifting /d/ category towards /t/ along VOT while keeping the latter unchanged \autocite[as qualitatively attested for, e.g., vowels in Spanish-accented English,][]{wade2007}. The middle row shows an example of \emph{contrast shift}, where two categories in L2-accented speech are shifted along one or more of the cue dimensions as compared to L1-accented speech. Here it is simulated by shifting both /d/ and /t/ categories towards lower VOT \autocite[qualitatively attested for, e.g., word-initial stops in French-accented English,][]{Sumner2011a}. The bottom row of Figure \ref{fig:AA-plot-results-additional-cases} shows a more extreme example, exhibiting almost complete \emph{contrast collapse} \autocite[similar to the loss of the {[}s{]}-{[}\ipatext{θ}{]} contrast that can occur in Mandarin-accented English,][]{zheng-samuel2020}. Here it is simulated by making /d/ identical to /t/.



\begin{figure}

{\centering \includegraphics{../figures/knitted/AA-plot-results-additional-cases-1} 

}

\caption{Predicted adaptation for three types of L2 accents, from top to bottom: contrast reduction, contrast shift and contrast collapse. Predictions were derived for one random experiment of the same number of exposure and test stimuli as in Case Study 2. Multiple simulations were performed following the same procedure described in the main scenario above. \textbf{Panel A - Distributions of L1-accented and L2-accented categories:} L1-accented categories, represented in light shades, are kept constant across the three scenarios and identical to that in the main scenario above; L2-accented categories, represented by solid ellipses, are varied across scenarios. \textbf{Panel B - Change model predictions:} Predicted categorization accuracies for the L2-accented test tokens after L1-accented and L2-accented exposure, for the best-performing parameterizations of each change model (cf.~highlighted panels in Figure \ref{fig:AA-result-changes-in-representations}-\ref{fig:AA-result-changes-in-normalization}). The average accuracy across all test tokens for each condition is shown above the bars. Error bars show 95\% bootstrapped confidence intervals.}\label{fig:AA-plot-results-additional-cases}
\end{figure}

\begin{table}

\caption{\label{tab:AA-table-best-performing-parameterization}Best-performing parameterizations across different accent scenarios (rounded). Recall that best-performing here refers to the parameterization that achieves the highest accuracy on L2-accented test tokens, rather than best quantitative fit against human data (which we do not have).}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{llll}
\toprule
L2 accent scenario & Changes in representations & Changes in decision-making & Changes in normalization\\
\midrule
Cue reweighting & $\kappa_{c,0}$ = 1, $\nu_{c,0}$ = 4 & $\lambda$ = 0, $\beta_\pi$ = 0.34 & $\kappa_0$ = 1\\
Contrast reduction & $\kappa_{c,0}$ = 19, $\nu_{c,0}$ = 10000 & $\lambda$ = 0.17, $\beta_\pi$ = 5.45 & $\kappa_0$ = 1\\
Contrast shift & $\kappa_{c,0}$ = 1, $\nu_{c,0}$ = 10000 & $\lambda$ = 0.51, $\beta_\pi$ = 0.31 & $\kappa_0$ = 1\\
Contrast collapse & $\kappa_{c,0}$ = 28, $\nu_{c,0}$ = 1652 & $\lambda$ = 0, $\beta_\pi$ = 0.08 & $\kappa_0$ = 5000\\
\bottomrule
\end{tabular}}
\end{table}

Figure \ref{fig:AA-plot-results-additional-cases}B provides the predictions of the best-performing parameterization for each change model (listed in Table \ref{tab:AA-table-best-performing-parameterization}) that yields the maximal benefit for the L2-accented exposure condition, following the same procedures described for the cue-reweighting case described above. The three accent scenarios differ in terms of levels of difficulties they are predicted to pose for L1 listeners \emph{prior to} exposure (see L1-accented exposure condition in Panel B of Figure \ref{fig:AA-plot-results-additional-cases}). Contrast reduction (top row) and contrast collapse (bottom row), for example, resemble the scenario considered in Case Study 2: without informative exposure, L1 listeners unfamiliar with the accent are expected to struggle with the recognition of /d/ than the recognition of /t/. The opposite is observed for our example of contrast shift (middle row). This illustrates how differences in the realization of L2 vs.~L1 accents can lead to different perceptual consequences for L1 listeners.

The three scenarios also differ in the maximal benefits that L1 listeners can gain from L2 accent exposure. Three aspects are worth noting. First, corroborating the point we raised above, changes of representations do not always outperform the other two mechanisms. For the case of contrast reduction (top row), changes in decision-making and normalization perform as well as, or better than, changes in category representations. This underscores our main conclusion from Case Studies 1 and 2 that the computational complexity afforded for the representational change model does not always result in higher recognition accuracy than the other two mechanisms.

Second, the same change model can support more or less overall accuracy improvements (as gleaned from comparing the panels within each column), depending on the specific differences between the L2 and L1 accents. Consider the case of contrast shift (middle row). While changes in representations and normalization both predict large improvements in overall recognition accuracy in the L2-accented exposure, changes in decision-making do not (i.e., the difference in the predicted accuracy across L1- and L2-accented exposure conditions is negligible). This outcome aligns with the zero-sum trade-offs often attributed to this change mechanism (i.e., the recognition of /d/ improves at the cost of the recognition of /t/). The heterogeneity of the results across the three scenarios makes it clear that the trade-off is not an inherent property of the decision-making mechanism. It is instead a possible outcome arising from a combination of the specific change model and properties of the L2 accent (or L1-L2 accent differences).

Third, \emph{none} of the change models predicts significant benefits from L2-accented exposure for contrast collapse (bottom row). This highlights an important, yet often under-appreciated, fact: sometimes null results are \emph{predicted} for exposure-test experiment, because of the specific stimulus statistics \autocites[see][]{tan2021,zheng-samuel2020}. Put differently, an absence of improvements does not \emph{necessarily} constitute evidence against accent adaptation or any of the three mechanisms.

While the scenarios considered here cover only a small subset of the cross-accent differences that exist in the world, they clearly illustrate a general challenge: different types of cross-talker differences (including L2 accents) pose different type of challenges to listeners. This is also reflected in the fact that the best-performing parameterizations vary across the different accent scenarios (see Table \ref{tab:AA-table-best-performing-parameterization}). It follows that, depending on the exact nature and sources of the difficulties, listeners would benefit from different \emph{mechanisms} and \emph{rates} of adaptation. Since the properties of unfamiliar accents are not \emph{a priori} known to listeners, this means that the optimal level of flexibility is achieved if the neural and cognitive systems underlying adaptive speech perception can adjust change rates based on the properties of the input \autocite[see discussion of the trade-off between flexibility and stability in][pp.~180-182]{kleinschmidt-jaeger2015}.

In summary, the expected benefits of L2-accented exposure under each change mechanism depend on the specifics of the L2-accented categories. Beyond the accent scenarios examined here (cue-reweighting, contrast reduction, contrast shift, and contrast collapse), naturally produced L2 accents exhibit complex variations, which can pose difficulties to L1-listeners and researchers alike. At the same time, these differences may provide a useful opportunity to contrast the three change mechanism or even to investigate their \emph{relative} engagement in adaptive speech perception. Since the three change mechanisms would predict different perceptual outcomes depending on the properties of the exposure and test stimuli, differences between accents can be natural testbeds for discriminating between model predictions. In the general discussion, we explore how such tests can be accomplished.

\hypertarget{sec:general-discussion}{%
\section{General discussion}\label{sec:general-discussion}}

The two case studies we have presented together provide two important insights. First, the signature results from two influential lines of research---often taken to lend support to changes in category representations---are actually compatible with computationally more parsimonious change mechanisms (pre-linguistic signal normalization and changes in post-perceptual decision-making). Additional case studies not reported here suggest that this finding is likely to generalize at least partly to other types of exposure-test paradigms, such as distributional learning paradigms over a single phonetic cue \autocite{clayards2008,kleinschmidt-jaeger2016cogsci} and dimension-based statistical learning paradigms that manipulate the relative informativity of cues \autocite{idemaru-holt2011,idemaru-holt2020}.\footnote{There are, however, important differences in the extent to which different research traditions have already implemented the recommendations we make below. For example, some lines of research tend to interpret findings relative to the distribution of phonetic cues in the speech input \autocites[e.g., studies on distributional learning,][]{bejjanki2011,clayards2008,kleinschmidt-jaeger2016pbr,malone2021}[studies on dimension-based statistical learning,][]{idemaru-holt2020,schertz2015}[for review, see][]{schertz-clare2020}, whereas this remains the exception in other lines of research \autocites[but see, e.g.,][]{hitczenko-feldman2016}[ for accent adaptation]{tan2021}{drouin2016}[ for perceptual recalibration]{theodore-monto2019}. See also SI (\ref{sec:sufficiency}).}

As already mentioned, this does not mean that normalization, representation, and decision-making mechanisms always predict the same response patterns---they do not. Nor does it mean that the relative involvement of the three mechanisms cannot be empirically determined through experiments. Rather, as we discuss in more detail below, it means that the \emph{design} and \emph{level of analysis} employed in many studies severely limit what the resulting experiments can tell us about the underlying change mechanisms. This, we argue, calls for changes to the paradigms and analyses that are commonly employed in research on adaptive speech perception, and we make specific recommendations below.

Key to these recommendations is the second insight derived from our case studies: as we illustrated at the end of Case Study 2, each of the three mechanisms is subject to different computational limitations, and thus yields different predictions depending on the specific acoustic-phonetic properties of the exposure and test stimuli. Below we discuss how future experiments can be designed and analyzed so as to maximize researchers' ability to detect the differences in the predictions of the different change models, and to determine the relative engagement of each mechanism. Central to our recommendations is the use of ASP or similar frameworks to inform experiment design and analysis through simulations and quantitative model comparisons. First, however, we discuss \emph{why} we believe that computational simulations and model comparisons will be critical to advancing the field.

\hypertarget{beyond-sufficiency-tests}{%
\subsection{Beyond sufficiency tests}\label{beyond-sufficiency-tests}}

For our two case studies, we considered each of the three change models of the ASP framework \emph{separately}. This served two purposes. The first was presentational: by presenting the predictions of each model while the other two change models were `switched off', we hope to have provided readers with clearer intuitions about the inner workings of each model. The second purpose was the test of \emph{(in)sufficiency} of each change mechanism in explaining the signature results from the two paradigms. We asked whether each of the mechanisms alone is sufficient to predict adaptive changes of speech perception. Ultimately, however, research on adaptive speech perception will have to go beyond such sufficiency tests. With a phenomenon as complex as human speech perception, it is most plausible that multiple mechanisms, from early perception to decision-making, \emph{jointly} contribute to the observed adaptive behavior.

Indeed, the existing evidence---summarized in more detail in SI (\ref{sec:sufficiency})---supports the idea that none of the three mechanisms alone is sufficient to explain a wide range of adaptive behaviors that listeners exhibit in response to recent exposure. For example, the finding that non-speech stimuli---such as sine tones---can affect subsequent vowel perception \autocites[e.g.,][]{holt2001,holt2006,huang-holt2011} is easily explained in terms of pre-linguistic normalization but difficult to explain through changes in linguistic representations or decision-making \autocite[see also][]{chodroff-wilson2020}. Other findings, however, cannot easily be accommodated by an account that solely relies on pre-linguistic normalization. One example comes from research on perceptual recalibration: the effects of exposure to fricative consonants (/f/ vs.~/s/) do not only depend on the acoustic-phonetic cues of the exposure tokens but also their category labels (e.g., Norris et al., 2003, Experiment 2; for details, see SI, \ref{sec:sufficiency}). Further support for the hypothesis that adaptive speech perception is the result of multiple mechanisms comes from neuroimaging studies: across paradigms, adaptive speech perception implicates brain regions ranging all the way from pre-cortical structures \autocites[e.g., in the brain stem,][]{chandrasekaran2009,polonenko2021,zhao2018} to the (pre-)frontal cortex \autocite{hickok-poeppel2007,blanco-elorriera2021,defenderfer2021}.

Existing findings thus strongly suggest that no single change mechanisms can explain the full variety of adaptive responses that humans exhibit. Put this way, it seems obvious that the field will have to move beyond (in)sufficiency tests, towards experiments that determine how multiple change mechanisms \emph{jointly} achieve adaptive speech perception. This will likely require research on how the relative engagement of different change mechanisms depends on stimulus properties, cue and category types, task demands, or individual differences between listeners. For example, simple shifts of categories along a single continuum (as simulated in perceptual recalibration experiments) may engage change mechanisms different from those that are used to navigate more complex shifts as seen in natural accent \autocites[for discussion, see][]{bent-baeseberk2021,samuel-kraljic2009,zheng-samuel2020}. Another, mutually compatible, hypothesis holds that the earliest moments of exposure to an unfamiliar talker primarily reflect change mechanisms that are computationally less flexible but simpler. With increasing input, computationally more complex mechanisms, which ultimately can support higher recognition accuracy, would increasingly come to determine listeners' behavior.\footnote{In ASP, this is most naturally be accounted for in terms of slow `learning rates' for the more complex change model (e.g., high \(\kappa_{0,c}\)s and \(\nu_{0,c}\)s for changes in representations).} For listeners, this would reduce the risk of overfitting parameters to the input, a risk that applies more strongly to change models with a larger number of parameters \autocites[for discussion, see][]{apfelbaum-mcmurray2015,kleinschmidt-jaeger2015,toscano-mcmurray2010}. Similarly, it is possible that the relative engagement of different change mechanisms depends on the type of phonetic contrast, or even the type of cue. This would be expected, for example, because different types of cues exhibit different degrees of within- and between-talker variability \autocites[see discussions in][p.~179-180]{kleinschmidt-jaeger2015}{kraljic-samuel2007,xie2021cognition}.

Currently, the paradigms and analyses commonly used in the study of adaptive speech perception are not well-suited to address these questions. This problem is most obvious for the behavioral paradigms we have discussed in our case studies: if a paradigm cannot distinguish between three mechanisms \emph{even if one assumes that only one of these mechanisms is at work}, that paradigm cannot possibly determine the relative engagement of multiple mechanisms. However, neuroimaging studies are not exempt from this problem. While they might determine the involvement of multiple brain regions, they often leave open what types of computations are performed in each. It is not known, for example, whether subcortical areas involved in auditory processing are necessarily restricted to normalization or whether these areas can take category identity into consideration \autocite[e.g., through documented feedback projections from cortical areas,][]{Erb2013}. Similarly, it is an open question whether differential activation of early cortical areas indicates changes in category representations or normalization \autocite[e.g., the involvement of Heschl's gyrus in adaptive speech perception,][]{sjerps2019}, and whether differential activation in frontal areas reflect changes in decision-making \autocite[as seems to be assumed in, e.g.,][]{myers-mesite2014} or rather the cumulative upstream effect of distributed changes in representations. Observations like these about the current state of the field motivate the recommendations we present next.

\hypertarget{sec:methodological-advances}{%
\subsection{Methodological advances to facilitate more informative model comparisons}\label{sec:methodological-advances}}

We offer five concrete recommendations as to how future research on speech perception can benefit from computationally-guided experiment design and data analysis methods. In addition, we emphasize here that improved standards of data annotation and sharing will be critical in supporting this endeavor, and to the field en large. Although listeners' responses are known to depend on the acoustic-phonetic properties of the speech input, studies on adaptive speech perception rarely provide annotations of those properties. We cannot hope to understand speech perception without moving beyond this status quo (for concrete suggestions, see SI \ref{sec:DataSharing}).

\hypertarget{recommendation-1-prediction-based-on-sufficiently-constraining-theories-instead-of-only-informal-reasoning}{%
\subsubsection{\texorpdfstring{Recommendation 1: \emph{Pre}diction based on sufficiently constraining theories instead of (only) informal reasoning}{Recommendation 1: Prediction based on sufficiently constraining theories instead of (only) informal reasoning}}\label{recommendation-1-prediction-based-on-sufficiently-constraining-theories-instead-of-only-informal-reasoning}}

Our first recommendation is to develop and employ strong theories and models, in the sense of Platt's strong inference approach to scientific inquiry \autocite{platt1964}. The majority of published research on adaptive speech perception---including some of our own work---continues to employ informally formulated, under-specified hypotheses. This leaves too much room for \emph{ad-hoc} or \emph{post-hoc} reasoning, with all the downsides that have been discussed in the context of the replicability crisis and elsewhere \autocites[e.g.,][]{guest-martin2021,starns2019,vasishth2021,yarkoni2022}. Even for research explicitly framed in terms of more or less clearly specified theories like ``distributional learning'' or ``exemplar theory'', reliance on informal reasoning alone is both risky and likely to miss insights that can be gained if computational frameworks are employed.

We have experienced this in our own work---for example, in \textcite{kleinschmidt-jaeger2015} we were initially surprised that standard perceptual recalibration results can be explained through changes in beliefs about category \emph{variances} rather than only through changes in beliefs about category means. It was the use of a computational model with spelled-out linking hypotheses about the entire chain from inputs (stimuli) to participants' categorization responses that led to that insight---an insight that in \emph{hindsight} is obvious. Another example from our own work is \textcite{tan2021}, which failed to replicate the benefit of L2 accent exposure observed in \textcite{xie2016jep} for another L1-L2 combination (Flemish-accented Swedish). To our surprise, post-hoc simulations found that the representational change model of \textcite{kleinschmidt-jaeger2015} indeed predicted this replication failure: the acoustic-phonetic distributions of the stimuli used in \textcite{tan2021} implied that even successful representational changes did not result in improved perception during test---despite the qualitative similarities of the L2 accents employed in the two studies. We surmise there are other null findings previously taken to motivate novel mechanisms that can be similarly reduced to the acoustic-phonetic properties of the exposure and test stimuli \autocites[see e.g.,][]{floccia2006,zheng-samuel2020}.

A third demonstration of how computational and theoretical rigor can revise intuitions comes from the now classic study ``The Weckud Wetch of the Wast'' \autocite{maye2008}. The study exposed listeners to speech in which the vowel categories had undergone systematic (phonological) shifts and observed adaptive changes in subsequent vowel recognition. Based on additional control conditions, Maye et al.~(2008) reasoned that this finding could not be explained by ``general relaxation of the criterion for what constitutes a good exemplar of the accented vowel category'' (p.543) but instead argued that listeners had learned ways in which vowel representations are shifted. \textcite{hitczenko-feldman2016} recently revisited this result, using the representational change model of \textcite{kleinschmidt-jaeger2015}. Hitczenko and Feldman found that category expansion (``relaxation'') explained the results of Maye et al.~(2008) just as well as category shift.

We submit that computational models are not only useful for research on speech perception, but likely indispensable given the complexity of multiple, potentially interacting, adaptive mechanisms. For example, all major theories of adaptive speech perception agree that two types of information affect listeners' interpretation of speech from an unfamiliar talker: (1) listeners' prior expectations based on the statistics of previously experienced speech input; and (2) the statistics of the present speech input (i.e., in test) relative to those prior expectations. In exposure-test experiments like those discussed in our case studies, (1) is further divided into two components: (1a) expectations based on the long-term speech statistics experienced prior to the experiment; and (1b) the input experienced during the exposure phase, where (1b) is assumed to incrementally change (1a) that a listener brings into a subsequent perceptual response. In short, there is broad agreement that the commonalities and differences between prior experience, exposure, and test jointly affect listeners' behavior in our experiments. This points to complex interactions that are difficult to understand without the use of computational models \autocites[for further examples and discussion, see][]{apfelbaum-mcmurray2015,sohoglu-davis2016,tan2021,theodore-monto2019,toscano2018,xie2021cognition}.

Just as computational researchers benefit from familiarity with experiments, experimental psychologists and neuroscientists stand to benefit from increased familiarity with the computational models that are directly relevant to their research. Computational models complement hypothesis testing by advancing our understanding of the cognitive functions underlying complex tasks such as adaptive speech perception \autocite{kriegeskorte2018}, and they are now more accessible than ever. For instance, formal models of normalization have been available for decades and some of them are no more complex than standard data analysis \autocite[e.g., linear regression for C-CuRE,][]{mcmurray-jongman2011}. Similarly, fully specified distributional learning models have been available for at least two decades \autocites[e.g., mixtures of Gaussians,][]{mcmurray2007,toscano-mcmurray2010,feldman2013}[exemplar models,][]{foulkes-hay2015,johnson2005,apfelbaum-mcmurray2015}[ideal adaptors,][]{kleinschmidt-jaeger2015}. Both classes of models have some variants that are now freely available as R library \autocites[e.g., for normalization: \texttt{phonTools},][]{barreda2015}[for changes in category representations: \texttt{beliefupdatr} and its extension \texttt{MVBeliefUpdatr},][]{kleinschmidt-jaeger2015}. ASP integrates models of all three change mechanisms in R (provided at \url{https://osf.io/q7gjp/}). Specifically, the three change models of ASP spell out competing hypotheses about how exposure (1b) is integrated with prior experience (1a). Then the categorization model (updated based on the exposure input) determines how these changes are expected to affect listeners' responses to test items (2).

These resources can help researchers derive predictions prior to conducting an experiment, based on all the relevant acoustic-phonetic properties of the planned exposure and test stimuli (see Recommendation 4), and to compare the fit of different combinations of change models to the results once data collection has been completed (Recommendations 2 and 3).

\hypertarget{recommendation-2-analyses-that-link-the-acoustic-phonetic-properties-of-stimuli-and-participants-responses}{%
\subsubsection{Recommendation 2: Analyses that link the acoustic-phonetic properties of stimuli and participants' responses}\label{recommendation-2-analyses-that-link-the-acoustic-phonetic-properties-of-stimuli-and-participants-responses}}



\begin{figure}

{\centering \subfloat[Predicted recognition accuracy from changes in representations, decision-making and normalization\label{fig:show-model-categorization-3D-plots-best-performing-1}]{\includegraphics[width=1\linewidth,height=0.15\textheight]{../figures/plotly/p.model_comparison_Cue_reweighting_best-performing} }\newline\subfloat[Representations: \newline{}L1-accented\label{fig:show-model-categorization-3D-plots-best-performing-2}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.model_Representations_L1-accented_Cue_reweighting_best-performing} }\subfloat[Decision making: \newline{}L1-accented\label{fig:show-model-categorization-3D-plots-best-performing-3}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.model_Decision_making_L1-accented_Cue_reweighting_best-performing} }\subfloat[Normalization: \newline{}L1-accented\label{fig:show-model-categorization-3D-plots-best-performing-4}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.model_Normalization_L1-accented_Cue_reweighting_best-performing} }\newline\subfloat[Representations: \newline{}L2-accented\label{fig:show-model-categorization-3D-plots-best-performing-5}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.model_Representations_L2-accented_Cue_reweighting_best-performing} }\subfloat[Decision making: \newline{}L2-accented\label{fig:show-model-categorization-3D-plots-best-performing-6}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.model_Decision_making_L2-accented_Cue_reweighting_best-performing} }\subfloat[Normalization: \newline{}L2-accented\label{fig:show-model-categorization-3D-plots-best-performing-7}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.model_Normalization_L2-accented_Cue_reweighting_best-performing} }\newline\subfloat[Representations:\newline{} difference\label{fig:show-model-categorization-3D-plots-best-performing-8}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.difference.model_Representations_Cue_reweighting_best-performing} }\subfloat[Decision making: \newline{}difference\label{fig:show-model-categorization-3D-plots-best-performing-9}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.difference.model_Decision_making_Cue_reweighting_best-performing} }\subfloat[Normalization: \newline{}difference\label{fig:show-model-categorization-3D-plots-best-performing-10}]{\includegraphics[width=0.33\linewidth,height=0.15\textheight]{../figures/plotly//p.3d.categorization.difference.model_Normalization_Cue_reweighting_best-performing} }\newline

}

\caption{Using the best-performing parameterization for each model, the three change models predict different categorization functions when applied to the data from Case Study 2. From \textbf{left to right:} predictions for changes in representations, decision-making, and normalization. \textbf{Top row:} Predicted recognition accuracy from the three change models. \textbf{Second row:} Predicted categorization functions after L1-accented exposure. \textbf{Third row:} Same but after L2-accented exposure. \textbf{Bottom row:} Differences in predicted posterior log-odds of /d/ between the two exposure conditions. Blue indicates higher predicted posterior log-odds of /d/ in the L2-accented exposure condition, relative to the L1-accented exposure condition. Red indicates the opposite. Gray indicates a difference of 0. The three models make distinct predictions about how exposure affects the perception of specific tokens across the VOT-f0 space.}\label{fig:show-model-categorization-3D-plots-best-performing}
\end{figure}

In our case studies, we followed the approaches that continue to be employed in the majority of experiments on adaptive speech perception. In Case Study 1, we analyzed changes in categorization responses at six different test items. Following the majority of research on perceptual recalibration \autocites[for exceptions, see][]{drouin2016,saltzman-myers2021}, we did not further relate these changes to the \emph{acoustic or phonetic properties} of the exposure or test stimuli \autocites[for related discussion, see also][]{clayards2018,theodore2021}. In Case Study 2, we analyzed changes in accuracy. Like some previous studies \autocites[e.g.,][]{xie2016jep,zheng-samuel2020}, we showed these changes separately for the two categories investigated in the case study. Other studies on accent adaptation further simplify the dependent measure and analyze only overall improvements in accuracy \autocite{bradlow-bent2008,sidaras2009,tzeng2016} or processing speed \autocite{clarke-garrett2004}.

Although in common use, these types of analyses over aggregated data constitute a missed opportunity. They discard stimulus-level data that could otherwise provide valuable information on the nature of the mechanisms underlying adaptive speech perception. This is a general, and well-known, problem: many competing hypotheses might provide plausible explanations for coarse-grained aggregate data (like overall or category-specific accuracies). On the other hand, models that make the same predictions for aggregate data might be distinguishable when compared against more fine-grained data. For experiments on adaptive speech perception, this means that researchers stand to benefit from analyzing changes in the \emph{categorization function} ---i.e., the mapping from acoustic-phonetic properties of the test stimuli to participants' responses---in response to recent exposure.

We illustrate this point in Figure \ref{fig:show-model-categorization-3D-plots-best-performing}, which shows the predicted categorization functions after L1- and L2-accented exposure for each of three change models for the best-performing parameters that predict highly similar overall accuracy. While predicted categorization functions are quite similar across the three change models for the L1-accented exposure condition (second row), they vary more widely for the L2-accented exposure condition (third row). The bottom row of Figure \ref{fig:show-model-categorization-3D-plots-best-performing} further suggests that the three change models differ qualitatively in what \emph{type} of change in the categorization function they predict after L2-accented exposure. Whereas the effect of L2-accented, compared to L1-accented, exposure can be complex for the representational model (bottom left panel), it appears more constrained for changes in decision-making (bottom center panel) and normalization (bottom right panel). These differences reflect the computational limitations of each change models.

Critically, differences such as those shown in Figures \ref{fig:show-model-categorization-3D-plots-best-performing} only becpme apparent when the data is analyzed at a sufficiently fine-grained level---i.e., by assessing changes in the mapping from acoustic-phonetic properties of stimuli to participants' responses. Fitting ASP and similar models to the results of experiments is one way to conduct such analyses. Previous work has done so for competing normalization models \autocites[see, e.g.,][]{mcmurray-jongman2011,apfelbaum-mcmurray2015,persson-jaeger2022,richter2017,xie2021cognition} and competing parameterization of representational change models \autocites[e.g.,][]{kleinschmidt-jaeger2016cogsci,kleinschmidt2020,tan2022}. Future work can employ ASP to compare the fit of all three change models---including combinations of them---to the results of experiments: with minimal modifications, the code we used to find the best-performing parameterizations in Case Study 2 can be used to find the parameterizations that best fit listeners' responses from a perception experiment. This will allow researchers to investigate what factors contribute to the relative engagement of different change mechanisms.

Alternatively, standard approaches to data analysis can be used to investigate changes in the mapping from acoustic-phonetic properties to participants' responses \autocite[for an excellent review, see][]{schertz-clare2020}. This includes regression models as long as they employ appropriate linking functions \autocites[e.g., logistic or multinomial regression for categorization,][]{jaeger2008,winter-wieling2016}. This approach is now increasingly common, taking advantage of stimulus-level variability within and across conditions to test hypotheses \autocites[e.g.,][]{clayards2018,idemaru-holt2020,schertz2015}. Regression analyses can further be expanded into psychometric models \autocite{wichmann-hill2001} by adding lapse rates and response biases \autocites[e.g.,][]{clayards2008,kleinschmidt2020}, and can be fit in standard statistics software \autocite[e.g., \texttt{brms},][]{burkner2017}. Optionally, such regression analyses can be enriched with predictive ASP simulations of the type we have employed in our case studies. Such simulations are computationally less demanding than \emph{fitting} ASP models to perception experiments, and can help guide the interpretation of results \autocites[for examples of approaches that mix predictive simulation with standard data analysis, see][]{bejjanki2011,clayards2008,hitczenko-feldman2016,tan2021,theodore-monto2019,xie2021cognition}[see also discussion in][]{bent-baeseberk2021}.

To fully take advantage of the types of analyses we have discussed here, it will, however, be important to plan experiments with these analyses in mind. This leads to our next recommendation.

\hypertarget{recommendation-3-dense-and-targeted-sampling-of-the-stimulus-space}{%
\subsubsection{Recommendation 3: Dense and targeted sampling of the stimulus space}\label{recommendation-3-dense-and-targeted-sampling-of-the-stimulus-space}}

Our third recommendation is to obtain data that better characterize incremental changes in categorization functions that occur with exposure. Simply put, the more data an experiment yields on how different types of exposure change the shape and location of the categorization function, the easier it is to determine the relative engagement of the different change mechanisms. Each change model is subject to different computational limitations, and these limitations affect what types of changes in categorization functions the different change models can account for (see, e.g., Figure \ref{fig:show-model-categorization-3D-plots-best-performing} in the previous section). This idea is further illustrated in Figure \ref{fig:repeated-sampling}. In this section, we discuss how researchers can design their experiments that generate results that are informative about changes in categorization functions.

\begin{figure}[h]
\begin{center}
\includegraphics[width=1 \columnwidth]{../figures/diagrams/repeated-sampling.png}
\caption{Illustrating how dense and repeated sampling of categorization responses can shed light on the relative engagement of different change mechanisms. {\bf Panel A:} Observed changes of human responses from before to after exposure can be characterized along multiple acoustic-phonetic dimensions. {\bf Panel B:} Different change models predict different incremental changes in human behavior. Repeated sampling of human responses after different types and amounts of exposure (e.g., Tests 1-3) increases researchers' power to distinguish between mechanisms. Each line represents a particular parameterization of ASP's change models.}\label{fig:repeated-sampling}
\end{center}
\end{figure}

Several design properties under researchers' control can make an experiment more informative about the constraints on changes in listeners' categorization functions. As described under Recommendation 1, the changes in categorization functions predicted by different change models depend on both the exposure and test stimuli. For exposure, both the \emph{stimulus location} in the acoustic-phonetic space---relative to listeners' prior expectations---and the \emph{number of exposure stimuli} affect the predictions of change models. The location of exposure stimuli determines the \emph{type} of change that is predicted to occur (e.g., shifts vs.~changes in the slope of categorization function; changes in the relative weighting of different cues). The amount of exposure determines \emph{how much} change of that type is predicted.

One example of examining multiple (exposure) stimulus locations comes from \textcite{kleinschmidt-jaeger2016cogsci}, who exposed different groups of participants to six different exposure conditions with distinct VOT distributions for /b/ and /p/. The data were then used to contrast different parameterizations of the same change model \autocite[for additional analyses and insightful discussion, see also][]{kleinschmidt2020}. While some studies have employed multiple exposure conditions in perceptual recalibration and similar paradigms \autocites[e.g.,][]{babel2019,sumner2011}, this approach remains under-utilized.

Similarly, incremental testing \emph{within} subjects can also increase the ability to contrast the predictions of different change models (Figure \ref{fig:repeated-sampling}). Even when all three change models predict more or less the same outcome of adaptation, they can differ in the trajectory of changes they predict \autocites[for some pioneering behavioral work, see e.g.,][]{bertelson2003,bonte2017,vroomen2007}[and for computational models, see][]{kleinschmidt-jaeger2012}. A particular parameterization of ASP's change model can simulate how human listeners' behavior could change after different types and amounts of exposure. Repeated and incremental sampling of human responses (e.g., as indicated as Tests 1-3 in Figure \ref{fig:repeated-sampling}) can thus boost researchers' abilities to distinguish between the underlying mechanisms. Note that the same logic and approach can be used to examine the scenarios where more than one---and possibly all---mechanisms are engaged, as shown in the lower right panel of Figure \ref{fig:repeated-sampling}. Just as we turned on one of the mechanisms and left the other two off in the case studies above, one can turn on two (or all) of the mechanisms to predict changes of human perception. Dense sampling of human data can provide the power that is needed to reliably contrast that these, potentially subtly, distinct models' predictions.

Researchers can also benefit from selecting test stimuli to be maximally informative about changes listeners' categorization functions. Paralleling our recommendations for exposure stimuli, this includes considerations about both the location and the number of test stimuli. For example, to detect differences between the different categorization functions, an experiment needs to employ test stimuli that are sufficiently distributed across the acoustic-phonetic space. This is worth emphasizing since it is typically \emph{not} the case in experiments on perceptual recalibration and accent adaptation. For perceptual recalibration, experimenters tend to target stimuli that are expected to be maximally ambiguous. This makes sense if the goal is to detect the \emph{existence} of a shift in the categorization function. But it is far from optimal when the goal is to understand the relative engagement of different change mechanisms, which requires identifying the nature of changes in the categorization function (Recommendation 2).

By similar reasoning, test phases in experiments on accent adaptation tend to densely sample the space around the accented category means---a side effect of using naturally accented stimuli. But any change model that can predict general improvements in accuracy will also tend to correctly predict the categorization for those stimuli, making them relatively uninformative about the types of changes that result from exposure. Future experiments can instead employ more targeted sampling of the acoustic-phonetic space during test. Researchers should first derive model predictions and sample test stimuli from the regions where model predictions would be maximally distinct from one another \autocite[for initial efforts along these lines, see][]{burchill-jaeger2022}. As shown in Figure \ref{fig:show-model-categorization-3D-plots-best-performing}, some of the largest predicted differences can occur in parts of the acoustic-phonetic space that are neither close to the category means, nor on the line between them.\footnote{The optimal choice of stimuli depends on the specific goals of the experiment. For example, stimuli locations that are optimal for estimating the categorization function after one exposure condition, are not necessarily optimal for estimating the categorization function in another exposure condition, and neither choice might be optimal if the goal is to detect \emph{differences} between the two conditions or \emph{differences} between the different change models. Predictive (power) simulations can help determine the optimal stimulus locations and repetitions, for any of these goals (Recommendation 4).}

While targeted sampling is most easily achieved for paradigms that employ (re)synthesized or otherwise phonetically manipulated stimuli \autocites[for examples, see][]{bejjanki2011,burchill-jaeger2022,idemaru-holt2020}, it is also possible in combination with naturally accented stimuli. For example, \textcite{chodroff-wilson2020} took advantage of naturally occurring variability, and created different exposure conditions by selecting different subsets of natural stimuli. This approach strikes a particularly intriguing balance between ecological validity and experimental control that deserves further attention in future work.

\hypertarget{recommendation-4-predictive-power-analyses-prior-to-conducting-experiments}{%
\subsubsection{Recommendation 4: Predictive power analyses prior to conducting experiments}\label{recommendation-4-predictive-power-analyses-prior-to-conducting-experiments}}

Like other computational frameworks, ASP can be used to estimate expected effect sizes and statistical power \emph{before} conducting an experiment. Unlike commonly used power estimates, which \emph{assume} effect sizes \autocite[e.g., ``moderate'' effects,][]{zheng-samuel2020}, predictive power-analyses of the type we envision are based on effect sizes that are derived from the acoustic properties of the very exposure and test stimuli used in the given experiment. This makes power simulations considerably more informative.

There are, however, challenges that will have to be met so that ASP can be used for predictive power simulations. For a specific set of exposure and test stimuli, and specific parameter settings for all change models, ASP predicts an expected categorization function. This is what we showed in, for example, in Figure \ref{fig:show-model-categorization-3D-plots-best-performing} \autocites[see also, e.g.,][]{tan2021,theodore-monto2019}[ for similar approaches]{xie2021cognition}. This categorization function can be used to derive a predictive power estimate by repeatedly sampling responses for the planned test stimuli and the planned number of participants. This can be done for any individual exposure conditions, for combinations of exposure conditions, and/or the predicted differences between exposure conditions.

However, the parameterization of the different models is typically not known to researchers---if they were, there would be no need for the present article. One important direction for future research will thus be to better characterize the functional consequences of each model's computational limitations. For example, in the SI (\ref{sec:consequences-of-lambda}), we show that changes in decision-making can only explain additive effects of exposure on the log-odds of categorization responses (for lapse rates of 0 or 1) or effects that resemble a step-function (for all other lapse rates). Further study of this and similar constraints will shed light on what range of adaptive behaviors each model can predict \autocites[for discussions of global, qualitative model comparisons, see][]{pitt2006,apfelbaum-mcmurray2015}. Another related approach is to derive predictions and calculate statistical power while marginalizing (i.e., averaging) over a plausible \emph{distribution} of ASP parameters for the models they wish to contrast. In Bayesian terminology, this is achieved by defining ``priors'' over the ASP parameters. Initially, when relatively little is known about the relevant distribution of parameters, very weak priors are recommended that will consider a wide range of parameters as probable. As additional studies become available, this will further inform the range of plausible priors. Even now though, the plausible range of parameters is constrained by the fact \emph{that} listeners can adapt within a certain number of observations. This constrains the joint distribution of ASP parameters since any plausible parameterization needs to give at least one of the change models sufficient flexibility.

If researchers wish to know which exposure or test stimuli would increase the predicted difference between hypotheses, this further requires comparison of different stimulus selections. Comparable approaches exist for simpler computational problems and have been used extensively in psychometric research \autocites[incl.~online stimulus selection during the experiment, depending on subject-specific performance, e.g.,][]{vul2011,prins2013}. We believe that this is an area of research with considerable potential.

\hypertarget{recommendation-5-integration-with-neural-models-of-speech-perception}{%
\subsubsection{Recommendation 5: Integration with neural models of speech perception}\label{recommendation-5-integration-with-neural-models-of-speech-perception}}

Our final recommendation is to further integrate ASP with neural models of speech perception. ASP as put forward here is a model that makes behavioral predictions. Similarly fully specified models of the neural computations underlying adaptive speech perception are a bigger challenge and remain lacking \autocites[but see][]{sohoglu-davis2016,sohoglu-davis2020}[for discussion, see][]{guediche2014}. In the introduction, we outlined that models like ASP can support neuroimaging research by making more concrete---and thus more testable---the types of computations that are hypothesized to take place in different brain regions or networks. At the same time, future neuroimaging research can further constrain the hypothesis space, and inform frameworks like ASP. One notable step towards integrating computational models of speech adaptation with evidence from neuroimaging is presented in \textcite{sohoglu-davis2016}. Sohoglu and Davis describe the possible neural implementations of both a categorization model and a change model for category representations \autocite[see also][]{sohoglu-davis2020}. The categorization model involves two interacting areas. One is the frontal areas associated with decision-making. These areas encode the relative evidence for the different categories, p(category \textbar{} cues, context), which eventually is used to decide that a category has been `recognized'. The other is the superior temporal gyrus that encodes predictions about phonetic inputs. These predictions are a function of the mapping from categories to cues, p(category \textbar{} cues), weighted by the relative expectations for the categories coded in the frontal areas, p(category \textbar{} context). The difference between these predictions and the observed phonetic inputs results in a prediction error that is used to update the activation of categories in frontal decision-making areas, changing p(category \textbar{} cues, context). This part of Sohogulu and Davis' model---initially validated by experiments in \textcite{sohoglu2014}---thus can be seen as implementing ASP's categorization model (without normalization). \textcite{sohoglu-davis2016} further propose that changes in category representations reflect changes in predictive coding in the superior temporal gyrus \autocites[see also][]{blank2016,wang2021}. Exactly how these changes come about remains open, constituting a question of importance for future research.

As in behavioral studies, the integration of computational models such as ASP will increase the specificity of how---through what mechanism---a given brain area or neural network relates to behavioral changes in response to recent exposure. Existing results have suggested that adaptive changes in speech perception involve several different brain regions \autocite{luthra2020a}. These include cerebellum \autocite{guediche2014}, early auditory regions \autocites[anterior planum temporale,][]{kilianhutten2011,bonte2017} and regions responsible for representing phonemes and syllables \autocites[e.g., posterior superior temporal gyri/superior temporal sulcus,][]{bonte2017,myers-mesite2014,ullas2020}, as well as regions for talker recognition \autocite[right temporal regions,][]{luthra2020b}. The left parietal lobe and the insula, which are implicated in perceptual decision-making \autocites[e.g.,][]{dacremont2013,keuken2014}, have also been shown to exhibit distinct activation for different exposure conditions.

While these regions have been found to respond differently to different exposure conditions (e.g., familiar vs.~unfamiliar talker accents, see for example, \textcite{adank2012neural}; \textcite{holmes2021speech}), most of them are also known to play multiple roles in cognitive processing. The exact interpretation of the results therefore depend largely on the individual researcher's assumptions about the underlying mechanism. For instance, the activation of left parietal lobe might reflect its general role in perceptual decision-making \autocites[e.g.,][]{dacremont2013,keuken2014}, or it could be due to a more specific role in phonological processing \autocite[e.g., processing abstract category information,][]{guediche2014}. Similarly, changes in inferior frontal gyri activation in response to accented speech has been interpreted as reflecting greater computational demand \autocite{yi2014neural} or decision-related phonetic categorization of ambiguous stimuli \autocite{myers-mesite2014}. This ambiguity is compounded by univariate analysis methods which test an increase or a decrease of neural activity while largely leaving open the information content represented in the distinct brain regions. Increased specificity in the links between acoustic-phonetic cues and expected changes of recognition, as facilitated by ASP, can help resolve this ambiguity.

Recent research has begun to apply multivariate analyses that can be effectively combined with the model-based approach we have described above. Multivariate analyses can be used to identify the encoding of perceptual experiences across a distinct array of experimental conditions, which helps to distinguish between cognitive models that make different predictions regarding the \emph{type} of information encoded by different brain regions and networks \autocite[e.g.,][]{blank2016}. An additional advantage of multivariate analyses is that they are more sensitive in detecting fine-grained patterns \emph{within} regions \autocites[e.g.,][]{bonte2017,luthra2020a}. Representational similarity analysis (RSA) of fMRI data allows researchers to take advantage of specific similarity matrix for a set of stimuli, rather than just the overall activation level for each condition. By selecting test stimuli so as to maximize the contrast between the similarity matrices of different change models, RSA can generate novel insights on the computations performed by different brain regions. Model-guided experimental designs as described in Recommendations 1-4 can facilitate such stimuli selection.

The synergy between behavioral, computational, and neural studies may uncover new knowledge about the relative engagement of the three mechanisms over time. \textcite{myers-mesite2014} found that boundary shifts in a perceptual recalibration study were initially driven by changes in the brain regions responsible for decision-making in the absence of changes in acoustic-phonetic representations (as indexed by STG activation). However, over the course of 20 critical exposure trials, evidence of retuned perceptual sensitivities slowly emerged. The authors concluded that this indicates a ``transfer of decision-related or lexical-level information to more bottom-up or perceptual processes in the temporal lobes'' (p.~91). To extend this finding, future research should track brain responses over a more extended period of exposure. ASP models' abilities to predict incremental changes for multiple change mechanisms make it a tool well-suited to aid the design of such new experiments.

Finally, another promising avenue is to pair temporally-sensitive techniques with imaging methods with good spatial resolution. For instance, recent electrocorticography studies have provided some direct evidence for prelinguistic cue-level normalization within the middle and posterior STG areas \autocites{johnson-sjerps2021,tang2017}[see also][]{sjerps2019}, although it remains an open question how normalization supports the kind of adaptive changes as observed in perceptual recalibration or accent adaptation. Studies employing a combination of EEG and MEG have also been used to tease apart the role of signal properties and prior expectations in regulating perceptual learning of degraded speech: although both properties seem to affect activation of the same region in STG, they operate over different timescales \autocite{sohoglu-davis2016,sohoglu-davis2020}. Temporally-sensitive methods are also helpful in distinguishing between distinct mechanisms that may underlie adaptive speech perception. Using P200 as an index for acoustic-phonetic processing, \textcite{romero2015} found that the extraction of spectral information and other acoustic features was more difficult for foreign-accented speech than for native speech. Critically, this difference in acoustic-phonetic mapping did not attenuate within a single exposure session, compared to faster improvements in lexical-semantic processing, as indexed by N400. To the extent that the P200 and N400 reflect acoustic-phonetic vs.~post-lexical processes, respectively, these results are compatible with the possibility that neural changes associated with decision-making criteria \autocites[driven by the prediction error experienced during lexical processing, cf.][]{delaney2019,kuperberg2016} occur faster than those responsible for acoustic-phonetic remapping.

\hypertarget{conclusion}{%
\subsection{Conclusion}\label{conclusion}}

We have introduced a theoretical and computational framework for adaptive speech perception (ASP). ASP formalizes three distinct mechanisms of speech perception (`categorization') and adaptive speech perception (`learning'), ranging from low-level auditory (normalization), to linguistic (category representations), and cognitive processes (decision-making). In the present paper, we have presented specific categorization and change models that aimed to be general-purpose, capturing the most common assumptions shared among theories of speech perception. By writing this article in R markdown, we hope to have made it easier for other researchers to revisit any of the assumptions we made, e.g., by substituting alternative models of normalization, category representations, and decision-making. Finally, the simulations in our case studies explored what can (and cannot) be concluded from previous work about the three mechanisms, and how future research can develop behavioral and neuroimaging studies that shed light on the relative engagement of these mechanisms.

\printbibliography
\erefsection

\brefsection

\newpage
\setcounter{page}{1}
\renewcommand{\thesection}{\S \arabic{section}}
\renewcommand{\theHsection}{sisection. \arabic{section}}

\hypertarget{supplementary-information-for-xie-jaeger-kurumada-2022.-what-we-do-not-know-about-the-mechanisms-underlying-adaptive-speech-perception}{%
\section*{\texorpdfstring{Supplementary information for \emph{Xie, Jaeger, \& Kurumada (2022). What we do (not) know about the mechanisms underlying adaptive speech perception}}{Supplementary information for Xie, Jaeger, \& Kurumada (2022). What we do (not) know about the mechanisms underlying adaptive speech perception}}\label{supplementary-information-for-xie-jaeger-kurumada-2022.-what-we-do-not-know-about-the-mechanisms-underlying-adaptive-speech-perception}}
\addcontentsline{toc}{section}{Supplementary information for \emph{Xie, Jaeger, \& Kurumada (2022). What we do (not) know about the mechanisms underlying adaptive speech perception}}

Both the main text and these supplementary information (SI) are derived from the same R markdown document available via OSF at \url{https://osf.io/q7gjp/}. \textbf{The PDFs for both the main text and the SI are best viewed using Acrobat Reader.} Some links and animations might not work in other PDF viewers.

\setcounter{section}{0}

\hypertarget{sec:SI-software}{%
\section{Required software}\label{sec:SI-software}}

The document was compiled using \texttt{knitr} \autocite{xie2021} in RStudio with R:

\begin{verbatim}
##                _                           
## platform       aarch64-apple-darwin20      
## arch           aarch64                     
## os             darwin20                    
## system         aarch64, darwin20           
## status                                     
## major          4                           
## minor          2.1                         
## year           2022                        
## month          06                          
## day            23                          
## svn rev        82513                       
## language       R                           
## version.string R version 4.2.1 (2022-06-23)
## nickname       Funny-Looking Kid
\end{verbatim}

We used the following R packages to create this document: R \autocite[Version 4.2.1;][]{R-base} and the R-packages \emph{assertthat} \autocite[Version 0.2.1;][]{R-assertthat}, \emph{brms} \autocites[Version 2.17.0;][]{R-brms_a,R-brms_b,R-brms_c}, \emph{cowplot} \autocite[Version 1.1.1;][]{R-cowplot}, \emph{curl} \autocite[Version 4.3.2;][]{R-curl}, \emph{data.table} \autocite[Version 1.14.8;][]{R-data.table}, \emph{devtools} \autocite[Version 2.4.4;][]{R-devtools}, \emph{diptest} \autocite[Version 0.76.0;][]{R-diptest}, \emph{dplyr} \autocite[Version 1.1.1;][]{R-dplyr}, \emph{forcats} \autocite[Version 1.0.0;][]{R-forcats}, \emph{gganimate} \autocite[Version 1.0.8;][]{R-gganimate}, \emph{ggplot2} \autocite[Version 3.4.1;][]{R-ggplot2}, \emph{kableExtra} \autocite[Version 1.3.4;][]{R-kableExtra}, \emph{LaplacesDemon} \autocite[Version 16.1.6;][]{R-LaplacesDemon}, \emph{latexdiffr} \autocite[Version 0.1.0;][]{R-latexdiffr}, \emph{linguisticsdown} \autocite[Version 1.2.0;][]{R-linguisticsdown}, \emph{lme4} \autocite[Version 1.1.30;][]{R-lme4}, \emph{magick} \autocite[Version 2.7.3;][]{R-magick}, \emph{magrittr} \autocite[Version 2.0.3;][]{R-magrittr}, \emph{Matrix} \autocite[Version 1.4.1;][]{R-Matrix}, \emph{mixtools} \autocite[Version 1.2.0;][]{R-mixtools}, \emph{modelr} \autocite[Version 0.1.9;][]{R-modelr}, \emph{papaja} \autocite[Version 0.1.1.9,001;][]{R-papaja}, \emph{phonR} \autocite[Version 1.0.7;][]{R-phonR}, \emph{plotly} \autocite[Version 4.10.0;][]{R-plotly}, \emph{processx} \autocite[Version 3.8.0;][]{R-processx}, \emph{purrr} \autocite[Version 1.0.1;][]{R-purrr}, \emph{Rcpp} \autocites[Version 1.0.10;][]{R-Rcpp_a,R-Rcpp_b}, \emph{readr} \autocite[Version 2.1.2;][]{R-readr}, \emph{rlang} \autocite[Version 1.1.0;][]{R-rlang}, \emph{rmarkdown} \autocites[Version 2.20;][]{R-rmarkdown_a,R-rmarkdown_b}, \emph{stringr} \autocite[Version 1.5.0;][]{R-stringr}, \emph{tibble} \autocite[Version 3.2.1;][]{R-tibble}, \emph{tidyr} \autocite[Version 1.3.0;][]{R-tidyr}, \emph{tidyverse} \autocite[Version 1.3.2;][]{R-tidyverse}, \emph{tinylabels} \autocite[Version 0.2.3;][]{R-tinylabels}, \emph{tufte} \autocite[Version 0.12;][]{R-tufte}, and \emph{usethis} \autocite[Version 2.1.6;][]{R-usethis}. If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. This includes the \texttt{MVBeliefUpdatr} library that supports working with multivariate Gaussian ideal observers and adaptors (see also Dave Kleinschmidt's \texttt{beliefupdatr} library that this library is based on). The full session information is provided at the end of this document.

The 3D figures require the \href{https://github.com/plotly/orca\#installation}{\texttt{orca} commandline tool}. It is recommended that you use ``Method 4'' to install the standalone binaries. Some of the R libraries evoked in the R markdown code might require additional freely available non-R software (e.g., the R library \texttt{sf} requires non-R \emph{gdal}). For line-by-line execution of this document, this is all you should need. If installation of any R library fails, follow the prompts in the error message.

If you want to knit the document into a PDF file, you will some additional packages. First, make sure that your version of the R package \texttt{tinytex} is up-to-date. Then run \texttt{tinytex::install\_tinytex()} to install latex on your computer and/or link it to R. As of 12/20/2022, it was also necessary to use \texttt{tinytex::tlmgr\_install('biber')} to install \texttt{biber}, though that step might become unnecessary in the future. Pandoc version 2.19.2 was tested. If you do not have at least that version then update RStudio, which will also install the newest version of pandoc (you can check by running \texttt{rmarkdown::pandoc\_version()}). Finally, you might need to download the IPA font \href{https://software.sil.org/doulos/download/}{SIL Doulos}, which is required for the IPA symbols in the paper.

\hypertarget{sec:SI-chodroff}{%
\section{\texorpdfstring{A database of natural productions of word-initial stop voicing in L1 US English \autocite{chodroff-wilson2018}}{A database of natural productions of word-initial stop voicing in L1 US English {[}@chodroff-wilson2018{]}}}\label{sec:SI-chodroff}}

All case studies presented in the main text are based on a phonetically annotated database of word-initial stop voicing in L1 US English \autocite{chodroff-wilson2018}. Chodroff and Wilson (2018, p.~3) describe the database:

\begin{quote}
The data was extracted from an audited subset of the Mixer 6 corpus (Brandschain et al.~2010, Brandschain
et al.~2013; Chodroff et al.~2016) containing approximately 45 minutes of read speech from 180 native AE
speakers (102 female). Transcripts were aligned to the corresponding WAV files with the Penn Forced Aligner
(Yuan and Liberman 2008), and all word-initial prevocalic stop consonants were further processed with
AutoVOT (Keshet et al.~2014). AutoVOT automatically identifies the stop release and following vowel onset
within a user-specified window of analysis. Further details about the talkers, read sentences, and boundary
alignments can be found in Chodroff and Wilson (2017).³
\end{quote}

\begin{quote}
COG, positive VOT, and onset f0 in the following vowel were measured for each stop. COG was calculated
from a smoothed spectrum over the initial portion of the release burst. Each spectrum was computed by averaging
FFTs from seven consecutive 3 ms windows, with the first window centered on the burst transient and
a window shift of 1 ms (Hanson and Stevens 2003; Flemming 2007; Chodroff and Wilson 2014). Positive VOT
was defined as the duration from stop release to the onset of periodicity in the vowel; this was automatically
extracted from the AutoVOT boundaries or from manually-corrected boundaries when available. The
f0 value was the first one measured by Praat (Boersma and Weenink 2016) within 50 ms after the following
vowel onset.
\end{quote}

Following advice from Eleanor Chodroff, we removed tokens with f0 measurements of above 350 Hz, as those were implausible and likely reflected a measurement error (pitch doubling). We further subset the data talkers for which all three cues (VOT, COG, and f0) were available for at least 25 observations each per stop category. This was done because we detected that the f0 of a good number of talkers exhibited evidence of bimodality. To remove talkers with bimodal f0 measures, we applied a test of multimodality \autocite[the dip test, as implemented in the library \texttt{diptest} in \texttt{R},][]{maechler2021}. Restricting our data set to talkers with at least 25 observations each per stop category allowed us to more reliably identify bimodal f0 distributions. Talkers for which the null of unimodality was rejected (\(p<\) 0.1) for the raw f0 or Mel-transformed f0 of \emph{any} stop category were excluded.\footnote{We thank Eleanor Chodroff for help with this issue. Her inspection of some example tokens revealed that both creaky voice and pitch halving contributed to the bimodal pattern. Pitch halving refers to cases in which the f0 detection algorithm wrongly infers the f0 to be half of its true value. One reason for such estimation mistakes can be when the true f0 falls outside the range considered by the algorithm (here: 75-500 Hz, regardless of talker gender, Eleanor Chodroff, p.c.).}

This left 40,459 observations from 104 different talkers and 98 words. These are the data that we used to derive plausible estimate of the overall and category-specific VOT and Mel distributions that underlie the case studies presented in the main text. Figure \ref{fig:chodroff-stop-VOT-f0ST} shows the individual tokens and fitted bivariate Gaussian category likelihood of three randomly selected talkers from this data set prior to C-CuRE normalization. Figure \ref{fig:chodroff-stop-VOT-f0ST-mu} further illustrates the degree of cross-talker variability by plotting all talkers' category means relative to the category likelihood across all talkers prior to normalization.



\begin{figure}

{\centering \includegraphics{../figures/knitted/chodroff-stop-VOT-f0ST-1} 

}

\caption{Unormalized VOT and f0 of word-initial stop consonants in L1 US English from 3 random talkers from the database \autocite{chodroff-wilson2018}. Transparent points show individual tokens, which cover a range of different phonotactic, lexical, and utterance contexts. Solid points show talker-specific means over these tokens, connected by a gray line. Ellipses show the 95\% probability mass boundary for the talker-specific bivariate Gaussian category likelihoods.}\label{fig:chodroff-stop-VOT-f0ST}
\end{figure}



\begin{figure}

{\centering \includegraphics{../figures/knitted/chodroff-stop-VOT-f0ST-mu-1} 

}

\caption{By-talker means of VOT and f0 for all six stop consonants and all 104 talkers in the database \autocite{chodroff-wilson2018} for which all phonetic cues were available. Ellipses show the 95\% probability mass for talker-independent bivariate Gaussian categories if the data from all talkers are pooled independent of talker identity.}\label{fig:chodroff-stop-VOT-f0ST-mu}
\end{figure}

\hypertarget{sec:SI-applying-C-CuRE}{%
\subsection{Applying C-CuRE}\label{sec:SI-applying-C-CuRE}}

We follow \textcite{mcmurray-jongman2011} and use linear regression to remove the effects of talker from each observation in the database. In extending normalization accounts to our present goals, we encountered three decision points that we have not previously seen discussed:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  C-CuRE removes the \emph{overall} cue mean from each observation's cue value. Based on this, it would seem most appropriate to include all six stop categories in the estimation of the cue means. However, the experiments we model in the main text involve only two of the categories (/d/ and /t/). We thus decided to only include tokens of /d/ and /t/ in the estimation of cue means. This decision was made because C-CuRE is intended to correct observed cue values for listeners' expectations \emph{for the current context}. Put differently, we assume that listeners expect that the sound will be a /d/ or a /t/ since these are the only two response options provided to participants. This assumption affects some of our results (but not as much as it would if we studied /b/-/p/ or /g/-/k/ since the mean cue values for /d/s and /t/s are closer to the mean cue values across all six categories).
\item
  C-CuRE is meant to correct for effects of both talkers and \emph{phonological} contexts. However, in the \textcite{chodroff-wilson2018} data /d/ and /t/ occur across different phonological contexts, so that inclusion of phonological contexts as a predictor in the regression model would indirectly capture information about category identity. We thus limited the data further to 9 pairs of /d/- and /t/-items that had identical vowels following the stop consonant (e.g, \emph{\emph{d}ied} and \emph{\emph{t}ime}). While this does not completely remove the effects of phonological context, it both reduces the variability in cue values associated with phonological contexts (thus providing a more accurate estimate of the expected outcome of normalization) and \emph{balances} the effect of phonological context (de-confounding it from category identity).
\item
  Finally, the original data from \textcite{chodroff-wilson2018} contained nearly three times as many /d/s as /t/s, and this asymmetry was retained after the data cleaning procedures described above. While this particular imbalance is probably not reflective of natural speech, natural speech will often exhibit \emph{some} form of asymmetry in the relative frequencies of categories. This raises the question of whether listeners somehow correct for these asymmetries when normalizing the speech input for an experiment for which there all categories can be expected to be equally frequent. Following similar considerations as in Point 1 above, we assume this to be the case. We thus randomly subsampled equal numbers of tokens for each /d/ and /t/ from all talkers. This number was determined by the number of tokens of the less frequent category for each talker. For instance, if a talker produced 40 /d/s and 20 /t/s, then 20 /d/ and 20 /t/ observations were randomly pulled from the data.
\end{enumerate}

For experimenters, the issues described here will arise whenever the distribution of categories and/or phonological contexts differ between prior exposure and the exposure in the experiment, or between exposure and test in the experiment: the estimated mean will then reflect expectations that do not veridically reflect the statistics of the current input. In previous evaluations of C-CuRE this was never the case, since those evaluations made a number of unrealistic assumptions: the data previously used to test C-CuRE typically were balanced across combinations of (1) categories, (2) phonological contexts, and (3) talkers, and (4) were so equally for both C-CuRE was `fit' on and the data that it was tested on \autocites[see, e.g.,][]{mcmurray-jongman2011}[for a critique and demonstration that these assumptions can affect the conclusions to be drawn about the plausibility of different normalization approaches, see][]{barreda-nearey2018}. For the present study, we investigate how listeners might transfer and adapt expectations based on previous long-term exposure to novel input from an unfamiliar talker. We therefore used subsamples that were well-balanced between /d/ and /t/ tokens in terms of their frequency and phonological contexts---a pattern matched to those employed during the exposure and test in typical experiments. We note, however, that none of our core results seem to depend on the assumptions we make here. We obtained qualitatively identical results for all critical comparisons in previous simulations that did not subsample the data to be balanced with regard to /d/ and /t/ or their phonological contexts.

The three issues raised above also raise questions about C-CuRE that go beyond practical concerns for experimenters. For any of the three decisions we described above, it is an empirical question whether listeners do something similar. Put differently, there are complexities in applying approaches like C-CuRE to scenarios that are likely to occur in everyday speech perception that need to be investigated in future research.

For example, our third assumption essentially means that normalization is sensitive to which category tokens are inferred to originate from (unlike any normalization accounts we know of), and this can result in some counter-intuitive predictions. Consider a scenario, in which a listener hears 100 typical /d/ tokens that match the listener's prior expectations for /d/. As formulated in \textcite{mcmurray-jongman2011}, C-CuRE would predict that the listener's estimate for the cue mean will move towards the category mean of /d/ (since normalization is assumed to be insensitive to which category tokens originate from).\footnote{Intriguingly, this would predict that the listener will categorize fewer tokens along the prototypical /d/ to /t/ continuum as /d/---in line with findings that repeated exposure to prototypical tokens of a category leads to selective adaptation \autocites[e.g.,][]{samuel1986,samuel2021,vroomen2007}[for discussion, see also][]{kleinschmidt-jaeger2016pbr}.} Alternatively, if normalization \emph{is} sensitive to which category a token is inferred to originate from, then the listener's estimate of the cue mean is predicted to not change much at all (since all tokens were expected given that they were a /d/). Which of these two rather different conceptualizations of normalization is empirically more adequate is, to the best of our knowledge, unknown.

Decisions 1-3 left a total of 5632 observations (2816 each for /d/ and /t/). To avoid over-fitting to individual talkers, we use linear mixed-effects regression rather than ordinary linear regression. This `shrinks' talker-specific estimates of the cue means towards the overall (population-level) mean, and does so more when less data is available for the particular talker. Separate regressions were use to predict VOT and f0 (Mel). Both regressions used the formula:

\begin{equation}\label{eq:c-cure-regression}
\begin{split}
cue \sim 1 + (1 | Talker)
\end{split}
\end{equation}

Rather than to just use the residuals of these regressions, we only subtracted the random effects (BLUPs) from each observation. This removes the talker-specific effects from each token (as intended by C-CuRE) but leaves observations in the original cue space (rather than residual VOTs centered around 0), thus achieving talker normalization without loss in interpretability. The R code for the steps described here is found in R markdown document for the main text, at the end of Section \ref{sec:framework}. Figures \ref{fig:chodroff-stop-VOT-f0ST-normalized} and \ref{fig:chodroff-stop-VOT-f0ST-mu-normalized} replot Figures \ref{fig:chodroff-stop-VOT-f0ST} and \ref{fig:chodroff-stop-VOT-f0ST-mu} after C-CuRE normalization has been applied to the data.



\begin{figure}

{\centering \includegraphics{../figures/knitted/chodroff-stop-VOT-f0ST-normalized-1} 

}

\caption{Same as Figure \ref{fig:chodroff-stop-VOT-f0ST} but for normalized VOT and f0. Normalization does not affect the relative placement of tokens within the talker's space but it does affect the absolute placement. This is also evident in Figure \ref{fig:chodroff-stop-VOT-f0ST-mu-normalized}}\label{fig:chodroff-stop-VOT-f0ST-normalized}
\end{figure}



\begin{figure}

{\centering \includegraphics{../figures/knitted/chodroff-stop-VOT-f0ST-mu-normalized-1} 

}

\caption{Same as Figure \ref{fig:chodroff-stop-VOT-f0ST-mu} but for normalized VOT and f0---both for the means and for the 95\% ellipse.}\label{fig:chodroff-stop-VOT-f0ST-mu-normalized}
\end{figure}

\hypertarget{sec:SI-models}{%
\section{Additional details about the change models}\label{sec:SI-models}}

This section contains additional details about the change models.

\hypertarget{sec:SI-models-changes-in-representations}{%
\subsection{Changes in category representations}\label{sec:SI-models-changes-in-representations}}

The updating of the four \(\mathcal{NW^{-1}}\) parameters after \(N\) observations of a category \(c\) from the talker is described by the equations in \eqref{eq:niw-updating-parameters}, and is deterministic \autocite[for details and derivation, see][p.~134]{murphy2012}. \(\kappa_c\) and \(\nu_c\) simply increase by 1 with each observation, capturing the fact that each observation adds additional information about the talker's category mean and covariances. \(\mathrm{m}_{N,c}\) is a weighted combination of its prior value \(\mathrm{m}_{0,c}\) and the category mean of the \(N\) observations \(\bar{x}\)---following the same logic that we applied to the inference of the overall cue mean in Equation \eqref{eq:normalization-change}. Similarly, \(\mathrm{S}_{N,c}\) is a weighted combination of its prior value \(\mathrm{S}_{0,c}\) and the category variability of the \(N\) observations,\footnote{\(\mathrm{S}_c \triangleq \Sigma_{i=1}^N x_{i,c} x_{i,c}^T\) is the uncentered sum of squares matrix of the \(N\) observations \autocite{murphy2012}.} plus an additional term that captures the uncertainty about the category mean. Figure \ref{fig:model-belief-updating} visualizes the belief-updating process in Equations \eqref{eq:niw-updating}-\eqref{eq:niw-updating-parameters} as a graphical model.

\begin{equation}\label{eq:niw-updating-parameters}
\begin{split}
\mathrm{m}_{N,c} & = \frac{\kappa_{0,c} \mathrm{m}_{0,c} + N_c \bar{x}}{\kappa_{N,c}} = \frac{\kappa_{0,c}}{\kappa_{0,c} + N_c} \mathrm{m}_{0,c} + \frac{N_c}{\kappa_{0,c} + N_c}\bar{x}_c \\
\kappa_{N,c} & = \kappa_{0,c} + N_c \\
\nu_{N,c} & = \nu_{0,c} + N_c \\
\mathrm{S}_{N,c} & = \mathrm{S}_{0,c} + \mathrm{S}_{\bar{x}_c} + \frac{\kappa_{0,c} N_c}{\kappa_{0,c} + N_c}\left( \bar{x}_c-\mathrm{m}_{0,c} \right) \left( \bar{x}_c-\mathrm{m}_{0,c} \right)^T \\
 & = \mathrm{S}_{0,c} + \mathrm{S}_c + \kappa_{0,c} \mathrm{m}_{0,c} \mathrm{m}_{0,c}^T - \kappa_{N,c} \mathrm{m}_{N,c} \mathrm{m}_{N,c}^T
\end{split}
\end{equation}

\begin{figure}
  \centering
  \tikz{ %
    % exposure
    \node[obs] (cue) {$x_n$} ; %
    \factor[above=of cue] {cuedist} {left:$\mathcal{N}$} {} {}; %
    \node[obs, right=of cuedist] (category) {$c_n$} ; %
    % category parameters
    \node[det, above=of cuedist] (mu) {$\mu_c$} ; %
    \node[det, right=of mu] (Sigma) {$\Sigma_c$} ; %
    \factor[above=of mu] {mudist} {left:$\mathcal{N}$} {} {}; %
    \factor[above=of Sigma] {Sigmadist} {left:$\mathcal{W}^{-1}$} {} {}; %
    % hyperparameters
    \node[det, above=of mudist] (kappa) {$\kappa_{n,c}$} ; %
    \node[det, left=of kappa] (m) {$\mathrm{m}_{n,c}$} ; %
    \node[det, above=of Sigmadist] (S) {$\mathrm{S}_{n,c}$} ; %
    \node[det, right=of S] (nu) {$\nu_{n,c}$} ; %
    % prior
    \factor[above=1 of m] {update_m} {} {} {}; %
    \factor[above=1.05 of kappa] {update_kappa} {} {} {}; %
    \factor[above=.9 of S] {update_S} {} {} {}; %
    \factor[above=1 of nu] {update_nu} {right:update (Equation \ref{eq:niw-updating-parameters}) } {} {}; %
    \node[det, above=of update_m] (prior_m) {$\mathrm{m}_{0,c}$} ; %
    \node[latent, above=5 of update_kappa] (prior_kappa) {$\kappa_{0}$} ; %
    \node[latent, above=5 of update_nu] (prior_nu) {$\nu_{0}$} ; %
    \node[det, above=of update_S] (prior_S) {$\mathrm{S}_{0,c}$} ; %
    % external phonetic database
    \node[obs, above=of prior_m] (expected_mu) {$\mathbf{E}(\mu_{c})$} ; %
    \node[obs, above=of prior_S] (expected_Sigma) {$\mathbf{E}(\Sigma_{c})$} ; %
    % extra nodes to allow connection from cues to updating
    \node[const, left=1.2 of update_m] (update_extra1) {} ; %
    \node[const, left=3 of cue] (update_extra2) {} ; %
    \node[const, right=1 of update_nu] (update_extra3) {} ; %
    % plates
    \plate[inner sep=0.24cm, xshift=-0.06cm, yshift=0.12cm] {plate2} {(mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S) (prior_m) (prior_S) (expected_mu) (expected_Sigma) (update_extra3) } {$\forall c \in categories $}; %
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(cue) (category) (cuedist) (mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S)} {$\forall n \in exposure\ observation$}; %
    \edge {cuedist} {cue} ; %
    \edge {category} {cuedist} ; %
    \edge {mu, Sigma} {cuedist} ; %
    \edge {Sigma} {mudist} ; %
    \edge {kappa,m} {mudist} ; %
    \edge {nu,S} {Sigmadist} ; %
    \edge {mudist} {mu} ; %
    \edge {Sigmadist} {Sigma} ; %
    % updating
    \edge {prior_kappa} {update_kappa} ; %
    \edge {prior_nu} {update_nu} ; %
    \edge {prior_kappa, prior_m} {update_m} ; %
    \edge {prior_kappa, prior_m, prior_S} {update_S} ; %
    \edge {update_kappa} {kappa} ; %
    \edge {update_nu} {nu} ; %
    \edge {update_m} {m} ; %
    \edge {update_S} {S} ; %
    \edge[-] {cue} {update_extra2} ; %
    \edge[-] {update_extra1} {update_extra2} ; %
    \edge {update_extra1} {update_m} ; %
    \edge[-] {update_m} {update_kappa} ; %
    \edge[-] {update_kappa} {update_S} ; %
    \edge[-] {update_S} {update_nu} ; %
    % link to phonetic database
    \edge {expected_mu} {prior_m} ;
    \edge {expected_Sigma} {prior_S} ;
    \edge {prior_nu} {prior_S} ;
  }
  \caption{$\mathcal{NW}^{-1}$ Bayesian belief-updating model, as employed here. We set $\mathrm{m}_{0,c} = \mathbf{E}(\mu_c)$ and $\mathrm{S}_{0,c}=\mathbf{E}(\Sigma_c)$, where $\mathbf{E}(\mu_c)$ and $\mathbf{E}(\Sigma_c)$ are observable from phonetically annotated databases. We further assume that all categories share a common $\kappa_{0,c}$ and $\nu_{0,c}$, shown as $\kappa_{0}$ and $\nu_{0}$. These $\kappa_{0}$ and $\nu_{0}$ are the only two degrees of freedom in this highly simplified model of distributional learning (in the manuscript, we keep the $c$ subscript to avoid confusion with the parameter for changes in normalization in Equation \ref{eq:normalization-change}). All other variables are either observable (filled gray circles) or fully determined by other variables.}\label{fig:model-belief-updating}
\end{figure}

The \(\kappa_{c}\)s and \(\nu_{c}\)s are also sometimes called ``pseudocounts'' because they have a rather intuitive interpretation: the value of these parameters can be seen as describing the number of observations of this category that the listener assumes to have observed from the talker. For example, a listener with \(\kappa_{c,0} = 100\) updates her beliefs about an unfamiliar talker's category mean as if she has already seen 100 observations of that category from the talker \emph{prior to having received any input from that talker}. After 900 observations of that category from the unfamiliar talker, this listener's belief about the talker's category mean would be a weighted mixture, made up to 10\% by the prior \(\mathrm{m}_{c,0}\) and to 90\% of the mean of the 900 observed category instances \(\bar{x}_c\).\footnote{The extent to which listeners transfer prior expectations to an unfamiliar talker or context (i.e., the values of \(\kappa_{c,0}\) and \(\nu_{c,0}\)) is expected to differ across phonological contrasts \autocite[see discussion in][]{kleinschmidt-jaeger2015}.}

\hypertarget{setting-the-m_0c-and-s_0c-parameters}{%
\subsubsection{\texorpdfstring{Setting the \(m_{0,c}\) and \(S_{0,c}\) parameters}{Setting the m\_\{0,c\} and S\_\{0,c\} parameters}}\label{setting-the-m_0c-and-s_0c-parameters}}

It is possible to treat \(m_{0,c}\) and \(S_{0,c}\) as free parameters of the Normal-Inverse-Wishart belief-updating model, and to infer them from participants' responses in perceptual experiments \autocite{kleinschmidt-jaeger2016cogsci}. This makes it possible to compare whether estimates of these parameters that are purely based on \emph{listeners'} behavior correspond to distributions of category means \(\mu_c\) and category covariances \(\Sigma_c\) that match those observed in the input that listeners have received throughout their lives---i.e., the joint distribution of \(\mu_c\) and \(\Sigma_c\) observed in phonetically annotated databases that are are representative of the input listeners have previously received. If such a match is indeed observed, this would lend support to the hypothesis that listeners learn and store knowledge about the statistics of cue-to-category mappings in the input \autocites[for initial tests of this type, and further discussion, see][]{kleinschmidt-jaeger2016cogsci,tan2022}.

However, it is also possible to instead set \(\mathrm{m}_{0,c}\) and \(\mathrm{S}_{0,c}\) based on phonetically annotated data. This fixes all \(J\) DFs for \(\mathrm{m}_{0,c}\) and \(\frac{J}{2}(K^2+K)\) DFs for \(\mathrm{S}_{0,c}\) prior to predicting listeners' behavior but \emph{assumes} (rather than tests) that listeners learn and store knowledge about the statistics of cue-to-category mappings in the input. This latter approach is the one that we took in our case studies. The core idea is to select \(\mathrm{m}_{0,c}\) and \(\mathrm{S}_{0,c}\) so that they yield a prior joint distribution of \(\mu_c\) and \(\Sigma_c\) that match those observed in a sufficiently large phonetic database that can reasonably assumed to approximate the type of input an average participant has received throughout their life.

For the present purpose, we simplify this estimation problem further by setting \(\mathrm{m}_{0,c}\) and \(\mathrm{S}_{0,c}\) so that the expected values of the \emph{marginal} distributions of \(\mu_c\) and \(\Sigma_c\) match the maximum likelihood estimates of the category mean and covariance matrix, respectively, in the phonetic database \autocite{chodroff-wilson2018}. This means that we set \(\mathrm{m_{0,c}}\) to the empirical mean (i.e., maximum likelihood estimate) of the category's cue distribution, \(\bar{x}_c\), estimated from the phonetic database. Since the Normal-Inverse-Wishart belief-updating model describes the prior distribution of the category mean \(\mu_c\) as a Normal distribution with mean \(\mathrm{m}_{0,c}\) and covariance matrix \(\frac{1}{\kappa_{0,c}}\Sigma_c\), this yields an expected prior category mean \(\mathbf{E}(\mu_c) = \mathrm{m_{0,c}} = \bar{x}_c\).

The prior distribution of the category covariance matrix \(\Sigma_c\) is described by the Inverse-Wishart distribution with scale parameter \(\mathrm{S_{0,c}}\) and degree of freedom \(\nu_{0,c}\). For our case studies, we thus again set \(\mathrm{S_{0,c}}\), such that the expected prior category covariance matrix matches the empirical covariance matrix \(\frac{\bar{S}_c}{N}\) (i.e., the empirical sum of squares matrix divided by the number of observations) estimated from the phonetic database \emph{given \(\nu_{0,c}\)}. This yields \(\mathrm{S_{0,c}} = \mathbf{E}(\Sigma_c)(\nu_{0,c}-D-1)\) = \(\bar{S}_c(\nu_{0,c}-D-1)\), where \(D\) is the dimensionality of input \autocite[i.e., the number of phonetic cues considered, cf.][p.~134-5]{murphy2012}.

We note that the approach employed in our case studies does not take into account that category means and covariance matrices can be correlated. In that case, the expected values of the marginal distributions of \(\mu_c\) and \(\Sigma_c\) will not be identical to the expected value of the joint distribution of \(\mu_c\) and \(\Sigma_c\).\footnote{This might also reveal that some of the assumptions of the Normal-Inverse-Wishart belief-updating model are problematic, either for researchers or for listeners.} For future work, it would thus be more appropriate to estimate the \emph{joint} distribution of \(\mu_c\) and \(\Sigma_c\) from phonetic data, and to find the values for \(\mathrm{m}_{0,c}\) and \(\mathrm{S}_{0,c}\) that best approximate this distribution (e.g., through moment-matching). For the present purpose, however, this change is not expected to qualitatively affect the conclusions reached in our case studies.

\hypertarget{sec:SI-models-changes-in-decision-making}{%
\subsection{Changes in decision-making}\label{sec:SI-models-changes-in-decision-making}}

Here, we model the prediction error as the surprisal experienced when observing the category label, \(I(c_{observed})\). This surprisal is a log-inverse function of the posterior probability of the category label given the beliefs held by the listener prior to observing the category label.

\begin{equation}\label{eq:bias-updating}
\begin{split}
\mathrm{logit}(\pi_{n,c_{observed}}) & = \mathrm{logit}(\pi_{n-1,c}) + \beta_{\pi} I(c_{observed}) \\
                          & = \mathrm{logit}(\pi_{n-1,c}) - \beta_{\pi} \log_2 p_{n-1}(c_{observed}) \\
\end{split}
\end{equation}

The same amount that is added to the bias of the observed (labeled) category is subtracted from the response biases for all other categories (uniformly distributed across those categories).
Like the representational change model, the change model for response biases is sensitive to the mismatch between listeners' expectations based on the input and labeling information provided by the context. Unlike the representational change model, however, the change model for response biases uses the new observations to update response biases rather than beliefs about category likelihoods. As such, the change for response biases does not \emph{directly} change the mapping from stimulus properties to categorization responses. The change model for response biases can, however, affect this mapping indirectly. This means that even simple changes in response biases can change listeners' categorization function in complex ways.

We also briefly considered another simple change model for decision-making but never implemented it since it could not possibly explain the findings we present in Sections \ref{sec:PR} and \ref{sec:AA}. This alternative model holds that listeners aim to infer the relative probability of each category based on recent input, and that response biases reflect these estimates. For example, listeners might enter an experiment with expectations about the relative probability of each category based on their relative frequency in previously experienced input, and then update their expectations based on the relative frequency of the categories within the experiment \autocites[similar to belief-updating models of syntactic adaptation,][]{fine-jaeger2013,jaeger2019,prasad2021}. However, such a model could not possibly explain exposure effects in, for example, a typical perceptual recalibration experiment since the relative frequency of the two categories does not differ between exposure conditions.

\hypertarget{sec:consequences-of-lambda}{%
\subsubsection{\texorpdfstring{(Non)-additivity of changes as a function of \(\lambda\)}{(Non)-additivity of changes as a function of \textbackslash lambda}}\label{sec:consequences-of-lambda}}

In Figures \ref{fig:demonstrate-lapse-bias-change} and \ref{fig:demonstrate-lapse-bias-change-nonzero-lapse} in the main text, we demonstrate that the (non)-additivity of changes in decision-making depends on \(\lambda\). Specifically, changes are additive in the log-odds of the posterior probability of categories if and only if \(\lambda \in \{0, 1\}\). To see how this limitation arises, consider how the \emph{log-odds} of a category---e.g., /d/---in Equation \eqref{eq:posterior-probability-lapse} depend on the response biases \(\pi_c\) when \(\lambda=0\):

\begin{equation}\label{eq:change-bias}
\begin{split}
p(/d/ | input) & = (1-\lambda) \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/}}{\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} + \lambda \frac{\pi_{/d/}}{\Sigma_i \pi_{c_i}} \\
 & = (1-\lambda) \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/}}{\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} + \lambda \pi_{/d/} \\
 & = \frac{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/} + \lambda \pi_{/d/} \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}}{\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} \Rightarrow \\
\log \frac{p(/d/ | input)}{p(/t/ | input)}  & = \log \frac{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/} + \lambda \pi_{/d/} \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}}{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right) \pi_{/t/} + \lambda \pi_{/t/} \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} \\
 & = \log \frac{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) + \lambda \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}}{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right)  + \lambda  \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} + \log\frac{\pi_{/d/}}{\pi_{/t/}} \\
\end{split}
\end{equation}

When \(\lambda=1\), this simplifies to:

\begin{equation}
\begin{split}
\log \frac{p(/d/ | input)}{p(/t/ | input)} & = \log\frac{\pi_{/d/}}{\pi_{/t/}}
\end{split}
\end{equation}

And for \(\lambda=0\):

\begin{equation}
\begin{split}
\log \frac{p(/d/ | input)}{p(/t/ | input)} & = \log \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right)}{\mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right)} + \log\frac{\pi_{/d/}}{\pi_{/t/}} 
\end{split}
\end{equation}

Either way, a change in responses (\(\pi\)) will result in a change in predicted categorization behavior that is independent of the \(input\), leading to changes in the log-odds of categorization responses that are constant across the acoustic-phonetic space.

Even when \(\lambda \not\in \{0,1\}\), equation \eqref{eq:change-bias} suggests that the type of changes in posterior log-odds that can be explained through changes in decision-making are constrained. For example, when the two categories exhibit equal variance (so that \(\log \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right)}{\mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right)}\) describes a line), changes in decision-biases are predicted to lead to changes in categorization that are most extreme between the two category means and converge against two different constants `outside' of the two category means (see Figure \ref{fig:lapse-rate-consequences}). The shape of the predicted changes further gains in complexity if the variances of the two categories differ (Figure \ref{fig:lapse-rate-consequences-unequal}). Further study of this constraint and similar limitations of the other changes models is needed to characterize the range of adaptive behaviors each model can predict (see Recommendation 5 in the general discussion).



\begin{figure}

{\centering \includegraphics{../figures/knitted/lapse-rate-consequences-1} 

}

\caption{Predicted difference in posterior log-odds of /d/ as a function of the lapse rate (\(\lambda\)), prior response bias for /d/ (\(\pi_{/d/}\)) and the change in that response bias after exposure (\(\Delta_{\pi_{/d/}}\)). For this example, only VOT is considered. The mean of /d/ and /t/ were set to that observed in Chodroff \& Wilson (2018) and the variance was held equal across the two categories (at the average of the variances observed in Chodroff \& Wilson, 2018).}\label{fig:lapse-rate-consequences}
\end{figure}



\begin{figure}

{\centering \includegraphics{../figures/knitted/lapse-rate-consequences-unequal-1} 

}

\caption{Same as in Figure \ref{fig:lapse-rate-consequences} but while setting the variances of the two categories to the unequal values observed in Chodroff \& Wilson (2018).}\label{fig:lapse-rate-consequences-unequal}
\end{figure}

\hypertarget{sec:SI-PR}{%
\section{Creating the stimuli for the (simulated) perceptual recalibration paradigm}\label{sec:SI-PR}}

As also mentioned in the main text, many studies on perceptual recalibration do not report the acoustic properties of the exposure and test stimuli,\footnote{Some studies provide aggregate information \autocites[e.g.,][]{kraljic-samuel2006,kraljic-samuel2007} and very few provide detailed information and visualization \autocite[e.g.,][]{drouin2016}.} and analyses that relate the acoustic properties of exposure and test stimuli to participants' responses during test remain the exception. In our experience, it is also not uncommon that original recordings are no longer available or only partially available (see one of our recommendations in the general discussions). Phonetic annotations that aid the extraction of acoustic information about these recordings are available for only a very small number of studies.\footnote{Some notable exceptions for research on US English include the labs of Drs. Babel (UBC), Myer (UConn), and Theodore (UConn). In an ongoing project at Rochester, we have collected a database of phonetically annotated perceptual recalibration experiments on L1 US English /s/-/\ipatext{ʃ}/ that links stimulus recordings, phonetic annotations, and over 20 phonetic extracted phonetic cues to categorization responses from several thousand participants across a dozen experiments from multiple labs. Researchers interested in the database can contact any of the authors.} For that reason, we use simulated data in our case study on perceptual recalibration. Our stimulus generation procedure aims to capture the qualitative properties of the most common approaches to stimulus selection in perceptual recalibration experiments.

\hypertarget{exposure}{%
\subsection{Exposure}\label{exposure}}

As described in the main text, the exposure phase of a typical perceptual recalibration experiment employs both typical stimuli and stimuli that are manipulated to be perceptually ambiguous between the two categories. In practice, researchers determine this point of perceptual ambiguity by first generating a continuum from the recording of the typical sound (e.g., \emph{crocodile}) to a recording in which that sound is replaced with the opposite sound (\emph{crocotile}). Procedures to create these continua range from simple blending of the two recordings---mixing the two recordings weighted by different amplitudes (from 100\% \emph{crocodile} and 0\% \emph{crocotile} to 0\% ``crocodile'' and 100\% \emph{crocotile})---to more careful phonetic manipulations (e.g., inserting addition silence to create longer VOTs) or the use of speech synthesis \autocite[for an insightful critique of the frequently used blending procedure, see][]{theodore-cummings2021}. The former is the by far more common approach. The perceptually most ambiguous point along the resulting continua is then determined either by the experimenter(s) or in a separate norming experiment, with the former approach being far more common. In short, perceptual recalibration experiments do neither carefully select the tokens within each category based on phonetic properties, nor is there any form of counter-balancing of the average phonetic or perceptual shifts across the two bias conditions.

For the sake of generality, we simulate the general outcome of any of these procedures by imaging an experimenter (or participants) listening to stimuli along a continuum ranging from typical /d/ to typical /t/. We simulate the experimenter with the same perceptual model we assume for general L1-listeners (see Section \ref{sec:framework}). We assume that the experimenter, who can listen arbitrarily often to any of the stimuli, will make her final decision as to which stimuli should be selected for the shifted tokens without attentional lapses (\(\lambda=0)\). We further assume that experimenter's response bias is affected by the lexical context. Specifically, we set \(\pi_{intended~category} =\) \(p(intended~category | lexical~context) =\) 0.7. This captures the fact that even the experimenter's perception is somewhat affected by the lexical context. Figure \ref{fig:PR-exposure-test-plot}A shows the phonetic properties of an instance of stimuli generated in this way.

\hypertarget{test}{%
\subsection{Test}\label{test}}

For the test phase, we followed a similar procedure. We used the same continuum from a typical /d/ to a typical /t/, and then selected six stimuli along that continuum that would be expected to yield 15\%, 35\%, 45\%, 55\%, 65\%, 85\% /d/-responses in a norming experiment without any prior exposure to the talker's speech. Since the test tokens present the phonetic contrast in a non-biasing context (e.g., /\ipatext{ɪ}\_\ipatext{ɪ}/), we set the effect of lexical context to \(p(category | lexical\ context) = .5\). The resulting test tokens shown in Figure \ref{fig:PR-exposure-test-plot}B closely resemble the placement of test stimuli expected in experiments on perceptual recalibration \autocites[e.g.,][]{norris2003,kraljic-samuel2005,kraljic-samuel2006}: as is typical for such experiments, test tokens are placed primarily where they are expected to be perceptually most ambiguous prior to exposure (i.e., close to the prior category boundary), with one additional test token towards each end of the continuum (typically somewhere between 10-25\% and 75-90\%, respectively). Our approach further assumes that the exposure and test stimuli are carefully chosen to have phonological contexts that allow generalization from exposure to test, as differences in the types of phonological context between exposure and test can reduce or even completely block perceptual recalibration \autocite{eisner2013,mitterer2013}. For example, the original study that we model our procedure on mostly used exposure stimuli in which /d/ or /t/ occurred word-medially at the onset of a syllable, between two vowels \autocite[e.g., \emph{crocodile}, \emph{academic}, etc., see Table 1 in][]{kraljic-samuel2006} or at least between two sonorants (e.g., \emph{legendary}, \emph{secondary}). In the test stimuli, the /d/-/t/ contrast occurred in a similar position (either /a\_a/ or /\ipatext{ɪ}\_\ipatext{ɪ}/).

\hypertarget{sec:SI-AA}{%
\section{Creating the stimuli for the (simulated) accent adaptation paradigm}\label{sec:SI-AA}}

As is the case for experiments on perceptual recalibration, the majority of studies on accent adaptation do not report the acoustic properties of the exposure and test stimuli, or present analyses that relate those properties to participants' responses.\footnote{Notable exceptions include, for example, the labs of Drs. Chodroff (UZurich), Kartushina (UOslo) and Schertz (UToronto). In past work, we have analyzed to what extent the results of experiments on accent adaptation (including differences between experiments) can be explained by the specific phonetic properties of the stimuli \autocites[e.g.,][]{tan2021,xie2017,xie2021jep}. These studies find that the results of experiments on accent adaptation can strongly depend on the choice of stimuli. On the one hand, this should not be surprising. But it is often not considered in the interpretation of seemingly unexpected results.} The stimulus generation procedure described below mimics a scenario that has been reported in previous work \autocite[e.g.,][]{schertz2015}.

\hypertarget{exposure-1}{%
\subsection{Exposure}\label{exposure-1}}

We simulate a scenario where the /t/ category remains unchanged between the L1- and L2-accents while the /d/ category is altered in the L2 accent. In the specific simulations presented in the main text, the cue weighting was changed so that the primary cue for L1 listeners (VOT) becomes the secondary cue in the L2-accented speech. This was achieved by adjusting the category mean of /d/ so that its distance from the category mean of /t/ is reduced along VOT but increased along f0, while preserving the relative ordering (i.e., the category mean of /d/ has smaller f0 and VOT than that of /t/). Additionally, we assume no difference in category covariance between the L1 and the L2 accents. This was done to focus our analysis on the influence of relative locations of the two categories between L1 and L2, keeping the effects of category dispersion constant. We note, however, that L1 and L2 categories also often differ both the location (category means) \emph{and} their dispersion \autocites[category variance-covariance, e.g.,][]{schertz2015,xie-jaeger2020}. The code in this R markdown document can be straightforwardly extended to simulate such cases as well.

\hypertarget{test-1}{%
\subsection{Test}\label{test-1}}

For the test phase, we randomly sample 60 tokens per category from the L2-accented categories. This procedure matches the approach typically taken by studies on accent adaptation: researchers record L2-accented speakers' productions of the target categories without making specific selection based on acoustic-phonetic features.

\hypertarget{optimization-steps-for-case-study-2}{%
\section{Optimization steps for Case Study 2}\label{optimization-steps-for-case-study-2}}

Figure \ref{fig:optimization-results} summarizes the effects of parameterizations for each of the three change models on the overall accuracy for L2-accented speech after L2-accented exposure in Case Study 2.



\begin{figure}

{\centering \includegraphics{../figures/knitted/optimization-results-1} 

}

\caption{Parameter space explored by the search for the best-performing parameterizations of the three different change models in Case Study 2.}\label{fig:optimization-results}
\end{figure}

\hypertarget{sec:sufficiency}{%
\section{\texorpdfstring{Computational limitations of change models afford qualitative tests of their \emph{sufficiency}}{Computational limitations of change models afford qualitative tests of their sufficiency}}\label{sec:sufficiency}}

The three change models---as well as the three general hypotheses that they aim to implement---differ in their computational assumptions. Given these assumptions, it is possible to conduct experiments that can decisively test the \emph{(in)sufficiency} of any of the three mechanisms. Indeed, though not necessarily framed as such, some previous findings already speak to the (in)sufficiency of the three mechanisms. Here we summarize some of those key findings, and describe possible extensions to them. We note, however, that---as we outlined in the general discussion---we do not believe that such (in)sufficiency tests are themselves sufficient to advance research on adaptive speech perception.

\hypertarget{normalization-vs.-changes-in-category-representations}{%
\subsection{Normalization vs.~changes in category representations}\label{normalization-vs.-changes-in-category-representations}}

While both the normalization and the representational change model assume that listeners are sensitive to talker-specific changes in the usage of phonetic cues, only the latter tracks those differences separately for each category (e.g., /d/ vs /t/). This makes normalization computationally more parsimonious than representational changes for both researchers and listeners: for the normalization model, the number of parameters that researchers need to determine (e.g., fit from the behavioral data of perception experiments), and the number of estimates that listeners need to infer and store from the speech input does not increase with the number of categories \autocite[for related discussion, see also][]{apfelbaum-mcmurray2015}. For example, for the C-CuRE-based normalization model employed in our case studies, researchers need to determine only one parameter (\(\kappa_0\)) and listeners are assumed to infer and store only the \emph{overall} means of all cues (\(K\) values for \(K\) cues). In contrast, the representational change model employed in our case studies requires researchers to determine two parameters \emph{per category}, and listeners are assumed to infer and store the cue means and covariance matrices of \emph{each category} (\(JK + \frac{J}{2}(K^2+K)\) values for \(K\) cues and \(J\) categories).\footnote{Even if C-CuRE normalization, which only centers cues, is made more comparable to the representational change model by extending normalization to include cue standardization \autocite[also known as z-scoring, as used in, e.g., Lobanov normalization,][]{lobanov1971}, this would introduce just one additional parameter for researchers (the equivalent for \(\kappa_0\) but for the estimation of the cue variances), and listeners would be assumed to learn and store \(2K\) values for \(K\) cues.}

While the parsimony of normalization offers a computational advantage in terms of efficiency, it also comes with limitations. We discuss three such limitations that future work can exploit to evaluate the sufficiency of this mechanism. First, normalization accounts predict that the effects of exposure on subsequent perception do not depend on the category membership inferred by listeners. For example, for C-CuRE, only the overall cue mean of the input can affect subsequent perception, regardless of whether the lexical context labels the input as one category or another.\footnote{To be precise, some normalization accounts allow the category identities of \emph{surrounding} segments of speech to affect the normalization of cues on the target segment. This is, for example, how C-CuRE normalizes for effects of surrounding phonetic context \autocite{mcmurray-jongman2011}.} Although not originally discussed in the context of normalization, there is evidence that challenges this prediction. In their seminal study on perceptual recalibration, \textcite{norris2003} exposed participants to /f/- or /s/-biased inputs using the same general design that we discussed for Case Study 1. As is now typical for perceptual recalibration experiments, participants were exposed either to words with typical /f/ and words with atypical (shifted) /s/ or to words with typical /s/ and words with atypical (shifted) /f/. This resulted in the signature perceptual recalibration effect.

Of relevance to the present discussion, one of several control experiments conducted by Norris and colleagues exposed participants either to words with typical /f/ and \emph{non}-words with atypical (shifted) /s/, or to words with typical /s/ and \emph{non}-words with atypical (shifted) /f/. The atypical /f/ and /s/ sounds in these control conditions were acoustically identical to those in the experimental conditions, and the non-word contexts in the control conditions matched the word contexts in terms of the phonetic context surrounding the critical /f/ and /s/ sounds (specifically, in terms of lexical stress and the vowel immediately /f/ or /s/). In short, the two control conditions differed from the experimental conditions almost exclusively in whether the atypical inputs were lexically labeled to be of a particular category. Unlike the experimental conditions, however, the control conditions did \emph{not} elicit the signature boundary shift \autocite[Experiment 2]{norris2003}. Perceptual recalibration thus seems to depend on the category that the shifted atypical tokens are attributed to \autocite[p.~227]{norris2003}---contrary to what would be expected if adaptive speech perception was solely achieved through cue normalization.\footnote{There might be a way to repair this apparent deficiency of normalization accounts that is compatible with the central idea behind C-CuRE. If the inferred category of a token is included in the contextual factors used to remove expectations from the observed cues, then the findings of \textcite{norris2003} can be accounted for (since the category label is unknown when the token is embedded in a non-word context). Specifically, normalization would have to remove from each cue expectations due to the token's inferred category and then add those expectations back in prior to interpreting the cue (e.g., following the same approach we employed throughout this study with regard to talkers; see \ref{sec:SI-applying-C-CuRE}). This modification to C-CuRE would entail that top-down feedback from category representations can affect the interpretation of cues (though one might argue that C-CuRE already makes the same assumption with regard to \emph{surrounding} phonological and lexical context)---an assumption compatible with neuro-anatomical evidence of feedback connections from cortical to subcortical areas involved in auditory processing but potentially in conflict with some theories and findings \autocites[e.g.,][]{norris2000a,norris-mcqueen2008}. This revision of C-CuRE would make it increasingly similar to, but still slightly more parsimonious than, representational change models (since the revised C-CuRE would assume \emph{independent} effects of talkers and categories). We are not aware that this change to C-CuRE has been discussed in the literature.}

A second computational limitation of normalization accounts is specific to accounts that only correct for differences in the overall mean of cues but not differences in cue variability (like C-CuRE). In a ground-breaking study that was targeted at a separate question, \textcite{clayards2008} exposed participants to distributions of synthesized /b/ and /p/ tokens (as in, e..g, \emph{beach}-\emph{peach} continuum). Between participants, the VOT of these tokens had been manipulated to either form wide or narrow VOT distributions for both /b/ and /p/. The VOT means of /b/ and /p/ were identical in both conditions. Since acoustic cues are encoded relative to the cue mean only in C-CuRE and similar normalization accounts, these accounts thus do \emph{not} predict any differences in the effects of these exposure conditions. Clayards and colleagues, however, found that participants in the wide variance condition exhibited shallower categorization functions along the VOT continuum \autocite[conceptually replicated in][]{nixon2016}---as predicted by representational change models \autocites[see][]{clayards2008,kleinschmidt-jaeger2015,theodore-monto2019}. Future research should test whether this finding replicates for more natural-sounding (rather than resynthesized, robotic-sounding) stimuli and in situations where task demands more closely resemble those of everyday speech perception (less repetition, more lexical heterogeneity, words presented in sentential contexts rather than in isolation, etc.). If such replications are obtained, this would argue that normalization would at least have to include standardization or similar corrections for the variability of cues \autocites[as proposed in][]{johnson2020,lobanov1971,monahan-idsardi2010}.

A third limitation of normalization accounts that can be productively explored in future research is the lack of category specificity. Consider for example, a possible extension of the study by Clayards and colleagues. In the study by \textcite{clayards2008}, both /b/ and /p/ either had narrow variance along VOT (SD = 8 msecs) or wide variance (SD = 14 msecs). This confounds the category-specific variance with the overall variance of the cues along VOT. It is, however, possible to manipulate the variance of the two categories while keeping both the means of the two categories (and thus the overall cue mean) and the overall cue variance constant. Figure \ref{fig:proposed-experiment-asymmetric-variance} depicts two exposure conditions that achieve this, along with recognition accuracies predicted by the normalization model and the representational change model.\footnote{The specific predictions for the representational change model shown here assume that listeners have equally strong prior beliefs about the variance of both of the two categories (i.e., one \(nu_{0,c}\) for both categories). This is what we assumed in our case studies---which aimed to show that even simplified versions of each change model can explain a wide range of findings in the literature---but it is not an inalienable assumption for representational change models \autocite[for discussion, see also][]{kleinschmidt-jaeger2015}. The general prediction of representational change models that listeners are sensitive to the category-specific variance should hold even if this assumption is removed. How easy to detect this predicted difference is, however, expected to depend on this assumption.} This demonstrates how researchers might use the lack of category-specificity to disentangle normalization and representational change accounts. Other possible manipulations in this vein could include category-specific changes in the \emph{co}variance of cues. Basically, any manipulation that leaves the \emph{overall} cue mean and variance unaffected while predicting differences in the categorization function under the representational change model can test whether the human listeners exhibit more flexibility than expected by normalization accounts. We note, however, that any such test should take into account that listeners can have strong prior beliefs based on the speech input they have received previously, and that this might include strong prior beliefs about \emph{how} the realization of categories varies across talkers \autocite{kleinschmidt-jaeger2015}. If the manipulations employed by researchers strongly violate those expectations, this needs to be carefully considered in the derivation of predictions.



\begin{figure}

{\centering \includegraphics{../figures/knitted/proposed-experiment-asymmetric-variance-1} 

}

\caption{A possible way to contrast normalization and representational change accounts. \textbf{Panel A:} two exposure conditions with identical overall means and variances along VOT, but different category-specific variances. \textbf{Panels B and C:} the predictions of the best-performing representational and normalization change models. For this purpose, the normalization model was extended to both center and standardize the cues.}\label{fig:proposed-experiment-asymmetric-variance}
\end{figure}

\hypertarget{changes-in-decision-making-vs.-changes-in-category-representations}{%
\subsection{Changes in decision-making vs.~changes in category representations}\label{changes-in-decision-making-vs.-changes-in-category-representations}}

In Section \ref{sec:framework}, we showed that the limitations of change models for decision-making are less well understood than sometimes assumed. One recommendation for future research is thus to further explore the mathematical limitations of decision-making change models. Designs that limit attentional lapses to basically zero \autocites[e.g., by employing more engaging tasks, as in gamified paradigms,][]{wade-holt2005,lim-holt2011} would emphasize the computational limitations of decision-making change models. In such scenarios, changes in response biases can only explain changes that are additive in the posterior log-odds of categories (Section \ref{sec:change-bias}). That is, changes in response biases cannot account for changes in the \emph{slope} of categorization functions. Future studies should use exposure conditions for which such changes are predicted by representational or normalization models to test whether changes in response biases are sufficient to explain adaptive speech perception. Since it can be difficult to detect changes in categorization slopes---especially without making strong linearity assumptions, we suspect that this question is better explored by manipulating the relative reliability of two cues (see also Figure \ref{fig:show-model-categorization-3D-plots-best-performing}). Such manipulations are routinely used in a paradigm known as ``dimension-based statistical learning'' \autocite{idemaru-holt2011,lehet-holt2020,liu-holt2015}. With recent trends towards analyses that more transparently link phonetic cues to changes in participants' responses \autocites{idemaru-holt2020}[see also Section \ref{sec:methodological-advances}]{schertz-clare2020} this field of study is in a good position to test the sufficiency of changes in decision-making accounts. An ongoing project from one of our labs, uses ASP-like simulations in combination with experimental designs that are intended to directly address this question \autocite{burchill-jaeger2022}.

Conversely, there are ways to assess whether representational changes alone are sufficient to explain all forms of adaptive speech perception. For example, if auditory input that arguably carries no information about category statistics affects subsequent speech perception, this provides evidence that changes in representations alone cannot explain all aspects of adaptive speech perception. Perhaps one of the most convincing demonstrations of this type comes from the findings of auditory enhancement effects, wherein non-speech stimuli (e.g., pure tones) can systematically alter the perception of subsequently played speech stimuli \autocites{chodroff-wilson2020,holt2001,holt2005,holt2006,huang-holt2011}[for review, see also][]{weatherholtz-jaeger2016}. While these demonstrations might be challenged for lack of ecological validity (potentially inviting meta-reasoning about experimenters' intentions that is unlikely to be present during everyday speech perception), it is unclear how representational changes---or, for that matter, changes in response biases---can explain such findings. At the very least then, these findings suggest that normalization \emph{can} be involved in adaptive speech perception.

In sum, by focusing on the computational assumptions of the different change models, it is possible to conduct behavioral experiments that can decisively determine whether either of the two computationally more parsimonious change models is \emph{sufficient} to explain adaptive speech perception, or whether changes in representations are necessary to explain adaptive speech perception.

\hypertarget{sec:DataSharing}{%
\section{Advanced standards of data annotation and sharing}\label{sec:DataSharing}}

As we mentioned under Recommendation 1, predicting adaptive changes of recognition requires estimates of the acoustic-phonetic properties of both input listeners have received prior to the experiment and the input they receive during the experiment. The former can be obtained from sufficiently large phonetically annotated databases that capture the type of speech input a typical participant in the experiment is likely to have received throughout their life. Fortunately, large and phonetically annotated databases can offer good estimates of the speech input of listeners receive. Such databases are now available for an increasing number of phonetic contrasts and languages \autocites[e.g.,][]{chodroff-wilson2018,clopper-pisoni2006,hillenbrand1995,newman2001,theodore2009}[among many others]{xie-jaeger2020}.

One caveat is that many of these databases either only contain a subset of the speech varieties that an average listener of a language has plausibly been exposed to or contain very little data for each variety. In particular, databases that contain both a large number of talkers and a large number of tokens per talker continue to be the exception. Researchers thus need to carefully consider these implications when employing databases. In some cases, researchers might find that the best way forward is to collect phonetically annotated data that meets the specific requirements for their study \autocites[see][]{mcmurray-jongman2011,persson-jaeger2023,xie2021cognition,tan2021}. An alternative option is to eschew phonetically annotated data in favor of computational methods that work from the raw signal or some automatically obtained transformation of the raw data \autocite[e.g., mel-frequency cepstral coefficients,][]{Mermelstein1976}. Such models have been developed for automatic speech recognition and have been employed to model human language acquisition \autocite{dupoux2018,feldman2013}. More recently, they have also been applied to address questions about the effects of recent exposure \autocite{richter2017}. The ASP framework can, at least in theory, be combined with such models.

The second set of estimates---the acoustic-phonetic properties of the stimuli---require researchers to phonetically annotate the stimuli in their perception experiments. Such annotations entail a substantial but manageable effort: perception experiments typically employ a small number of speech stimuli that are repeated across participants. A typical perceptual recalibration experiment would require the annotation of fewer than 100 isolated word recordings. A large study on accent adaptation like Bradlow and Bent's (2008) Experiment 2 would require the annotation of about 1000 sentences. As a reference point, studies on phonetic production regularly annotate data sets many times larger. Researchers' efforts will be supported by clear standards for reproducibility and software developments that aid phonetic annotation and data sharing \autocites[e.g.,][]{cassidy-schmidt2017,picoral2021,roettger2019,winkelmann2017}. If annotations are not reported and shared---and ideally even if they are---then all audio recordings should be shared in an open and accessible way (e.g., OSF). This will require perception researchers to use human subject protocols that gives them the consent to distribute stimulus recordings for the purpose of scientific inquiry. In other words, perception and production researchers should make concerted efforts to link the listener's knowledge of productions and inferences they make during perception.

\printbibliography
\erefsection


\printbibliography

\end{document}
