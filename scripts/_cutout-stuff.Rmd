---
title: "Most experiments on exposure effects in speech perception do not distinguish between underlying mechanisms"
subtitle: "A computational review"
date: "`r format(Sys.time(), '%B %d, %Y')`"
thanks: "We are grateful to Meghan Clayards and Eleanor Chodroff for sharing their data, doing so in an accessible format, and for helping prepare their data for this study. We thank the participants in the 2021 summer 'mega-lab' meetings for many insightful discussions that shaped the perspectives discussed here (all mistakes remain our own). This includes, in particular, Ann Bradlow, Melissa Baese-Berk, Eleanor Chodroff, Jennifer Cole, Shawn Cummings, Laura Dilley, James McQueen, Arty Samuel, Maryann Tan, and Rachel Theodore. We also owe thanks for early feedback on this project to Marc Allassonnière-Tang, Jing Liu, Gerda Melnik, and Anna Persson."
author:
  - name: Xin Xie^[Department of Language Science, UC Irvine]
  - name: T. Florian Jaeger^[Department of Brain & Cognitive Sciences, University of Rochester]
  - name: Chigusa Kurumada^[Department of Brain & Cognitive Sciences, University of Rochester]
keywords:
  - speech perception
  - accent adaptation
  - perceptual recalibration
  - computational modeling
header-includes: 
 - \usepackage{animate}
 - \usepackage{amsmath}
 - \usepackage{tikz}
 - \usetikzlibrary{bayesnet}
 - \usepackage{booktabs}
 - \usepackage{siunitx}
 - \usepackage{soul}
 - \usepackage{tabto}
 - \usepackage{xcolor}
 - \usepackage{placeins}
 - \setstcolor{red}
 - \usepackage{sectsty}
 - \sectionfont{\color{red}}
 - \subsectionfont{\color{red}}
 - \subsubsectionfont{\color{red}}
 - \usepackage{setspace}\doublespacing
output: 
  papaja::apa6_pdf:
    citation_package: biblatex
    extra_dependencies: "subfig" 
    latex_engine: xelatex
    includes:
      in_header: header.tex
    fig_caption: yes
    fig_width: 2.5
    number_sections: yes
    toc: no
    toc_depth: 3
  word_document:
    toc: yes
always_allow_html: true
fontsize: 11pt
bibliography: [library.bib]
link-citations: yes
csl: apa-6th-edition.csl
---

```{r}
# For brms
priors.weakly_regularizing <- c(
  # weakly regularizing prior for fixed effect coefficients.
  prior(student_t(3, 0, 2.), class = b), 
  # weakly regularizing prior for random effect SDs.
  prior(cauchy(0, 2), class = sd),
  # weakly regularizing prior for random effect correlations.
  prior(lkj(1), class = cor))
```

### Estimating the typical (population-level) category-specific distribution of VOT {#sec:estimating-expected-category-statistics}
We use Bayesian hierarchical (mixed-effects) *distributional* linear regression to infer the population-level mean and standard deviation of each stop category in the database. Together, these population-level estimates are taken to describe the `typical' likelihood function of each category. This approach is an extension of linear mixed-effects models (LMMs). In addition to predicting the mean of an outcome that is assumed to be normally distributed, we predict both the mean and the standard deviation of that outcome. The use of hierarchical distributional regression has a number of advantages of standard approaches to estimating typical category likelihoods (such as simply aggregating the data by-talker to calculate by-talker means and standard deviations):

 * It recognizes the hierarchical structure of the data (see also REF-nalborczyk2019), estimating population-level effects while taking into account uncertainty about individual differences between talkers and between lexical contexts:
    * it efficiently accounts for variability in category likelihoods across talkers.
    * it efficiently accounts for variability in category likelihoods across lexical (and phonological) contexts.
    * it is more suitable of imbalanced data, with different amounts of information per talker and/or lexical contexts. Estimates of by-talker or by-lexical context differences are subject to `shrinkage' that is stronger for talkers/lexical contexts with fewer observations. This ameliorates the risk of over-fitting.
  * It estimates the mean and standard deviation of each category *jointly*. This also allows us to take into account correlations between these two parameters.
  * While we make the simplifying assumption of Gaussian categories, the approach taken here can be applied to outcomes that are assumed to follow non-Gaussian distributions (e.g., lognormal or skew normal distributions).
  * While we focus on just one cue here (VOT), the approach taken here extends to multiple phonetic cues (multivariate distributional mixed-effects regression), allowing us to model within- and cross-category correlations between different cues.

Specifically, we predict the mean VOT of each of the six stop categories (5 population-level DFs), while allowing random (normal) variation in these means by talkers (intercepts, slopes and their correlations, $\frac{1}{2}(6^2-6) = 15$ group-level DFs) and lexical contexts (intercepts only since stop vary *between* lexical context, 1 group-level DF). Simultaneously, we predict the (log-transformed) standard deviation of each of the six stop categories, using the exact same formula. Finally, the model predicts the group-level correlations between means and standard deviations across both talkers () and lexical contexts (1 group-level DF).

We fit this model using the function \texttt{brm} from library \texttt{brms} [@burkner2019] in \texttt{R} [version 3.5.2 @R]. \texttt{brms} provides an interface to \texttt{Stan} for Bayesian generalized mixed-effects models, including distributional regression [@burkner2017]. \texttt{Stan} [@carpenter2017] allows the efficient implementation of Bayesian data analysis through No-U-Turn Hamiltonian Monte Carlo sampling. We follow common practice and use weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we use Student priors centered around zero with a scale of 2.5 units [following @gelman2008] and 3 degrees of freedom. For random effect standard deviations, we use a Cauchy prior with location 0 and scale 2, and for random effect correlations, we use an uninformative LKJ-Correlation prior with its only parameter set to 1 [@lewandowski2009], describing a uniform prior over correlation matrices. The model was fit using four chains with 1,000 post-warmup samples per chain, for a total of 4,000 posterior samples. Each chain used 1,000 warmup samples to calibrate Stan's No U-Turn Sampler. All analyses reported here converged (e.g., all $1 \leq \hat{R}{\rm s} \ll 1.01$).

```{r}
# If one was to run this as one model for all stops, the following code would work. However, this would assume
# that the correlation between VOT and f0 is constant across all stops. We therefore fit separate models to
# each stop.
#
# bf_vot =
#   bf(vot ~ -1 + poa : voicing + (-1 + poa : voicing | p1 | subj) + (-1 + poa : voicing | p2 | word)) +
#   lf(sigma ~ -1 + poa : voicing + (-1 + poa : voicing | q1 | subj) + (-1 + poa : voicing | q2 | word)) + gaussian()
#
# bf_f0 =
#   bf(f0_Mel ~ -1 + poa : voicing + (-1 + poa : voicing | p1 | subj) + (-1 + poa : voicing | p2 | word)) +
#   lf(sigma ~ -1 + poa : voicing + (-1 + poa : voicing | q1 | subj) + (-1 + poa : voicing | q2 | word)) + gaussian()

bf_vot =
  bf(vot ~ 1 + (1 | p1 | subj) + (1 | p2 | word)) +
  lf(sigma ~ 1 + (1 | q1 | subj) + (1 | q2 | word)) + gaussian()

bf_f0 =
  bf(f0_Mel ~ 1 + (1 | p1 | subj) + (1 | p2 | word)) +
  lf(sigma ~ 1 + (1 | q1 | subj) + (1 | q2 | word)) + gaussian()


# Helpful to get general structure of priors for this type of model:
#
# get_prior(
#   formula = bf_vot + bf_f0,
#   data = d.chodroff_wilson)

my.priors.bivariate = c(
  # No intercept priors since model is reparameterized to have no intercepts
#  set_prior("student_t(3, 0, 2.5)", class = "Intercept", resp = c("vot","f0Mel")),
  # weakly regularizing prior for fixed effect coefficiencts for sigma.
  set_prior("student_t(3, 0, 2.5)", class = "Intercept", resp = c("vot","f0Mel"), dpar = "sigma"),
  # weakly regularizing prior for fixed effect coefficiencts for mu.
#  set_prior("student_t(3, 0, 2.5)", class = "b", resp = c("vot","f0Mel")),
  # weakly regularizing prior for fixed effect coefficiencts for sigma.
#  set_prior("student_t(3, 0, 2.5)", class = "b", resp = c("vot","f0Mel"), dpar = "sigma"),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel")),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), dpar = "sigma"),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), group = c("subj", "word")),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), group = c("subj", "word"), dpar = "sigma")
)

# for (s in c("P", "B")) { # unique(d.chodroff_wilson$stop)) {
#   m <- brm(
#     formula = bf_vot + bf_f0,
#     data = d.chodroff_wilson %>%
#       filter(category == s),
#     prior = my.priors.bivariate,
#     chains = 4, cores = 4,
#     warmup = 3000,
#     iter = 4500,
#     control = list(adapt_delta = .995, max_treedepth = 20),
#     backend = "cmdstanr",
#     file = get_path(paste0("../models/production-syllable_initial_stop-VOT_f0Mel-bivariate_normal-simple_effects-", s)))
# }
```

<!-- (ref:chodroff-stop-VOT-mu-sigma) Estimated by-talker voice onset time (VOT) means $\hat\mu$ and standard deviations $\hat\sigma$ of word-initial stops in connected American English. The number of observations that went into the estimates ($n$) differ between talkers due to missing values [e.g., because of measurement errors or outliers; for details, see @chodroff-wilson2018]. Lines connect data from the same talker. Note that the y-axis is log-stepped. -->

<!-- (ref:chodroff-stop-f0ST-mu-sigma) Same as \@ref(fig:chodroff-stop-VOT-mu-sigma) but for the fundamental frequency (f0) at the onset of the vowel following the stop. -->

<!-- ```{r chodroff-means-sds, fig.width=10, fig.height = 3.5, fig.cap=c("(ref:chodroff-stop-VOT-mu-sigma)", "(ref:chodroff-stop-f0ST-mu-sigma)")} -->
<!-- # plot correlation between by-talker mean and sd for each cue -->
<!-- d.chodroff_wilson %>%  -->
<!--   group_by(Talker, gender, category, voicing, poa) %>% -->
<!--   summarise_at( -->
<!--     vars(VOT, f0_Mel, f0_semitones), -->
<!--     list("mean" = function(x) mean(x, na.rm = T), "sd" = function(x) sd(x, na.rm = T), "n" = length)) %>% -->
<!--   ggplot(aes(x = VOT_mean, y = VOT_sd, color = voicing, shape = gender, size = VOT_n)) + -->
<!--   geom_line(alpha = .1, size = .3, color = "gray", aes(group = Talker)) + -->
<!--   geom_point(alpha = .3) + -->
<!--   scale_x_continuous(expression("VOT" ~ hat(mu) ~ "(ms)")) + -->
<!--   scale_y_continuous(expression("VOT" ~ hat(sigma) ~ "(ms)"), trans = "log10") + -->
<!--   scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) + -->
<!--   scale_shape_discrete("Gender") + -->
<!--   scale_size_continuous("n", range = c(.1, 3)) + -->
<!--   facet_grid(. ~ poa)  -->

<!-- ggplot2::last_plot() +  -->
<!--   aes(x = f0_Mel_mean, y = f0_Mel_sd, size = f0_Mel_n) + -->
<!--   scale_x_continuous(expression("f0" ~ hat(mu) ~ "(Mel)")) + -->
<!--   scale_y_continuous(expression("f0" ~ hat(sigma) ~ "(Mel)"), trans = "log10") -->

<!-- # plot correlation between by-talker means of different cues -->
<!-- # last_plot() + aes(x = vot_mean, y = usef0_mean) -->
<!-- # last_plot() + aes(x = usef0_mean, y = cog_mean) -->
<!-- # last_plot() + aes(x = cog_mean, y = vot_mean) -->

<!-- # d.chodroff_wilson %>%  -->
<!-- #   group_by(Talker, stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(vot, usef0), -->
<!-- #     list("mean" = mean, "sd" = sd), -->
<!-- #     na.rm = T) %>% -->
<!-- #   group_by(stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(ends_with("mean"), ends_with("sd")), -->
<!-- #     list("mean" = mean, "median" = median, "sd" = sd), -->
<!-- #     na.rm = T) -->
<!-- #  -->
<!-- # d.chodroff_wilson.byTalker <- d.chodroff_wilson %>%  -->
<!-- #   group_by(Talker, stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(vot, usef0, cog), -->
<!-- #     list("mean" = mean, "sd" = sd), -->
<!-- #     na.rm = T) -->
<!-- ``` -->

# Introduction

  * ^[Additional lines of work have investigated exposure to synthesized, vocoded, or otherwise distorted speech [e.g., @adank2009; @davis2005; @fenn2003; @mattys2012] or have used distributional learning paradigms [e.g., @clare2019; @clayards2008; @idemaru-holt2011; @maye2008; @schertz2013; @theodore-monto2019; @wade2007]. In the general discussion, we return to distributional learning paradigms, which we believe to hold particular promise in addressing the issues we identify below.] <!--TO_DO:this footnote should perhaps be moved elsewhere now that we do not talk about the two paradigms here--> 
  
  * Even if one were to take for granted that the three mechanisms in Figure \@ref(fig:overview) *jointly* underlie the effects of recent exposure, it remains unclear which of those mechanisms any given results sheds light on. For example, can we conclude from experiments like @clarke-garrett2004 that listeners can learn new representations (or at least select some mixture of existing representations) within two minutes of exposure? Or are those results due to normalization, and if so, where should we draw the line between processing and learning? Also, do the results of perceptual recalibration and accent adaptation stem from the same mechanism(s), or do they reflect different mechanisms [@baeseberk2018; @samuel-kraljic2009; @zheng-samuel2020]? Our comprehensive framework will make it possible to empirically test whether two paradigms that differ substantially in their design, tasks, and ecological validity of the speech stimuli would in fact engage the same mechanism(s). 
  

# Accent adaptation

 * There are a few noteworthy differences between the accent adaptation and the perceptual recalibration paradigms. First, most work on accent adaptation uses naturally-produced speech, instead of creating artificial speech via acoustic manipulation [e.g., @bradlow-bent2008; @sidaras2009; @baeseberk2013; @smith2014; @mitterer-mcqueen2009]. The specific manner in which L2-accented speech categories differ from L1-accented counterparts is constrained by sound systems of both languages [@bent-baeseberk2021]. L2-accented speech therefore naturally deviates from L1-accented speech along multiple cue dimensions[@xie-jaeger2020; @vaughn2019], rather than along just a single cue dimension as is often assumed for a typical perceptual recalibration experiment. 

 * Second, and relatedly, L1 and L2 accents can vary not only in categories' central tendencies (means) and dispersions (variances and covariances) but also types of phonetic cues that are used to distinguish the categories as well as weights given to each of them [@kim2002; @schertz2013; @ingvalson2011; @yamada-tohkura1992; @liu-holt2015; @harmon2019]. For instance, in Korean-accented English, the primary and and the secondary cues to the word-initial /d/ vs. /t/ contrast are reversed: f0, which is the secondary cue to voicing in L1-accented English, outweighs VOT [e.g., @schertz2015]. Another case in point is the word-final stops in Mandarin-accented English, where burst duration substitutes preceding vowel duration as the primary cue to voicing distinction [e.g., @xie2017].
 
 
  * It is often assumed, for example, that response biases can only change the relative proportion of answers but cannot explain overall improvements in accuracy. Under this assumption, overall improvements in the perception of an accent would seem to rule out explanations in terms of response biases, at least if the experimenter carefully balanced how often each category occurs during test. This is, however, a common misconception (one that we shared, for what it is worth). As we demonstrate below, changes in response biases can explain findings that are often attributed to changes in representations. The same applies to pre-linguistic normalization: contrary to common intuitions, simple normalization mechanisms can explain seemingly complex changes in listeners' categorization following exposure to L2-accented or otherwise shifted speech (as, e.g., in perceptual recalibration paradigms).

# General discussion

  * ... in the broader context of other research. This includes research on accent adaptation and related topics that has pioneered analyses beyond changes in accuracy or processing speed, towards a more in-depth understanding of the link between the acoustic input and perception [e.g., @clare2019; @idemaru-holt2020; @kartushina2015; @kim2020; @schertz2015; @wade2007].
 
  * Additionally, specific linking hypotheses---like the ones we have introduced here---can be compared in terms of their *quantitative* fit against the data, while appropriately taking into account their functional flexibility [e.g., through model comparison measures like the Bayesian leave-one-out information criterion, @vehtari2017]. 
  
    *These separate lines of research (our own work included) seem to have drawn theoretical conclusions through convention, rather than through strong empirical evidence in favor of a particular mechanism. 

### Facilitating theory building 
Besides the two classes of experimental paradigms we detailed above, there are a wider range of results cited as evidence for or against a particular mechanism underlying accurate and robust speech perception. In particular, the assumption that recent exposure can shape linguistic representations, or at least their weighting or selection (X. Xie et al., 2018), is central to many accounts of sociophonetics and exemplar-based theories of speech perception more broadly (for review, see Hay et al., 2019; Kleinschmidt & Jaeger, 2015; Sjerps et al., 2019). Multiple non-linguistic factors have been shown to inform and impact the selection, such as the (inferred) physiology [@krauss2002], social identity of a talker [e.g., regional origin, @hay-drager2010; @niedzielski1999; sex, @johnson1999; @strand1999; age, @skoogwaller2015; @walker-hay2011; and individual identity, @nygaard1994; @remez2018], or a situational context [e.g., being in a car, @hay2017]. These effects are thought to draw on episodic memory traces from the listeners' past linguistic experiences and hence inexplicable based soley on lower-level auditory signal normalization. Conversely, a separate line of research has found that non-speech stimuli (e.g., pure tones) can systematically alter the perception of subsequently played speech stimuli [@chodroff-wilson2020; @holt2005; @holt2006; @huang-holt2011], which seems to be harder to explain through changes in, or a selection of, representations. Tested separately on distinct types of stimuli within distinct experimental procedures, these theoretical positions have coexisted for decades by now. 
 
The current results urge us, and hopefully other researchers in the field, to revisit this equilibrium. The empirical indeterminacy we identified for perceptual recalibration and accent adaptation likely applies to *any* form of exposure-test paradigm, regardless of whether the exposure stimuli being researcher curated (in the case of non-speech sound exposure) or natural (in the case of exposure to various talkers throughout lifetime). As shown in Case Studies 1 and 2, depending on the listeners' prior expectations and exact stimulus features in the input, normalization and changes in decision-biases can predict _a wider range of highly complex response patterns than previously recognized_. This means that none of the three mechanisms can be simply dismissed without explicit, empirical tests. In other words, any behavioral experiment that does not annotate/examine stimulus features and analyze link between those features and participants' responses cannot be claimed to support one of the hypotheses over the others.


### Do the mechanisms combine or trade-off? If so, when?
The second avenue in which we anticipate a breakthrough pertains to development of theories that delve into different assumptions about each mechanism. For instance, it has so far been assumed that distributional learning of category statistics [as shown by @clayards2008 and subsequent replications and extensions @bejjanki2011; @nixon2016; @theodore-monto2019] entails representational changes. Specifically, participants that were exposed to /b/ and /p/ categories with larger variance along VOT exhibited more shallow categorization slopes than participants who were exposed to /b/ and /p/ categories with smaller variance (but the same means as the other condition). These changes may not be easily explained by a cue-based normalization mechanism such as C-CuRE. This does not, however, rule out explanations in terms of *other* normalization. For example, normalization that standardizes cues relative to expectations, in addition to centering them---which we might call *S*-CuRE (for "standardizing")--- may be able to account for the variance shifts found by Clayards et al. (2008). Further, it is possible that listeners may learning distributional structures *post* low-level normalization. e.g., They might first normalize/standardize cues [as has been proposed for the perception of vowels  @johnson2020; @lobanov1971; @monahan-idsardi2010] and then learn and store residual idiosyncrasies [For a preliminary test of this idea, see @xie2021cognition]. Yet alternatively, the findings of Clayards et al. (2008) might also be accounted for by changes in response biases provided that the lapse rate is not zero: recall that changes in response biases can have non-additive effects when the lapse rate is non-zero (Section \@ref(sec:change-bias)). And non-zero lapse rates were indeed observed by Clayards et al. (2008, Figure 3B and footnote 2). Changes in response biases could thus potentially account for changes in the slope of categorization functions---the result observed in Clayards et al. (2008). 

A related thought-provoking possibility may be that distinct mechanisms can combine or trade off with one another. We have so far attempted to contrast the three mechanisms as contenders. ASP, however, will also help us move beyond this either-or comparisons and assess the relative involvement of the mechanisms as a function of stimulus features and the amount of input. This is important as the three mechanisms could be distinct in terms of cognitive and memory demands and hence differentially costly. For example, to arrive at a precise estimate of a given talker's cue mean (for normalization), listeners must receive at least some amount of exposure to the talker. The minimum amount of exposure required for deriving category means and variances (for learning category representations) is expected to be larger, especially for variance estimates. This makes the representational learning mechanism relatively costly and taxing in terms of memory demands. On the other hand, the decision-bias changes do not require any storage of acoustic-phonetic details, and hence are relatively (computationally) cheap. It is possible that L1 listeners who encounter an unfamiliar accent might initially resort to the decision-making changes to boost their recognition accuracy. As they accrue input to a given talker or accent, they may begin to rely more on the other two mechanisms. Relatedly, listeners might opt to expend more resources to learn distributional properties of categories when doing so would in fact be expected to improve recognition accuracies. They might be less likely to do so when L2 categories are, for instance, phonetically overlapping with one another (e.g., Mandarin-accented talkers tend to pronounce [`r linguisticsdown::cond_cmpl("θ")`] (as in *thick*) as [s] (as in *sick*), Zheng and Samuel (2020)). In such a scenario, the bottom-up signal does not distinguish between intended categories, which makes cue normalization or representational learning rather inefficient as a strategy. Instead, simply increasing the rate of [`r linguisticsdown::cond_cmpl("θ")`] independent of the stimulus could more efficiently improve recognition accuracies. Indeed, a recent MEG study by Blanco-Elorrieta et al., (2021) found strong engagement of a pre-frontal cortex in processing phoneme substitutions (e.g., [s] mispronouced as [`r linguisticsdown::cond_cmpl("ʃ")`]), implicating post-perceptual decision-making processes. Testing these fine-grained hypothesis would require careful parameterization of acoustic-phonetic features, categories/contrasts, and amounts of exposure within subjects. The current analytical framework and the new standards of experimental design and analyses laid out above will be indispensable for this endevour.