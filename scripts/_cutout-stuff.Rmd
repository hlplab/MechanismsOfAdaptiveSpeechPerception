---
title: "Most experiments on exposure effects in speech perception do not distinguish between underlying mechanisms"
subtitle: "A computational review"
date: "`r format(Sys.time(), '%B %d, %Y')`"
thanks: "We are grateful to Meghan Clayards and Eleanor Chodroff for sharing their data, doing so in an accessible format, and for helping prepare their data for this study. We thank the participants in the 2021 summer 'mega-lab' meetings for many insightful discussions that shaped the perspectives discussed here (all mistakes remain our own). This includes, in particular, Ann Bradlow, Melissa Baese-Berk, Eleanor Chodroff, Jennifer Cole, Shawn Cummings, Laura Dilley, James McQueen, Arty Samuel, Maryann Tan, and Rachel Theodore. We also owe thanks for early feedback on this project to Marc Allassonni√®re-Tang, Jing Liu, Gerda Melnik, and Anna Persson."
author:
  - name: Xin Xie^[Department of Language Science, UC Irvine]
  - name: T. Florian Jaeger^[Department of Brain & Cognitive Sciences, University of Rochester]
  - name: Chigusa Kurumada^[Department of Brain & Cognitive Sciences, University of Rochester]
keywords:
  - speech perception
  - accent adaptation
  - perceptual recalibration
  - computational modeling
header-includes: 
 - \usepackage{animate}
 - \usepackage{amsmath}
 - \usepackage{tikz}
 - \usetikzlibrary{bayesnet}
 - \usepackage{booktabs}
 - \usepackage{siunitx}
 - \usepackage{soul}
 - \usepackage{tabto}
 - \usepackage{xcolor}
 - \usepackage{placeins}
 - \setstcolor{red}
 - \usepackage{sectsty}
 - \sectionfont{\color{red}}
 - \subsectionfont{\color{red}}
 - \subsubsectionfont{\color{red}}
 - \usepackage{setspace}\doublespacing
output: 
  papaja::apa6_pdf:
    citation_package: biblatex
    extra_dependencies: "subfig" 
    latex_engine: xelatex
    includes:
      in_header: header.tex
    fig_caption: yes
    fig_width: 2.5
    number_sections: yes
    toc: no
    toc_depth: 3
  word_document:
    toc: yes
always_allow_html: true
fontsize: 11pt
bibliography: [library.bib]
link-citations: yes
csl: apa-6th-edition.csl
---

```{r}
# For brms
priors.weakly_regularizing <- c(
  # weakly regularizing prior for fixed effect coefficients.
  prior(student_t(3, 0, 2.), class = b), 
  # weakly regularizing prior for random effect SDs.
  prior(cauchy(0, 2), class = sd),
  # weakly regularizing prior for random effect correlations.
  prior(lkj(1), class = cor))
```

### Estimating the typical (population-level) category-specific distribution of VOT {#sec:estimating-expected-category-statistics}
We use Bayesian hierarchical (mixed-effects) *distributional* linear regression to infer the population-level mean and standard deviation of each stop category in the database. Together, these population-level estimates are taken to describe the `typical' likelihood function of each category. This approach is an extension of linear mixed-effects models (LMMs). In addition to predicting the mean of an outcome that is assumed to be normally distributed, we predict both the mean and the standard deviation of that outcome. The use of hierarchical distributional regression has a number of advantages of standard approaches to estimating typical category likelihoods (such as simply aggregating the data by-talker to calculate by-talker means and standard deviations):

 * It recognizes the hierarchical structure of the data (see also REF-nalborczyk2019), estimating population-level effects while taking into account uncertainty about individual differences between talkers and between lexical contexts:
    * it efficiently accounts for variability in category likelihoods across talkers.
    * it efficiently accounts for variability in category likelihoods across lexical (and phonological) contexts.
    * it is more suitable of imbalanced data, with different amounts of information per talker and/or lexical contexts. Estimates of by-talker or by-lexical context differences are subject to `shrinkage' that is stronger for talkers/lexical contexts with fewer observations. This ameliorates the risk of over-fitting.
  * It estimates the mean and standard deviation of each category *jointly*. This also allows us to take into account correlations between these two parameters.
  * While we make the simplifying assumption of Gaussian categories, the approach taken here can be applied to outcomes that are assumed to follow non-Gaussian distributions (e.g., lognormal or skew normal distributions).
  * While we focus on just one cue here (VOT), the approach taken here extends to multiple phonetic cues (multivariate distributional mixed-effects regression), allowing us to model within- and cross-category correlations between different cues.

Specifically, we predict the mean VOT of each of the six stop categories (5 population-level DFs), while allowing random (normal) variation in these means by talkers (intercepts, slopes and their correlations, $\frac{1}{2}(6^2-6) = 15$ group-level DFs) and lexical contexts (intercepts only since stop vary *between* lexical context, 1 group-level DF). Simultaneously, we predict the (log-transformed) standard deviation of each of the six stop categories, using the exact same formula. Finally, the model predicts the group-level correlations between means and standard deviations across both talkers () and lexical contexts (1 group-level DF).

We fit this model using the function \texttt{brm} from library \texttt{brms} [@burkner2019] in \texttt{R} [version 3.5.2 @R]. \texttt{brms} provides an interface to \texttt{Stan} for Bayesian generalized mixed-effects models, including distributional regression [@burkner2017]. \texttt{Stan} [@carpenter2017] allows the efficient implementation of Bayesian data analysis through No-U-Turn Hamiltonian Monte Carlo sampling. We follow common practice and use weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we use Student priors centered around zero with a scale of 2.5 units [following @gelman2008] and 3 degrees of freedom. For random effect standard deviations, we use a Cauchy prior with location 0 and scale 2, and for random effect correlations, we use an uninformative LKJ-Correlation prior with its only parameter set to 1 [@lewandowski2009], describing a uniform prior over correlation matrices. The model was fit using four chains with 1,000 post-warmup samples per chain, for a total of 4,000 posterior samples. Each chain used 1,000 warmup samples to calibrate Stan's No U-Turn Sampler. All analyses reported here converged (e.g., all $1 \leq \hat{R}{\rm s} \ll 1.01$).

```{r}
# If one was to run this as one model for all stops, the following code would work. However, this would assume
# that the correlation between VOT and f0 is constant across all stops. We therefore fit separate models to
# each stop.
#
# bf_vot =
#   bf(vot ~ -1 + poa : voicing + (-1 + poa : voicing | p1 | subj) + (-1 + poa : voicing | p2 | word)) +
#   lf(sigma ~ -1 + poa : voicing + (-1 + poa : voicing | q1 | subj) + (-1 + poa : voicing | q2 | word)) + gaussian()
#
# bf_f0 =
#   bf(f0_Mel ~ -1 + poa : voicing + (-1 + poa : voicing | p1 | subj) + (-1 + poa : voicing | p2 | word)) +
#   lf(sigma ~ -1 + poa : voicing + (-1 + poa : voicing | q1 | subj) + (-1 + poa : voicing | q2 | word)) + gaussian()

bf_vot =
  bf(vot ~ 1 + (1 | p1 | subj) + (1 | p2 | word)) +
  lf(sigma ~ 1 + (1 | q1 | subj) + (1 | q2 | word)) + gaussian()

bf_f0 =
  bf(f0_Mel ~ 1 + (1 | p1 | subj) + (1 | p2 | word)) +
  lf(sigma ~ 1 + (1 | q1 | subj) + (1 | q2 | word)) + gaussian()


# Helpful to get general structure of priors for this type of model:
#
# get_prior(
#   formula = bf_vot + bf_f0,
#   data = d.chodroff_wilson)

my.priors.bivariate = c(
  # No intercept priors since model is reparameterized to have no intercepts
#  set_prior("student_t(3, 0, 2.5)", class = "Intercept", resp = c("vot","f0Mel")),
  # weakly regularizing prior for fixed effect coefficiencts for sigma.
  set_prior("student_t(3, 0, 2.5)", class = "Intercept", resp = c("vot","f0Mel"), dpar = "sigma"),
  # weakly regularizing prior for fixed effect coefficiencts for mu.
#  set_prior("student_t(3, 0, 2.5)", class = "b", resp = c("vot","f0Mel")),
  # weakly regularizing prior for fixed effect coefficiencts for sigma.
#  set_prior("student_t(3, 0, 2.5)", class = "b", resp = c("vot","f0Mel"), dpar = "sigma"),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel")),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), dpar = "sigma"),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), group = c("subj", "word")),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), group = c("subj", "word"), dpar = "sigma")
)

# for (s in c("P", "B")) { # unique(d.chodroff_wilson$stop)) {
#   m <- brm(
#     formula = bf_vot + bf_f0,
#     data = d.chodroff_wilson %>%
#       filter(category == s),
#     prior = my.priors.bivariate,
#     chains = 4, cores = 4,
#     warmup = 3000,
#     iter = 4500,
#     control = list(adapt_delta = .995, max_treedepth = 20),
#     backend = "cmdstanr",
#     file = get_path(paste0("../models/production-syllable_initial_stop-VOT_f0Mel-bivariate_normal-simple_effects-", s)))
# }
```

<!-- (ref:chodroff-stop-VOT-mu-sigma) Estimated by-talker voice onset time (VOT) means $\hat\mu$ and standard deviations $\hat\sigma$ of word-initial stops in connected American English. The number of observations that went into the estimates ($n$) differ between talkers due to missing values [e.g., because of measurement errors or outliers; for details, see @chodroff-wilson2018]. Lines connect data from the same talker. Note that the y-axis is log-stepped. -->

<!-- (ref:chodroff-stop-f0ST-mu-sigma) Same as \@ref(fig:chodroff-stop-VOT-mu-sigma) but for the fundamental frequency (f0) at the onset of the vowel following the stop. -->

<!-- ```{r chodroff-means-sds, fig.width=10, fig.height = 3.5, fig.cap=c("(ref:chodroff-stop-VOT-mu-sigma)", "(ref:chodroff-stop-f0ST-mu-sigma)")} -->
<!-- # plot correlation between by-talker mean and sd for each cue -->
<!-- d.chodroff_wilson %>%  -->
<!--   group_by(Talker, gender, category, voicing, poa) %>% -->
<!--   summarise_at( -->
<!--     vars(VOT, f0_Mel, f0_semitones), -->
<!--     list("mean" = function(x) mean(x, na.rm = T), "sd" = function(x) sd(x, na.rm = T), "n" = length)) %>% -->
<!--   ggplot(aes(x = VOT_mean, y = VOT_sd, color = voicing, shape = gender, size = VOT_n)) + -->
<!--   geom_line(alpha = .1, size = .3, color = "gray", aes(group = Talker)) + -->
<!--   geom_point(alpha = .3) + -->
<!--   scale_x_continuous(expression("VOT" ~ hat(mu) ~ "(ms)")) + -->
<!--   scale_y_continuous(expression("VOT" ~ hat(sigma) ~ "(ms)"), trans = "log10") + -->
<!--   scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) + -->
<!--   scale_shape_discrete("Gender") + -->
<!--   scale_size_continuous("n", range = c(.1, 3)) + -->
<!--   facet_grid(. ~ poa)  -->

<!-- ggplot2::last_plot() +  -->
<!--   aes(x = f0_Mel_mean, y = f0_Mel_sd, size = f0_Mel_n) + -->
<!--   scale_x_continuous(expression("f0" ~ hat(mu) ~ "(Mel)")) + -->
<!--   scale_y_continuous(expression("f0" ~ hat(sigma) ~ "(Mel)"), trans = "log10") -->

<!-- # plot correlation between by-talker means of different cues -->
<!-- # last_plot() + aes(x = vot_mean, y = usef0_mean) -->
<!-- # last_plot() + aes(x = usef0_mean, y = cog_mean) -->
<!-- # last_plot() + aes(x = cog_mean, y = vot_mean) -->

<!-- # d.chodroff_wilson %>%  -->
<!-- #   group_by(Talker, stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(vot, usef0), -->
<!-- #     list("mean" = mean, "sd" = sd), -->
<!-- #     na.rm = T) %>% -->
<!-- #   group_by(stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(ends_with("mean"), ends_with("sd")), -->
<!-- #     list("mean" = mean, "median" = median, "sd" = sd), -->
<!-- #     na.rm = T) -->
<!-- #  -->
<!-- # d.chodroff_wilson.byTalker <- d.chodroff_wilson %>%  -->
<!-- #   group_by(Talker, stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(vot, usef0, cog), -->
<!-- #     list("mean" = mean, "sd" = sd), -->
<!-- #     na.rm = T) -->
<!-- ``` -->

# Introduction

  * ^[Additional lines of work have investigated exposure to synthesized, vocoded, or otherwise distorted speech [e.g., @adank2009; @davis2005; @fenn2003; @mattys2012] or have used distributional learning paradigms [e.g., @clare2019; @clayards2008; @idemaru-holt2011; @maye2008; @schertz2013; @theodore-monto2019; @wade2007]. In the general discussion, we return to distributional learning paradigms, which we believe to hold particular promise in addressing the issues we identify below.] <!--TO_DO:this footnote should perhaps be moved elsewhere now that we do not talk about the two paradigms here--> 
  
  * Even if one were to take for granted that the three mechanisms in Figure \@ref(fig:overview) *jointly* underlie the effects of recent exposure, it remains unclear which of those mechanisms any given results sheds light on. For example, can we conclude from experiments like @clarke-garrett2004 that listeners can learn new representations (or at least select some mixture of existing representations) within two minutes of exposure? Or are those results due to normalization, and if so, where should we draw the line between processing and learning? Also, do the results of perceptual recalibration and accent adaptation stem from the same mechanism(s), or do they reflect different mechanisms [@baeseberk2018; @samuel-kraljic2009; @zheng-samuel2020]? Our comprehensive framework will make it possible to empirically test whether two paradigms that differ substantially in their design, tasks, and ecological validity of the speech stimuli would in fact engage the same mechanism(s). 
  
  * Changes in speech perception as a function of recent exposure have now been documented across a broad range of experimental paradigms and tasks [e.g., @bradlow-bent2008; @clayards2008; @eisner2013; @idemaru-holt2011; @norris2003; @vroomen2007]. different modes of speech [e.g., "clear" and "conversational" speech, @zhang-samuel2014], isolated and connected speech, [e.g., @eisner-mcqueen2005; @reinisch-holt2013], and languages [e.g., @chladkova2017; @eisner2013; @hanulikova-weber2012; @sebastian-galles2000; @schertz2015]. Changes can occur even under cognitive load, arguing for a relatively high degree of automaticity of some of the underlying mechanisms [@zhang-samuel2014; though both top-down attention-direction and interference can modulate adaptation, @mcauliffe-babel2016; @samuel2016].
  
  * ^[To the extent that behavioral research has begun to contrast competing hypotheses, these efforts have been limited to (A) normalization and (B) representational changes while ignoring the possibility of (C) post-perceptual decision-making. In contrast to behavioral work, neuroimaging research has focused more on the contrast between (C) vs. (A) and (B), grouping the latter together as functionally distinct from higher-level decision-related processes [but see @REF-XIN-DOES-SOMETHING-COME-TO-MIND].]

# Accent adaptation

 * There are a few noteworthy differences between the accent adaptation and the perceptual recalibration paradigms. First, most work on accent adaptation uses naturally-produced speech, instead of creating artificial speech via acoustic manipulation [e.g., @bradlow-bent2008; @sidaras2009; @baeseberk2013; @smith2014; @mitterer-mcqueen2009]. The specific manner in which L2-accented speech categories differ from L1-accented counterparts is constrained by sound systems of both languages [@bent-baeseberk2021]. L2-accented speech therefore naturally deviates from L1-accented speech along multiple cue dimensions[@xie-jaeger2020; @vaughn2019], rather than along just a single cue dimension as is often assumed for a typical perceptual recalibration experiment. 

 * Second, and relatedly, L1 and L2 accents can vary not only in categories' central tendencies (means) and dispersions (variances and covariances) but also types of phonetic cues that are used to distinguish the categories as well as weights given to each of them [@kim2002; @schertz2013; @ingvalson2011; @yamada-tohkura1992; @liu-holt2015; @harmon2019]. For instance, in Korean-accented English, the primary and and the secondary cues to the word-initial /d/ vs. /t/ contrast are reversed: f0, which is the secondary cue to voicing in L1-accented English, outweighs VOT [e.g., @schertz2015]. Another case in point is the word-final stops in Mandarin-accented English, where burst duration substitutes preceding vowel duration as the primary cue to voicing distinction [e.g., @xie2017].
 
 
  * It is often assumed, for example, that response biases can only change the relative proportion of answers but cannot explain overall improvements in accuracy. Under this assumption, overall improvements in the perception of an accent would seem to rule out explanations in terms of response biases, at least if the experimenter carefully balanced how often each category occurs during test. This is, however, a common misconception (one that we shared, for what it is worth). As we demonstrate below, changes in response biases can explain findings that are often attributed to changes in representations. The same applies to pre-linguistic normalization: contrary to common intuitions, simple normalization mechanisms can explain seemingly complex changes in listeners' categorization following exposure to L2-accented or otherwise shifted speech (as, e.g., in perceptual recalibration paradigms).
  
  <!--^[Likewise, an *absence of improvements* should not be taken as evidence to reject the hypothesis of representational changes. Such a conclusion is only valid if it is also shown that *successful learning of the exposure statistics* would be *predicted* to facilitate comprehension during test. Whether this prediction holds for a given experiment depends on the exposure and test stimuli---a fact that is sometimes not considered in the interpretation of experiments [cf. @zheng-samuel2020].]--> 
  
  <!--And these differences are often asymmetrically affecting the categories participating in a phonetic contrast. For example, L1-German speakers of English often have non-native realizations of /`r linguisticsdown::cond_cmpl("Œ∏")`/ but have no trouble pronouncing /s/, which unlike /`r cond_cmpl("Œ∏")`/ has a counterpart in their L1. These asymmetries in the production of L2 speech are reflected in L1 listeners' perception of L2-accented speech. For instance, L1-English listeners usually correctly identify German-accented word-final /t/s (e.g., *seat*), but mishear word-final /d/s as devoiced [for other asymmetries, see @schertz2015; @xie2017; @zheng-samuel2020].-->
  
  <!--In light of this state of the art, it would be ill-advised to interpret the  quantitative differences between the results of the two change models as meaningful. For these reasons, we focus on each model's ability to predict the qualitative results found in previous work. -->

# General discussion

  * ... in the broader context of other research. This includes research on accent adaptation and related topics that has pioneered analyses beyond changes in accuracy or processing speed, towards a more in-depth understanding of the link between the acoustic input and perception [e.g., @clare2019; @idemaru-holt2020; @kartushina2015; @kim2020; @schertz2015; @wade2007].
 
  * Additionally, specific linking hypotheses---like the ones we have introduced here---can be compared in terms of their *quantitative* fit against the data, while appropriately taking into account their functional flexibility [e.g., through model comparison measures like the Bayesian leave-one-out information criterion, @vehtari2017]. 
  
    *These separate lines of research (our own work included) seem to have drawn theoretical conclusions through convention, rather than through strong empirical evidence in favor of a particular mechanism. 

### Facilitating theory building 
Besides the two classes of experimental paradigms we detailed above, there are a wider range of results cited as evidence for or against a particular mechanism underlying accurate and robust speech perception. In particular, the assumption that recent exposure can shape linguistic representations, or at least their weighting or selection (X. Xie et al., 2018), is central to many accounts of sociophonetics and exemplar-based theories of speech perception more broadly (for review, see Hay et al., 2019; Kleinschmidt & Jaeger, 2015; Sjerps et al., 2019). Multiple non-linguistic factors have been shown to inform and impact the selection, such as the (inferred) physiology [@krauss2002], social identity of a talker [e.g., regional origin, @hay-drager2010; @niedzielski1999; sex, @johnson1999; @strand1999; age, @skoogwaller2015; @walker-hay2011; and individual identity, @nygaard1994; @remez2018], or a situational context [e.g., being in a car, @hay2017]. These effects are thought to draw on episodic memory traces from the listeners' past linguistic experiences and hence inexplicable based soley on lower-level auditory signal normalization. Conversely, a separate line of research has found that non-speech stimuli (e.g., pure tones) can systematically alter the perception of subsequently played speech stimuli [@chodroff-wilson2020; @holt2005; @holt2006; @huang-holt2011], which seems to be harder to explain through changes in, or a selection of, representations. Tested separately on distinct types of stimuli within distinct experimental procedures, these theoretical positions have coexisted for decades by now. 
 
The current results urge us, and hopefully other researchers in the field, to revisit this equilibrium. The empirical indeterminacy we identified for perceptual recalibration and accent adaptation likely applies to *any* form of exposure-test paradigm, regardless of whether the exposure stimuli being researcher curated (in the case of non-speech sound exposure) or natural (in the case of exposure to various talkers throughout lifetime). As shown in Case Studies 1 and 2, depending on the listeners' prior expectations and exact stimulus features in the input, normalization and changes in decision-biases can predict _a wider range of highly complex response patterns than previously recognized_. This means that none of the three mechanisms can be simply dismissed without explicit, empirical tests. In other words, any behavioral experiment that does not annotate/examine stimulus features and analyze link between those features and participants' responses cannot be claimed to support one of the hypotheses over the others.


### Do the mechanisms combine or trade-off? If so, when?
The second avenue in which we anticipate a breakthrough pertains to development of theories that delve into different assumptions about each mechanism. For instance, it has so far been assumed that distributional learning of category statistics [as shown by @clayards2008 and subsequent replications and extensions @bejjanki2011; @nixon2016; @theodore-monto2019] entails representational changes. Specifically, participants that were exposed to /b/ and /p/ categories with larger variance along VOT exhibited more shallow categorization slopes than participants who were exposed to /b/ and /p/ categories with smaller variance (but the same means as the other condition). These changes may not be easily explained by a cue-based normalization mechanism such as C-CuRE. This does not, however, rule out explanations in terms of *other* normalization. For example, normalization that standardizes cues relative to expectations, in addition to centering them---which we might call *S*-CuRE (for "standardizing")--- may be able to account for the variance shifts found by Clayards et al. (2008). Further, it is possible that listeners may learning distributional structures *post* low-level normalization. e.g., They might first normalize/standardize cues [as has been proposed for the perception of vowels  @johnson2020; @lobanov1971; @monahan-idsardi2010] and then learn and store residual idiosyncrasies [For a preliminary test of this idea, see @xie2021cognition]. Yet alternatively, the findings of Clayards et al. (2008) might also be accounted for by changes in response biases provided that the lapse rate is not zero: recall that changes in response biases can have non-additive effects when the lapse rate is non-zero (Section \@ref(sec:change-bias)). And non-zero lapse rates were indeed observed by Clayards et al. (2008, Figure 3B and footnote 2). Changes in response biases could thus potentially account for changes in the slope of categorization functions---the result observed in Clayards et al. (2008). 

A related thought-provoking possibility may be that distinct mechanisms can combine or trade off with one another. We have so far attempted to contrast the three mechanisms as contenders. ASP, however, will also help us move beyond this either-or comparisons and assess the relative involvement of the mechanisms as a function of stimulus features and the amount of input. This is important as the three mechanisms could be distinct in terms of cognitive and memory demands and hence differentially costly. For example, to arrive at a precise estimate of a given talker's cue mean (for normalization), listeners must receive at least some amount of exposure to the talker. The minimum amount of exposure required for deriving category means and variances (for learning category representations) is expected to be larger, especially for variance estimates. This makes the representational learning mechanism relatively costly and taxing in terms of memory demands. On the other hand, the decision-bias changes do not require any storage of acoustic-phonetic details, and hence are relatively (computationally) cheap. It is possible that L1 listeners who encounter an unfamiliar accent might initially resort to the decision-making changes to boost their recognition accuracy. As they accrue input to a given talker or accent, they may begin to rely more on the other two mechanisms. Relatedly, listeners might opt to expend more resources to learn distributional properties of categories when doing so would in fact be expected to improve recognition accuracies. They might be less likely to do so when L2 categories are, for instance, phonetically overlapping with one another (e.g., Mandarin-accented talkers tend to pronounce [`r linguisticsdown::cond_cmpl("Œ∏")`] (as in *thick*) as [s] (as in *sick*), Zheng and Samuel (2020)). In such a scenario, the bottom-up signal does not distinguish between intended categories, which makes cue normalization or representational learning rather inefficient as a strategy. Instead, simply increasing the rate of [`r linguisticsdown::cond_cmpl("Œ∏")`] independent of the stimulus could more efficiently improve recognition accuracies. Indeed, a recent MEG study by Blanco-Elorrieta et al., (2021) found strong engagement of a pre-frontal cortex in processing phoneme substitutions (e.g., [s] mispronouced as [`r linguisticsdown::cond_cmpl(" É")`]), implicating post-perceptual decision-making processes. Testing these fine-grained hypothesis would require careful parameterization of acoustic-phonetic features, categories/contrasts, and amounts of exposure within subjects. The current analytical framework and the new standards of experimental design and analyses laid out above will be indispensable for this endevour.

### Neurobiological bases of adaptation
The primary goal of the current investigation has been to make strong inference through behavioral testing. Once achieved, it will facilitate more targeted neuro-imaging research. As discussed in the introduction, shifted categorization boundaries in response to the same physical input have been associated with multiple regions, such as early auditory regions [anterior PT, @kilianhutten2011], regions implicated in phoneme classification [posterior STG/STS, @bonte2017; @myers-mesite2014; @ullas2020], as well as regions for talker identity processing [right temporal regions, @luthra2020a]. The left parietal lobe and the insula, which are implicated in perceptual decision-making [e.g., @dacremont2013; @keuken2014], have also been shown to exhibit distinct activation for different exposure conditions [@bonte2017; @myers-mesite2014; @ullas2020]. As in behavioral experiments, existing results leave open which neural networks are responsible for the behavioral changes observed after recent exposure. Even a significant activation of a given brain region can be associated with functionally-distinct sources. For instance, the activation of left parietal lobe might reflect its general role in perceptual decision making [e.g., @dacremont2013; @keuken2014], or it could be due to a more specific role in phonological processing [e.g., processing abstract category information, @guediche2014]. One limitation of these works is that the conclusion is based primarily on a binary distinction in a brain region‚Äôs activation pattern (e.g., activity between categorization trials that a /d/ response is made and those in which a /t/ response is made for the same stimulus). More recently, neuroimaging studies have started to approach questions about the underlying neural mechanisms through multivariate analyses. These analyses are crucial for identifying the encoding of perceptual experiences across a distinct array of experimental conditions. Multivariate analyses can also be more sensitive in detecting fine-grained patterns *within* regions responsible for multiple cognitive demands [e.g., @bonte2017; @luthra2020a]. The methodological recommendations we made above may help generate behavioral responses that enable multivariate analyses required for further dissociation among regions functionally responsible for exposure-driven changes.

Finally, Another promising avenue is to pair temporally-sensitive techniques with imaging methods with good spatial resolution. For instance, studies using a combination of EEG and MEG have identified key regions that regulate perceptual learning of degraded speech by responding to manipulations of either low-level signal clarity or high-level prior knowledge of the speech content [@sohoglu-davis2016; @sohoglu-davis2020]. 



<!--there are distinct lines of work that normaliz as episodic  taken to suggest that listeners have implicit representations that encode expectations about talkers and types of talkers. These findings do, however, leave open whether these expectations relate to pre-linguistic normalization or to linguistic categories [see also discussion of "relativization" in @apfelbaum-mcmurray2015, p. 936-938]. They also leave open to what extent recent (e.g., within-experiment) experience affects speech perception through the same mechanisms that underlie the effects of inferred physiology or social identity. (This includes the question of whether and how recent experiences can lead to the learning of new linguistic representations. (e.g., Can listeners *learn* characteristics of a talker within 2 mins of exposure?) The approach and the recommendations provided above will make it possible to explicitly test these subtly distinct possibilities, which have not thus far possible.--> 



  * And while distinct normalization procedures have been compared to each other [e.g., @adank2004; @hoffmanbion-escudero2007; @kiefte-nearey2019] or against the absence of normalization [@mcmurray-jongman2011], they are rarely compared to the competing hypothesis that changes in representations underlie the effects of recent exposure [for notable exceptions, see @apfelbaum-mcmurray2015; @lehet-holt2020; @xie2021cognition]. ^[It is, however, worth pointing out that the exact nature of normalization remains unclear. This includes questions about the specific transformations that are applied [@REFS] but also questions about whether normalization is best viewed as an autonomous, encapsulated process, or as part of a larger hierarchical inference process that includes inferences at both levels traditionally considered linguistic and levels traditionally considered pre-linguistic. Research on automatic speech recognition, for example, has found that deep neural network models can to some extent remove the need for specialized signal transformations [@deng2016]. Under this view, normalization is itself the result of a hierarchical predictive process that seeks to efficiently predict the incoming signal [for related discussion, see @clark2013; @kell2018; @kleinschmidt-jaeger2015; @kuperberg-jaeger2016].] 


  * <!--leave it here for now, but potentially can be integrated into GD-->An explicit mechanistic account delineating how adaptive changes also strengthens our ability to probe its neural substrates. To date, multiple neurobiological systems have been proposed to support these rapid changes. They range from the primary auditory cortex  [e.g., Heschl‚Äôs gyrus] to those that respond to acoustic-phonetic features in speech [e.g., superior temporal gyrus, @Yi2019]. Even wider heterogeneous networks have been implicated in studies assessing brain responses to novel speech input, including those responsible for recognizing a voice/talker [@luthra2020b], attending selectively to a given talker's speech [@wong2004], and correcting prediction errors when there is a discrepancy between a predicted vs. an actual input [e.g., @guediche2015evidence], to name a few. Identifying the roles these brain regions play during adaptive speech perception is not straightforward, as these regions often play multiple roles in cognitive processing. Therefore, the exact interpretation of the findings still depends on the researchers‚Äô hypothesis of the underlying mechanism. For instance, changes in inferior frontal gyri activation in response to accented speech has been interpreted as reflecting greater computational demand [@yi2014neural] or decision related phonetic categorization of ambiguous stimuli [@myers-mesite2014]. A proper theoretical framework that specifies how the alternative mechanisms can account for adaptive changes in speech perception, either in isolation or jointly, will facilitate the design and interpretations of neural investigations.
  
  * This doesn actually seem to be what we do:  Results of our simulations illuminate when---to what stimuli and under what circumstances---the three mechanisms would provide diverging predictions about adaptive changes of perception. We also learn about how much data will be needed to draw reliable statistical inferences from behavioral data. 
  
  * Perhaps better for discussion: Frameworks like ASP that encompasses (A)-(C) thus hold the potential to meaningfully link across multiple lines of behavioral and neuroimaging research [for a related discussion, see @guediche2014, p. 8]. 
  
  <!-- By developing a computational framework that at least acknowledges, and integrates these aspects of speech perception, we take on (small) step towards addressing the challenge identified by @guediche2014: "there is no formal speech perception model that relates activity in the cortical regions identified via neuroimaging to the computational demands of adaptive plasticity in speech perception. Conversely, the classic computational models of speech perception that have attempted to differentiate how the system may meet the computational demands of adaptive plasticity have not made specific predictions of the underlying neural mechanisms".  -->