---
title: "Most experiments on exposure effects in speech perception do not distinguish between underlying mechanisms"
subtitle: "A computational review"
date: "`r format(Sys.time(), '%B %d, %Y')`"
thanks: "We are grateful to Meghan Clayards and Eleanor Chodroff for sharing their data, doing so in an accessible format, and for helping prepare their data for this study. We thank the participants in the 2021 summer 'mega-lab' meetings for many insightful discussions that shaped the perspectives discussed here (all mistakes remain our own). This includes, in particular, Ann Bradlow, Melissa Baese-Berk, Eleanor Chodroff, Jennifer Cole, Shawn Cummings, Laura Dilley, James McQueen, Arty Samuel, Maryann Tan, and Rachel Theodore. We also owe thanks for early feedback on this project to Marc Allassonnière-Tang, Jing Liu, Gerda Melnik, and Anna Persson."
author:
  - name: Xin Xie^[Department of Language Science, UC Irvine]
  - name: T. Florian Jaeger^[Department of Brain & Cognitive Sciences, University of Rochester]
  - name: Chigusa Kurumada^[Department of Brain & Cognitive Sciences, University of Rochester]
keywords:
  - speech perception
  - accent adaptation
  - perceptual recalibration
  - computational modeling
header-includes: 
 - \usepackage{animate}
 - \usepackage{amsmath}
 - \usepackage{tikz}
 - \usetikzlibrary{bayesnet}
 - \usepackage{booktabs}
 - \usepackage{siunitx}
 - \usepackage{soul}
 - \usepackage{tabto}
 - \usepackage{xcolor}
 - \usepackage{placeins}
 - \setstcolor{red}
 - \usepackage{sectsty}
 - \sectionfont{\color{red}}
 - \subsectionfont{\color{red}}
 - \subsubsectionfont{\color{red}}
 - \usepackage{setspace}\doublespacing
output: 
  papaja::apa6_pdf:
    citation_package: biblatex
    extra_dependencies: "subfig" 
    latex_engine: xelatex
    includes:
      in_header: header.tex
    fig_caption: yes
    fig_width: 2.5
    number_sections: yes
    toc: no
    toc_depth: 3
  word_document:
    toc: yes
always_allow_html: true
fontsize: 11pt
bibliography: [library.bib]
link-citations: yes
csl: apa-6th-edition.csl
---

```{r}
# For brms
priors.weakly_regularizing <- c(
  # weakly regularizing prior for fixed effect coefficients.
  prior(student_t(3, 0, 2.), class = b), 
  # weakly regularizing prior for random effect SDs.
  prior(cauchy(0, 2), class = sd),
  # weakly regularizing prior for random effect correlations.
  prior(lkj(1), class = cor))
```

### Estimating the typical (population-level) category-specific distribution of VOT {#sec:estimating-expected-category-statistics}
We use Bayesian hierarchical (mixed-effects) *distributional* linear regression to infer the population-level mean and standard deviation of each stop category in the database. Together, these population-level estimates are taken to describe the `typical' likelihood function of each category. This approach is an extension of linear mixed-effects models (LMMs). In addition to predicting the mean of an outcome that is assumed to be normally distributed, we predict both the mean and the standard deviation of that outcome. The use of hierarchical distributional regression has a number of advantages of standard approaches to estimating typical category likelihoods (such as simply aggregating the data by-talker to calculate by-talker means and standard deviations):

 * It recognizes the hierarchical structure of the data (see also REF-nalborczyk2019), estimating population-level effects while taking into account uncertainty about individual differences between talkers and between lexical contexts:
    * it efficiently accounts for variability in category likelihoods across talkers.
    * it efficiently accounts for variability in category likelihoods across lexical (and phonological) contexts.
    * it is more suitable of imbalanced data, with different amounts of information per talker and/or lexical contexts. Estimates of by-talker or by-lexical context differences are subject to `shrinkage' that is stronger for talkers/lexical contexts with fewer observations. This ameliorates the risk of over-fitting.
  * It estimates the mean and standard deviation of each category *jointly*. This also allows us to take into account correlations between these two parameters.
  * While we make the simplifying assumption of Gaussian categories, the approach taken here can be applied to outcomes that are assumed to follow non-Gaussian distributions (e.g., lognormal or skew normal distributions).
  * While we focus on just one cue here (VOT), the approach taken here extends to multiple phonetic cues (multivariate distributional mixed-effects regression), allowing us to model within- and cross-category correlations between different cues.

Specifically, we predict the mean VOT of each of the six stop categories (5 population-level DFs), while allowing random (normal) variation in these means by talkers (intercepts, slopes and their correlations, $\frac{1}{2}(6^2-6) = 15$ group-level DFs) and lexical contexts (intercepts only since stop vary *between* lexical context, 1 group-level DF). Simultaneously, we predict the (log-transformed) standard deviation of each of the six stop categories, using the exact same formula. Finally, the model predicts the group-level correlations between means and standard deviations across both talkers () and lexical contexts (1 group-level DF).

We fit this model using the function \texttt{brm} from library \texttt{brms} [@burkner2019] in \texttt{R} [version 3.5.2 @R]. \texttt{brms} provides an interface to \texttt{Stan} for Bayesian generalized mixed-effects models, including distributional regression [@burkner2017]. \texttt{Stan} [@carpenter2017] allows the efficient implementation of Bayesian data analysis through No-U-Turn Hamiltonian Monte Carlo sampling. We follow common practice and use weakly regularizing priors to facilitate model convergence. For fixed effect parameters, we use Student priors centered around zero with a scale of 2.5 units [following @gelman2008] and 3 degrees of freedom. For random effect standard deviations, we use a Cauchy prior with location 0 and scale 2, and for random effect correlations, we use an uninformative LKJ-Correlation prior with its only parameter set to 1 [@lewandowski2009], describing a uniform prior over correlation matrices. The model was fit using four chains with 1,000 post-warmup samples per chain, for a total of 4,000 posterior samples. Each chain used 1,000 warmup samples to calibrate Stan's No U-Turn Sampler. All analyses reported here converged (e.g., all $1 \leq \hat{R}{\rm s} \ll 1.01$).

```{r}
# If one was to run this as one model for all stops, the following code would work. However, this would assume
# that the correlation between VOT and f0 is constant across all stops. We therefore fit separate models to
# each stop.
#
# bf_vot =
#   bf(vot ~ -1 + poa : voicing + (-1 + poa : voicing | p1 | subj) + (-1 + poa : voicing | p2 | word)) +
#   lf(sigma ~ -1 + poa : voicing + (-1 + poa : voicing | q1 | subj) + (-1 + poa : voicing | q2 | word)) + gaussian()
#
# bf_f0 =
#   bf(f0_Mel ~ -1 + poa : voicing + (-1 + poa : voicing | p1 | subj) + (-1 + poa : voicing | p2 | word)) +
#   lf(sigma ~ -1 + poa : voicing + (-1 + poa : voicing | q1 | subj) + (-1 + poa : voicing | q2 | word)) + gaussian()

bf_vot =
  bf(vot ~ 1 + (1 | p1 | subj) + (1 | p2 | word)) +
  lf(sigma ~ 1 + (1 | q1 | subj) + (1 | q2 | word)) + gaussian()

bf_f0 =
  bf(f0_Mel ~ 1 + (1 | p1 | subj) + (1 | p2 | word)) +
  lf(sigma ~ 1 + (1 | q1 | subj) + (1 | q2 | word)) + gaussian()


# Helpful to get general structure of priors for this type of model:
#
# get_prior(
#   formula = bf_vot + bf_f0,
#   data = d.chodroff_wilson)

my.priors.bivariate = c(
  # No intercept priors since model is reparameterized to have no intercepts
#  set_prior("student_t(3, 0, 2.5)", class = "Intercept", resp = c("vot","f0Mel")),
  # weakly regularizing prior for fixed effect coefficiencts for sigma.
  set_prior("student_t(3, 0, 2.5)", class = "Intercept", resp = c("vot","f0Mel"), dpar = "sigma"),
  # weakly regularizing prior for fixed effect coefficiencts for mu.
#  set_prior("student_t(3, 0, 2.5)", class = "b", resp = c("vot","f0Mel")),
  # weakly regularizing prior for fixed effect coefficiencts for sigma.
#  set_prior("student_t(3, 0, 2.5)", class = "b", resp = c("vot","f0Mel"), dpar = "sigma"),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel")),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), dpar = "sigma"),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), group = c("subj", "word")),
  set_prior("cauchy(0, 5)", class = "sd", resp = c("vot", "f0Mel"), group = c("subj", "word"), dpar = "sigma")
)

# for (s in c("P", "B")) { # unique(d.chodroff_wilson$stop)) {
#   m <- brm(
#     formula = bf_vot + bf_f0,
#     data = d.chodroff_wilson %>%
#       filter(category == s),
#     prior = my.priors.bivariate,
#     chains = 4, cores = 4,
#     warmup = 3000,
#     iter = 4500,
#     control = list(adapt_delta = .995, max_treedepth = 20),
#     backend = "cmdstanr",
#     file = get_path(paste0("../models/production-syllable_initial_stop-VOT_f0Mel-bivariate_normal-simple_effects-", s)))
# }
```

<!-- (ref:chodroff-stop-VOT-mu-sigma) Estimated by-talker voice onset time (VOT) means $\hat\mu$ and standard deviations $\hat\sigma$ of word-initial stops in connected American English. The number of observations that went into the estimates ($n$) differ between talkers due to missing values [e.g., because of measurement errors or outliers; for details, see @chodroff-wilson2018]. Lines connect data from the same talker. Note that the y-axis is log-stepped. -->

<!-- (ref:chodroff-stop-f0ST-mu-sigma) Same as \@ref(fig:chodroff-stop-VOT-mu-sigma) but for the fundamental frequency (f0) at the onset of the vowel following the stop. -->

<!-- ```{r chodroff-means-sds, fig.width=10, fig.height = 3.5, fig.cap=c("(ref:chodroff-stop-VOT-mu-sigma)", "(ref:chodroff-stop-f0ST-mu-sigma)")} -->
<!-- # plot correlation between by-talker mean and sd for each cue -->
<!-- d.chodroff_wilson %>%  -->
<!--   group_by(Talker, gender, category, voicing, poa) %>% -->
<!--   summarise_at( -->
<!--     vars(VOT, f0_Mel, f0_semitones), -->
<!--     list("mean" = function(x) mean(x, na.rm = T), "sd" = function(x) sd(x, na.rm = T), "n" = length)) %>% -->
<!--   ggplot(aes(x = VOT_mean, y = VOT_sd, color = voicing, shape = gender, size = VOT_n)) + -->
<!--   geom_line(alpha = .1, size = .3, color = "gray", aes(group = Talker)) + -->
<!--   geom_point(alpha = .3) + -->
<!--   scale_x_continuous(expression("VOT" ~ hat(mu) ~ "(ms)")) + -->
<!--   scale_y_continuous(expression("VOT" ~ hat(sigma) ~ "(ms)"), trans = "log10") + -->
<!--   scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) + -->
<!--   scale_shape_discrete("Gender") + -->
<!--   scale_size_continuous("n", range = c(.1, 3)) + -->
<!--   facet_grid(. ~ poa)  -->

<!-- ggplot2::last_plot() +  -->
<!--   aes(x = f0_Mel_mean, y = f0_Mel_sd, size = f0_Mel_n) + -->
<!--   scale_x_continuous(expression("f0" ~ hat(mu) ~ "(Mel)")) + -->
<!--   scale_y_continuous(expression("f0" ~ hat(sigma) ~ "(Mel)"), trans = "log10") -->

<!-- # plot correlation between by-talker means of different cues -->
<!-- # last_plot() + aes(x = vot_mean, y = usef0_mean) -->
<!-- # last_plot() + aes(x = usef0_mean, y = cog_mean) -->
<!-- # last_plot() + aes(x = cog_mean, y = vot_mean) -->

<!-- # d.chodroff_wilson %>%  -->
<!-- #   group_by(Talker, stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(vot, usef0), -->
<!-- #     list("mean" = mean, "sd" = sd), -->
<!-- #     na.rm = T) %>% -->
<!-- #   group_by(stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(ends_with("mean"), ends_with("sd")), -->
<!-- #     list("mean" = mean, "median" = median, "sd" = sd), -->
<!-- #     na.rm = T) -->
<!-- #  -->
<!-- # d.chodroff_wilson.byTalker <- d.chodroff_wilson %>%  -->
<!-- #   group_by(Talker, stop, voicing, poa) %>% -->
<!-- #   summarise_at( -->
<!-- #     vars(vot, usef0, cog), -->
<!-- #     list("mean" = mean, "sd" = sd), -->
<!-- #     na.rm = T) -->
<!-- ``` -->

# Introduction

  * ^[Additional lines of work have investigated exposure to synthesized, vocoded, or otherwise distorted speech [e.g., @adank2009; @davis2005; @fenn2003; @mattys2012] or have used distributional learning paradigms [e.g., @clare2019; @clayards2008; @idemaru-holt2011; @maye2008; @schertz2013; @theodore-monto2019; @wade2007]. In the general discussion, we return to distributional learning paradigms, which we believe to hold particular promise in addressing the issues we identify below.] <!--TO_DO:this footnote should perhaps be moved elsewhere now that we do not talk about the two paradigms here--> 
  
  * Even if one were to take for granted that the three mechanisms in Figure \@ref(fig:overview) *jointly* underlie the effects of recent exposure, it remains unclear which of those mechanisms any given results sheds light on. For example, can we conclude from experiments like @clarke-garrett2004 that listeners can learn new representations (or at least select some mixture of existing representations) within two minutes of exposure? Or are those results due to normalization, and if so, where should we draw the line between processing and learning? Also, do the results of perceptual recalibration and accent adaptation stem from the same mechanism(s), or do they reflect different mechanisms [@baeseberk2018; @samuel-kraljic2009; @zheng-samuel2020]? Our comprehensive framework will make it possible to empirically test whether two paradigms that differ substantially in their design, tasks, and ecological validity of the speech stimuli would in fact engage the same mechanism(s). 
  
  * Changes in speech perception as a function of recent exposure have now been documented across a broad range of experimental paradigms and tasks [e.g., @bradlow-bent2008; @clayards2008; @eisner2013; @idemaru-holt2011; @norris2003; @vroomen2007]. different modes of speech [e.g., "clear" and "conversational" speech, @zhang-samuel2014], isolated and connected speech, [e.g., @eisner-mcqueen2005; @reinisch-holt2013], and languages [e.g., @chladkova2017; @eisner2013; @hanulikova-weber2012; @sebastian-galles2000; @schertz2015]. Changes can occur even under cognitive load, arguing for a relatively high degree of automaticity of some of the underlying mechanisms [@zhang-samuel2014; though both top-down attention-direction and interference can modulate adaptation, @mcauliffe-babel2016; @samuel2016].
  
  * ^[To the extent that behavioral research has begun to contrast competing hypotheses, these efforts have been limited to (A) normalization and (B) representational changes while ignoring the possibility of (C) post-perceptual decision-making. In contrast to behavioral work, neuroimaging research has focused more on the contrast between (C) vs. (A) and (B), grouping the latter together as functionally distinct from higher-level decision-related processes [but see @REF-XIN-DOES-SOMETHING-COME-TO-MIND].]

# Accent adaptation

 * There are a few noteworthy differences between the accent adaptation and the perceptual recalibration paradigms. First, most work on accent adaptation uses naturally-produced speech, instead of creating artificial speech via acoustic manipulation [e.g., @bradlow-bent2008; @sidaras2009; @baeseberk2013; @smith2014; @mitterer-mcqueen2009]. The specific manner in which L2-accented speech categories differ from L1-accented counterparts is constrained by sound systems of both languages [@bent-baeseberk2021]. L2-accented speech therefore naturally deviates from L1-accented speech along multiple cue dimensions[@xie-jaeger2020; @vaughn2019], rather than along just a single cue dimension as is often assumed for a typical perceptual recalibration experiment. 

 * Second, and relatedly, L1 and L2 accents can vary not only in categories' central tendencies (means) and dispersions (variances and covariances) but also types of phonetic cues that are used to distinguish the categories as well as weights given to each of them [@kim2002; @schertz2013; @ingvalson2011; @yamada-tohkura1992; @liu-holt2015; @harmon2019]. For instance, in Korean-accented English, the primary and and the secondary cues to the word-initial /d/ vs. /t/ contrast are reversed: f0, which is the secondary cue to voicing in L1-accented English, outweighs VOT [e.g., @schertz2015]. Another case in point is the word-final stops in Mandarin-accented English, where burst duration substitutes preceding vowel duration as the primary cue to voicing distinction [e.g., @xie2017].
 
 
  * It is often assumed, for example, that response biases can only change the relative proportion of answers but cannot explain overall improvements in accuracy. Under this assumption, overall improvements in the perception of an accent would seem to rule out explanations in terms of response biases, at least if the experimenter carefully balanced how often each category occurs during test. This is, however, a common misconception (one that we shared, for what it is worth). As we demonstrate below, changes in response biases can explain findings that are often attributed to changes in representations. The same applies to pre-linguistic normalization: contrary to common intuitions, simple normalization mechanisms can explain seemingly complex changes in listeners' categorization following exposure to L2-accented or otherwise shifted speech (as, e.g., in perceptual recalibration paradigms).
  
  <!--^[Likewise, an *absence of improvements* should not be taken as evidence to reject the hypothesis of representational changes. Such a conclusion is only valid if it is also shown that *successful learning of the exposure statistics* would be *predicted* to facilitate comprehension during test. Whether this prediction holds for a given experiment depends on the exposure and test stimuli---a fact that is sometimes not considered in the interpretation of experiments [cf. @zheng-samuel2020].]--> 
  
  <!--And these differences are often asymmetrically affecting the categories participating in a phonetic contrast. For example, L1-German speakers of English often have non-native realizations of /`r linguisticsdown::cond_cmpl("θ")`/ but have no trouble pronouncing /s/, which unlike /`r cond_cmpl("θ")`/ has a counterpart in their L1. These asymmetries in the production of L2 speech are reflected in L1 listeners' perception of L2-accented speech. For instance, L1-English listeners usually correctly identify German-accented word-final /t/s (e.g., *seat*), but mishear word-final /d/s as devoiced [for other asymmetries, see @schertz2015; @xie2017; @zheng-samuel2020].-->
  
  <!--In light of this state of the art, it would be ill-advised to interpret the  quantitative differences between the results of the two change models as meaningful. For these reasons, we focus on each model's ability to predict the qualitative results found in previous work. -->

# General discussion

  * ... in the broader context of other research. This includes research on accent adaptation and related topics that has pioneered analyses beyond changes in accuracy or processing speed, towards a more in-depth understanding of the link between the acoustic input and perception [e.g., @clare2019; @idemaru-holt2020; @kartushina2015; @kim2020; @schertz2015; @wade2007].
 
  * Additionally, specific linking hypotheses---like the ones we have introduced here---can be compared in terms of their *quantitative* fit against the data, while appropriately taking into account their functional flexibility [e.g., through model comparison measures like the Bayesian leave-one-out information criterion, @vehtari2017]. 
  
    *These separate lines of research (our own work included) seem to have drawn theoretical conclusions through convention, rather than through strong empirical evidence in favor of a particular mechanism. 

### Facilitating theory building 
Besides the two classes of experimental paradigms we detailed above, there are a wider range of results cited as evidence for or against a particular mechanism underlying accurate and robust speech perception. In particular, the assumption that recent exposure can shape linguistic representations, or at least their weighting or selection (X. Xie et al., 2018), is central to many accounts of sociophonetics and exemplar-based theories of speech perception more broadly (for review, see Hay et al., 2019; Kleinschmidt & Jaeger, 2015; Sjerps et al., 2019). Multiple non-linguistic factors have been shown to inform and impact the selection, such as the (inferred) physiology [@krauss2002], social identity of a talker [e.g., regional origin, @hay-drager2010; @niedzielski1999; sex, @johnson1999; @strand1999; age, @skoogwaller2015; @walker-hay2011; and individual identity, @nygaard1994; @remez2018], or a situational context [e.g., being in a car, @hay2017]. These effects are thought to draw on episodic memory traces from the listeners' past linguistic experiences and hence inexplicable based soley on lower-level auditory signal normalization. Conversely, a separate line of research has found that non-speech stimuli (e.g., pure tones) can systematically alter the perception of subsequently played speech stimuli [@chodroff-wilson2020; @holt2005; @holt2006; @huang-holt2011], which seems to be harder to explain through changes in, or a selection of, representations. Tested separately on distinct types of stimuli within distinct experimental procedures, these theoretical positions have coexisted for decades by now. 
 
The current results urge us, and hopefully other researchers in the field, to revisit this equilibrium. The empirical indeterminacy we identified for perceptual recalibration and accent adaptation likely applies to *any* form of exposure-test paradigm, regardless of whether the exposure stimuli being researcher curated (in the case of non-speech sound exposure) or natural (in the case of exposure to various talkers throughout lifetime). As shown in Case Studies 1 and 2, depending on the listeners' prior expectations and exact stimulus features in the input, normalization and changes in decision-biases can predict _a wider range of highly complex response patterns than previously recognized_. This means that none of the three mechanisms can be simply dismissed without explicit, empirical tests. In other words, any behavioral experiment that does not annotate/examine stimulus features and analyze link between those features and participants' responses cannot be claimed to support one of the hypotheses over the others.


### Do the mechanisms combine or trade-off? If so, when?
The second avenue in which we anticipate a breakthrough pertains to development of theories that delve into different assumptions about each mechanism. For instance, it has so far been assumed that distributional learning of category statistics [as shown by @clayards2008 and subsequent replications and extensions @bejjanki2011; @nixon2016; @theodore-monto2019] entails representational changes. Specifically, participants that were exposed to /b/ and /p/ categories with larger variance along VOT exhibited more shallow categorization slopes than participants who were exposed to /b/ and /p/ categories with smaller variance (but the same means as the other condition). These changes may not be easily explained by a cue-based normalization mechanism such as C-CuRE. This does not, however, rule out explanations in terms of *other* normalization. For example, normalization that standardizes cues relative to expectations, in addition to centering them---which we might call *S*-CuRE (for "standardizing")--- may be able to account for the variance shifts found by Clayards et al. (2008). Further, it is possible that listeners may learning distributional structures *post* low-level normalization. e.g., They might first normalize/standardize cues [as has been proposed for the perception of vowels  @johnson2020; @lobanov1971; @monahan-idsardi2010] and then learn and store residual idiosyncrasies [For a preliminary test of this idea, see @xie2021cognition]. Yet alternatively, the findings of Clayards et al. (2008) might also be accounted for by changes in response biases provided that the lapse rate is not zero: recall that changes in response biases can have non-additive effects when the lapse rate is non-zero (Section \@ref(sec:change-bias)). And non-zero lapse rates were indeed observed by Clayards et al. (2008, Figure 3B and footnote 2). Changes in response biases could thus potentially account for changes in the slope of categorization functions---the result observed in Clayards et al. (2008). 

A related thought-provoking possibility may be that distinct mechanisms can combine or trade off with one another. We have so far attempted to contrast the three mechanisms as contenders. ASP, however, will also help us move beyond this either-or comparisons and assess the relative involvement of the mechanisms as a function of stimulus features and the amount of input. This is important as the three mechanisms could be distinct in terms of cognitive and memory demands and hence differentially costly. For example, to arrive at a precise estimate of a given talker's cue mean (for normalization), listeners must receive at least some amount of exposure to the talker. The minimum amount of exposure required for deriving category means and variances (for learning category representations) is expected to be larger, especially for variance estimates. This makes the representational learning mechanism relatively costly and taxing in terms of memory demands. On the other hand, the decision-bias changes do not require any storage of acoustic-phonetic details, and hence are relatively (computationally) cheap. It is possible that L1 listeners who encounter an unfamiliar accent might initially resort to the decision-making changes to boost their recognition accuracy. As they accrue input to a given talker or accent, they may begin to rely more on the other two mechanisms. Relatedly, listeners might opt to expend more resources to learn distributional properties of categories when doing so would in fact be expected to improve recognition accuracies. They might be less likely to do so when L2 categories are, for instance, phonetically overlapping with one another (e.g., Mandarin-accented talkers tend to pronounce [`r linguisticsdown::cond_cmpl("θ")`] (as in *thick*) as [s] (as in *sick*), Zheng and Samuel (2020)). In such a scenario, the bottom-up signal does not distinguish between intended categories, which makes cue normalization or representational learning rather inefficient as a strategy. Instead, simply increasing the rate of [`r linguisticsdown::cond_cmpl("θ")`] independent of the stimulus could more efficiently improve recognition accuracies. Indeed, a recent MEG study by Blanco-Elorrieta et al., (2021) found strong engagement of a pre-frontal cortex in processing phoneme substitutions (e.g., [s] mispronouced as [`r linguisticsdown::cond_cmpl("ʃ")`]), implicating post-perceptual decision-making processes. Testing these fine-grained hypothesis would require careful parameterization of acoustic-phonetic features, categories/contrasts, and amounts of exposure within subjects. The current analytical framework and the new standards of experimental design and analyses laid out above will be indispensable for this endevour.

### Neurobiological bases of adaptation
The primary goal of the current investigation has been to make strong inference through behavioral testing. Once achieved, it will facilitate more targeted neuro-imaging research. As discussed in the introduction, shifted categorization boundaries in response to the same physical input have been associated with multiple regions, such as early auditory regions [anterior PT, @kilianhutten2011], regions implicated in phoneme classification [posterior STG/STS, @bonte2017; @myers-mesite2014; @ullas2020], as well as regions for talker identity processing [right temporal regions, @luthra2020a]. The left parietal lobe and the insula, which are implicated in perceptual decision-making [e.g., @dacremont2013; @keuken2014], have also been shown to exhibit distinct activation for different exposure conditions [@bonte2017; @myers-mesite2014; @ullas2020]. As in behavioral experiments, existing results leave open which neural networks are responsible for the behavioral changes observed after recent exposure. Even a significant activation of a given brain region can be associated with functionally-distinct sources. For instance, the activation of left parietal lobe might reflect its general role in perceptual decision making [e.g., @dacremont2013; @keuken2014], or it could be due to a more specific role in phonological processing [e.g., processing abstract category information, @guediche2014]. One limitation of these works is that the conclusion is based primarily on a binary distinction in a brain region’s activation pattern (e.g., activity between categorization trials that a /d/ response is made and those in which a /t/ response is made for the same stimulus). More recently, neuroimaging studies have started to approach questions about the underlying neural mechanisms through multivariate analyses. These analyses are crucial for identifying the encoding of perceptual experiences across a distinct array of experimental conditions. Multivariate analyses can also be more sensitive in detecting fine-grained patterns *within* regions responsible for multiple cognitive demands [e.g., @bonte2017; @luthra2020a]. The methodological recommendations we made above may help generate behavioral responses that enable multivariate analyses required for further dissociation among regions functionally responsible for exposure-driven changes.

Finally, Another promising avenue is to pair temporally-sensitive techniques with imaging methods with good spatial resolution. For instance, studies using a combination of EEG and MEG have identified key regions that regulate perceptual learning of degraded speech by responding to manipulations of either low-level signal clarity or high-level prior knowledge of the speech content [@sohoglu-davis2016; @sohoglu-davis2020]. 



<!--there are distinct lines of work that normaliz as episodic  taken to suggest that listeners have implicit representations that encode expectations about talkers and types of talkers. These findings do, however, leave open whether these expectations relate to pre-linguistic normalization or to linguistic categories [see also discussion of "relativization" in @apfelbaum-mcmurray2015, p. 936-938]. They also leave open to what extent recent (e.g., within-experiment) experience affects speech perception through the same mechanisms that underlie the effects of inferred physiology or social identity. (This includes the question of whether and how recent experiences can lead to the learning of new linguistic representations. (e.g., Can listeners *learn* characteristics of a talker within 2 mins of exposure?) The approach and the recommendations provided above will make it possible to explicitly test these subtly distinct possibilities, which have not thus far possible.--> 



  * And while distinct normalization procedures have been compared to each other [e.g., @adank2004; @hoffmanbion-escudero2007; @kiefte-nearey2019] or against the absence of normalization [@mcmurray-jongman2011], they are rarely compared to the competing hypothesis that changes in representations underlie the effects of recent exposure [for notable exceptions, see @apfelbaum-mcmurray2015; @lehet-holt2020; @xie2021cognition]. ^[It is, however, worth pointing out that the exact nature of normalization remains unclear. This includes questions about the specific transformations that are applied [@REFS] but also questions about whether normalization is best viewed as an autonomous, encapsulated process, or as part of a larger hierarchical inference process that includes inferences at both levels traditionally considered linguistic and levels traditionally considered pre-linguistic. Research on automatic speech recognition, for example, has found that deep neural network models can to some extent remove the need for specialized signal transformations [@deng2016]. Under this view, normalization is itself the result of a hierarchical predictive process that seeks to efficiently predict the incoming signal [for related discussion, see @clark2013; @kell2018; @kleinschmidt-jaeger2015; @kuperberg-jaeger2016].] 


  * <!--leave it here for now, but potentially can be integrated into GD-->An explicit mechanistic account delineating how adaptive changes also strengthens our ability to probe its neural substrates. To date, multiple neurobiological systems have been proposed to support these rapid changes. They range from the primary auditory cortex  [e.g., Heschl’s gyrus] to those that respond to acoustic-phonetic features in speech [e.g., superior temporal gyrus, @Yi2019]. Even wider heterogeneous networks have been implicated in studies assessing brain responses to novel speech input, including those responsible for recognizing a voice/talker [@luthra2020b], attending selectively to a given talker's speech [@wong2004], and correcting prediction errors when there is a discrepancy between a predicted vs. an actual input [e.g., @guediche2015evidence], to name a few. Identifying the roles these brain regions play during adaptive speech perception is not straightforward, as these regions often play multiple roles in cognitive processing. Therefore, the exact interpretation of the findings still depends on the researchers’ hypothesis of the underlying mechanism. For instance, changes in inferior frontal gyri activation in response to accented speech has been interpreted as reflecting greater computational demand [@yi2014neural] or decision related phonetic categorization of ambiguous stimuli [@myers-mesite2014]. A proper theoretical framework that specifies how the alternative mechanisms can account for adaptive changes in speech perception, either in isolation or jointly, will facilitate the design and interpretations of neural investigations.
  
  * This doesn actually seem to be what we do:  Results of our simulations illuminate when---to what stimuli and under what circumstances---the three mechanisms would provide diverging predictions about adaptive changes of perception. We also learn about how much data will be needed to draw reliable statistical inferences from behavioral data. 
  
  * Perhaps better for discussion: Frameworks like ASP that encompasses (A)-(C) thus hold the potential to meaningfully link across multiple lines of behavioral and neuroimaging research [for a related discussion, see @guediche2014, p. 8]. 
  
  <!-- By developing a computational framework that at least acknowledges, and integrates these aspects of speech perception, we take on (small) step towards addressing the challenge identified by @guediche2014: "there is no formal speech perception model that relates activity in the cortical regions identified via neuroimaging to the computational demands of adaptive plasticity in speech perception. Conversely, the classic computational models of speech perception that have attempted to differentiate how the system may meet the computational demands of adaptive plasticity have not made specific predictions of the underlying neural mechanisms".  -->
  
  
## Computational limitations of change models afford qualitative tests of their *sufficiency* {#sec:computational-limitations}
The three change models---as well as the three eneral hypotheses that they aim to implement---differ in their computational parsimony. For example, while both normalization and the representational change model assume that listeners are sensitive to talker-specific changes in the usage of phonetic cues, only the latter tracks those differences separately for each category (e.g., /d/ vs /t/). This makes normalization computationally more parsimonious than representational changes for both researchers and listeners: for the normalization model, the number of parameters that researchers need to determine (e.g., fit from the behavioral data of perception experiments), and the number of estimates that listeners need to infer and store from the speech input does not increase with the number of categories [for related discussion, see also @apfelbaum-mcmurray2015]. <!-- TO DO: Xin, this was stating something wrong. I hope it's now clearer --> For example, for the C-CuRE-based normalization model employed in our case studies, researchers need to determine only one parameter ($\kappa_0$) and listeners are assumed to infer and store only the *overall* means of all cues ($K$ values for $K$ cues). In contrast, the representational change model employed in our case studies requires researchers to determine two parameters *per category*, and listeners are assumed to infer and store the cue means and covariance matrices of *each category* ($JK + \frac{J}{2}(K^2+K)$ values for $K$ cues and $J$ categories).^[Even if C-CuRE normalization, which only centers cues, is made more comparable to the representational change model by extending normalization to include cue standardization [also known as z-scoring, as used in, e.g., Lobanov normalization, @lobanov1971], this would introduce just one additional parameter for researchers (the equivalent for $\kappa_0$ but for the estimation of the cue variances), and listeners would be assumed to learn and store $2K$ values for $K$ cues.] <!-- TO DO: Xin, Chigusa: I moved this into footnote. It felt like an aside. ok? --> 

While the parsimony of normalization offers a computational advantage in terms of efficiency, it also comes with limitations. We discuss three such limitations that future work can exploit to evaluate the sufficiency of this mechanism. First, normalization accounts predict that the effects of exposure on subsequent perception do not depend on the category membership inferred by listeners.<!--^[To be precise, some normalization accounts allow the category identities of *surrounding* segments of speech to affect the normalization of cues on the target segment. This is, for example, how C-CuRE normalizes for effects of surrounding phonetic context [@mcmurray-jongman2011].<!-- This makes me wonder how they explain the Holt findings that a sine tone can affect subsequent perception. Do McMurray & Jongman even discuss this finding? [CK] They do. They actually say that C-CuRE builds on the type of "auditory contrast accounts" driven by any surrounding speech. They suggest that the phonetic context can be used but are not always needed. e.g., "C-CuRE also builds on auditory contrast accounts (Lotto &Kluender, 1998, Holt, 2006; Kluender, Coady & Kiefte, 2003) by proposing that cues are interpreted relative to expectations, though these expectations can be driven by categories (perhaps in addition to lower-level expectations)"(Section 5.5)This does not, however, affect the point we make here.]--> For example, for C-CuRE, only the overall cue mean of the input can affect subsequent perception, regardless of whether the lexical context labels the input as one category or another. Although not originally discussed in the context of normalization, there is evidence that challenges this prediction. In their seminal study on perceptual recalibration, @norris2003 exposed participants to /f/- or /s/-biased inputs using the same general design that we discussed for Case Study 1. The critical conditions exposed participants either to words with typical /f/ and words with atypical (shifted) /s/ or to words with typical /s/ and words with atypical (shifted) /f/. This resulted in the signature perceptual recalibration effect.
Importantly though, no boundary shift was observed in control conditions where the shifted tokens were embedded in non-words instead of real words, everything else being identical. In short, the control conditions differed from the experimental conditions almost exclusively in whether the atypical inputs were lexically labeled to be of a particular category.<!--Importantly though, Norris and colleagues also included several control conditions. Two control conditions of relevance to the present discussion exposed participants either to words with typical /f/ and *non*-words with atypical (shifted) /s/ or to words with typical /s/ and *non*-words with atypical (shifted) /f/. The atypical /f/ and /s/ sounds in these control conditions were acoustically identical to those in the experimental conditions, and the non-word contexts in the control conditions matched the word contexts in terms of the phonetic context surrounding the critical /f/ and /s/ sounds (specifically, in terms of lexical stress and the vowel immediately /f/ or /s/). In short, the two control conditions differed from the experimental conditions almost exclusively in whether the atypical inputs were lexically labeled to be of a particular category. Unlike the experimental conditions, however, the control conditions did *not* elicit the signature boundary shift [@norris2003, Experiment 2].--><!-- As Norris and colleagues concluded, "[...] perceptual learning depended on exposure to an ambiguous speech sound in lexically biased contexts" (p. 227).--> The perceptual recalibration effect thus seems to depend on the category that the shifted atypical tokens are attributed to [@norris2003, p. 227]---contrary to what would be expected if adaptive speech perception was solely achieved through cue-level normalization.<!-- ^[Later work found that the label (biasing information) does not have to be provided by the *lexical* context. For example, visual information about lip position [@vroomen2007] can induce perceptual recalibration, too. Neither do the shifted atypical tokens have to be precisely "ambiguous" (i.e., located at the prior category boundary). For example, smaller shifts from the prior category mean towards the category boundary can also elicit (smaller) boundary shifts [@babel2019]. But this does not affect the take-home message of Norris et al.'s (2003) finding.]--> The fact that this finding was obtained for (Dutch) fricatives is of particular interest since C-CuRE normalization has been found to provide a good fit against the recognition of (American English) fricatives. While most of the tests of C-CuRE were non-contrastive [e.g., @mcmurray-jongman2011; @mcmurray-jongman2016], @apfelbaum-mcmurray2015 compared C-CuRE to a representational change model (specifically, exemplar models) and found very similar performance, with the latter providing a slightly better fit against human responses. <!-- TO DO: Xin or Chigusa: is this (preceding sentence) what you meant? I tried to make it more explicit. --> We consider the question of how labeling information affects adaptive speech perception as a productive venue for future research. 

A second computational limitation of normalization accounts is specific to accounts that only correct for differences in the overall mean of cues but not differences in cue variability (like C-CuRE). <!--Such normalization accounts seem to be in conflict with existing findings, although these findings have to the best of our knowledge not previously been discussed in these terms. -->In a ground-breaking study that was targeted at a separate question, @clayards2008 exposed participants to distributions of synthesized /b/ and /p/ tokens (as in, e..g, *beach*-*peach* continuum). Between participants, the VOT of these tokens had been manipulated to either form wide or narrow VOT distributions for both /b/ and /p/. The VOT means of /b/ and /p/ were identical in both conditions<!-- (0 and 50 msecs, respectively)-->. Since acoustic cues are encoded relative to the cue mean only in C-CuRE and similar normalization accounts, these accounts thus do *not* predict any differences in the effects of these exposure conditions. Clayards and colleagues, however, found that participants in the wide variance condition exhibited shallower categorization functions along the VOT continuum [conceptually replicated in @nixon2016]---as predicted by representational change models [see @clayards2008; @kleinschmidt-jaeger2015; @theodore-monto2019]. Future research should test whether this finding replicates for more natural-sounding (rather than resynthesized, robotic-sounding) stimuli and in situations where task demands more closely resemble those of everyday speech perception (less repetition, more lexical heterogeneity, words presented in sentential contexts rather than in isolation, etc.). <!-- TO DO: was Theodore & Monto not natural sounding? I think it was. So then the first part of this has been done. Let's discuss. --> If such replications are obtained, this would argue that normalization would at least have to include standardization or similar corrections for the variability of cues [as proposed in @johnson2020; @lobanov1971; @monahan-idsardi2010].

A third limitation of normalization accounts that can be productively explored in future research is the lack of category specificity. Consider for example, a possible extension of the study by Clayards and colleagues. In the study by @clayards2008, both /b/ and /p/ either had narrow variance along VOT (SD = 8 msecs) or wide variance (SD = 14 msecs). This confounds the category-specific variance with the overall variance of the cues along VOT. It is, however, possible to manipulate the variance of the two categories while keeping both the means of the two categories (and thus the overall cue mean) and the overall cue variance constant. Figure \@ref(fig:proposed-experiment-asymmetric-variance) depicts two exposure conditions that achieve this, along with changes in the categorization function predicted by normalization and representational change models. This demonstrates how researchers might use the lack of category-specificity to disentangle normalization and representational change accounts. Other possible manipulations in this vein could include category-specific changes in the *co*variance of cues.<!--^[This is not to be confused with another experimental manipulation that is sometimes describes in terms of covariance [@REF]: changes in which of two or more cues exhibits the largest relative distance between two category means [which changes the relative reliability of cues, as discussed in @schertz-clare2020]. ASP-based simulations would be required to assess whether such manipulations cannot also be explained by normalization. We suspect that they can.]--> Basically, any manipulation that leaves the *overall* cue mean and variance unaffected while predicting differences in the categorization function under the representational change model can test whether the human listeners exhibit more flexibility than expected by normalization accounts. We note, however, that any such test should take into account that listeners can have strong prior beliefs based on the speech input they have received previously, and that this might include strong prior beliefs about *how* the realization of categories varies across talkers [@kleinschmidt-jaeger2015]. If the manipulations employed by researchers strongly violate those expectations, this needs to be carefully considered in the derivation of predictions. 


(ref:proposed-experiment-asymmetric-variance) A possible way to contrast normalization and representational change accounts. Panel A: two exposure conditions with identical overall means and variances along VOT, but different category-specific variances. Panel B: the predictions of the best-performing normalization and representational change models. For this purpose, the normalization model was extended to both center and standardized the cues.^[We note that the specific predictions for the representational change model shown here assume that listeners have equally strong prior beliefs about the variance of both of the two categories (i.e., one $nu_{0,c}$ for both categories). This is what we assumed in our case studies---which aimed to show that even simplified versions of each change model can explain a wide range of findings in the literature---but it is not an inalienable assumption for representational change models [for discussion, see also @kleinschmidt-jaeger2015]. <!-- TO DO: can we find the page number where K&J discuss separate nus for each category and add it here? --> The general prediction of representational change models that listeners are sensitive to the category-specific variance should hold even if this assumption is removed. How easy to detect this predicted difference is, however, expected to depend on this assumption.] <!-- TO DO: Xin, any chance you could add such a figure. for this we can just perfectly center and standardize (i.e., kappa_0 and the newly introduced nu_0 are both 0); the best performing representational change model should be nu_0,c = 4 and kappa_0,c = 1024, assuming you put the category means on the a priori expected locations. -->

```{r proposed-experiment-asymmetric-variance, fig.cap="(ref:proposed-experiment-asymmetric-variance)"}
# Put figure here.
```

<!--Just like normalization accounts are limited in the types of changes they can account for, so are change models for decision-making. -->In Section \@ref(sec:framework), we showed that the limitations of change models for  decision-making are less well understood than sometimes assumed. One recommendation for future research is thus to further explore the mathematical limitations of decision-making change models. Designs that limit attentional lapses to basically zero [e.g., by employing more engaging tasks, as in gamified paradigms, @wade-holt2005; @lim-holt2011] would emphasize the computational limitations of decision-making change models. In such scenarios, changes in response biases can only explain changes that are additive in the posterior log-odds of categories (Section \@ref(sec:change-bias)). That is, changes in response biases cannot account for changes in the *slope* of categorization functions. Future studies should use exposure conditions for which such changes are predicted by representational or normalization models to test whether changes in response biases are sufficient to explain adaptive speech perception<!-- [e.g., if zero lapses had been observed in @clayards2008, which was, however, not the case]-->. Since it can be difficult to detect changes in categorization slopes---especially without making strong linearity assumptions, we suspect that this question is better explored by manipulating the relative reliability of two cues (see also Figure \@ref(fig:show-model-categorization-3D-plots-similar-accuracy)). Such manipulations are routinely used in a paradigm known as "dimension-based statistical learning" [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015]. With recent trends towards analyses that more transparently link phonetic cues to changes in participants' responses [@idemaru-holt2020; @schertz-clare2020, see also recommendation XXX in Section \@ref(sec:methodological-advances)]  <!-- TO DO: fill in --> this field of study is in a good position to test the sufficiency of changes in decision-making accounts. An ongoing project from one of our labs, uses ASP-like simulations in combination with experimental designs that are intended to directly address this question [@burchill-jaeger2022].

Conversely, there are ways to assess whether representational changes alone are sufficient to explain all forms of adaptive speech perception. For example, if auditory input that arguably carries no information about category statistics affects subsequent speech perception, this provides evidence that changes in representations alone cannot explain all aspects of adaptive speech perception. Perhaps one of the most convincing demonstrations of this type comes from the findings of auditory enhancement effects, wherein non-speech stimuli (e.g., pure tones) can systematically alter the perception of subsequently played speech stimuli [@chodroff-wilson2020; @holt2001; @holt2005; @holt2006; @huang-holt2011; for review, see also @weatherholtz-jaeger2016]. While these demonstrations might be challenged for lack of ecological validity (potentially inviting meta-reasoning about experimenters' intentions that is unlikely to be present during everyday speech perception), it is unclear how representational changes---or, for that matter, changes in response biases---can explain such findings. At the very least then, these findings suggest that normalization *can* be involved in adaptive speech perception.

In sum, by focusing on the computational assumptions of the different change models, it is possible to conduct behavioral experiments that can decisively determine whether either of the two computationally more parsimonious change models is *sufficient* to explain adaptive speech perception, or whether changes in representations are necessary to explain adaptive speech perception. 

```{r AA-result-comparison-additional-cases-history, warning=FALSE, eval = FALSE}
#DEBUG remove if done
cases = c("Cue_reweighting", "Contrast_shift", "Contrast_reduction", "Contrast_collapse")
history.optimization.representations.all = data.frame()
history.optimization.bias.all = data.frame()
history.optimization.normalization.all = data.frame()

for (i in 1:length(cases)){
  example_label = cases[i]
  
  #load history of optimization for representations model
  history.optimization.representations.current <- 
    readRDS(get_path(paste0("../models/d.AA.history.optimization.representations_", example_label,".rds"))) %>%
    mutate(Case = example_label) 
  history.optimization.representations.all <- 
    bind_rows(history.optimization.representations.all, history.optimization.representations.current)
  rm(history.optimization.representations.current)
  
  #load history of optimization for bias model
  history.optimization.bias.current <- 
    readRDS(get_path(paste0("../models/d.AA.history.optimization.bias_", example_label,".rds"))) %>%
    mutate(Case = example_label) 
  history.optimization.bias.all <- 
    bind_rows(history.optimization.bias.all, history.optimization.bias.current)
  rm(history.optimization.bias.current)
  
  #load history of optimization for normalization model
  history.optimization.normalization.current <- 
    readRDS(get_path(paste0("../models/d.AA.history.optimization.normalization_", example_label,".rds"))) %>%
    mutate(Case = example_label) 
  
  history.optimization.normalization.all <- 
    bind_rows(history.optimization.normalization.all, history.optimization.normalization.current)
  rm(history.optimization.normalization.current)
}
```

```{r compare-AA-models-results-across-three-mechanisms-best-performing-nearest, warning=FALSE, eval= FALSE}
# DEBUG: remove this chunk if we opt to use the exact best-performing parameters
# load best-performing parameterization for the cue-reweighting case
example_label = "Cue_reweighting"
best_performing_parameters.representations <- readRDS(get_path(paste0("../models/best_performing_parameters.representations_", example_label,".rds")))
best_performing_parameters.bias <- readRDS(get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))
best_performing_parameters.normalization <- readRDS(get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))
example_label = paste0(example_label, "_best-performing")
# select the model parameters to be illustrated as an example where all three models predict a similar overall accuracy
prior_kappa.selected = d.AA.representations$prior_kappa[which.min(abs(as.numeric(as.character(d.AA.representations$prior_kappa)) - best_performing_parameters.representations$par[1]))]
prior_nu.selected = d.AA.representations$prior_nu[which.min(abs(as.numeric(as.character(d.AA.representations$prior_nu)) - best_performing_parameters.representations$par[2]))]
posterior.lapse_rate.selected = d.AA.bias$posterior.lapse_rate[which.min(abs(d.AA.bias$posterior.lapse_rate - best_performing_parameters.bias$par[1]))]
beta_pi.selected = d.AA.bias$beta_pi[which.min(abs(d.AA.bias$beta_pi - best_performing_parameters.bias$par[2]))]
prior_kappa.normalization.selected = d.AA.normalization$prior_kappa.normalization[which.min(abs(as.numeric(as.character(d.AA.normalization$prior_kappa.normalization)) - best_performing_parameters.normalization$par[1]))]


compare_models.AA(d.AA.representations, d.AA.bias, d.AA.normalization, VOT_range, f0_range, n_points)

```

```{r code-from-bias-model-in-AA-section}
# sanity check to see if the target_accuracy_se is achieved before the max.simulations
# temp <- bias.pred %>%
#   ungroup() %>%
#   group_by(Condition, Subject, posterior.lapse_rate, beta_pi, sim) %>%
#   summarise(response.mean = mean(response)) %>%
#   group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
#   summarise(
#     n.sims_so_far = n_distinct(sim),
#     response.mean.mean = mean(response.mean),
#     response.mean.se = sd(response.mean) / sqrt(n_distinct(sim)))
```



(ref:PR-result-summary) Predictions of the three models compared against human responses. **Panel A - Categorization functions observed from human perception experiment:** Same as Figure \@ref(fig:kraljic-samuel-2007-replotted). **Panel B - Change model predictions:** From left to right: predicted categorization responses from changes in representations, decision-making, and normalization, shown for the `r length(test_locations)` test locations after `r paste(categories.PR, collapse = "- and")`-shifted exposure. For each change model, a set of parameterization that yields performance similar to human responses is selected for illustrative purpose.

```{r PR-result-summary, fig.width= base.width * 5, fig.height= base.height + 1, fig.cap="(ref:PR-result-summary)"}

prow.B <- plot_grid(
    p.PR.representations.highlighted + theme(legend.position="none") + myGplot.defaults(base_size = 12, set_theme = F),
    p.PR.bias.highlighted + theme(legend.position="none") + myGplot.defaults(base_size = 12, set_theme = F),
    p.PR.normalization.highlighted + theme(legend.position="none") + myGplot.defaults(base_size = 12, set_theme = F),
    ncol = 3, 
    rel_widths = c(0.25, 0.25, 0.23))
prow <- 
  plot_grid(
    p20 + myGplot.defaults(base_size = 12, set_theme = F) + theme(legend.position="none", plot.margin = margin(t = 1.3, r = 1, b = 0.7, l = 0, unit = "cm")), 
    prow.B,
    labels = c('A)', 'B)'), 
    ncol = 2, 
    rel_widths = c(0.27, 0.73))

# extract a legend that is laid out horizontally
legend_prow <- get_legend(p.PR.representations.highlighted)
plot_grid(legend_prow, prow, ncol = 1, rel_heights = c(.2, 1))  
```