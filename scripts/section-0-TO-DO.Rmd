# THINGS TO DO
Let's delete to-dos after they have been addressed, and post in Slack what we've done. Thanks.

  * ALL: 
    * Think about where comment about parameter space partitioning could go.
    * Should we either use 1\$\beta$ of 1\$\kappa$ etc. to conceptually make all parameters either 'confidence' / weight of prior knowledge OR learning rates? 
    * What's going on with "talker-related" vs. "talker-specific". I guess the former was introduced by someone to NOT assume talker-specific. That's the type of terminological change that one should make others aware of to make sure we are consistent throughout the paper.

  * Xin:
    *  **Relative benefit of the three models: carefully adjust the caption and main texts for case study 2 and the general discussion accordingly.**
    * Figures at end of Section 4
    * Missing Figure in discussion
    * Integrate MVPA into general discussion (currently 6.2) and revise those parts. Also consider 1 sentence in introduction ("we return to this point in the general discussion") following the sentence about neuroimaging leaving open what types of computations take place in which area.
    * Check whether you can fix toc refs / links to SI sections with paragraph symbol.
    * CHECK ALL REMAINING TO-DO comments in the text. THERE ARE NEW ONES.
    
  * Xin and Chigusa:
    * I cut some passages. There are linked in this section below. Please check whether they ought to go somewhere else. If you're ok cutting them, please move them into Cutout-stuff.Rmd.

  * Florian: 
    * CHECK OUT WHETHER CHANGES IN NORMALIZATION CAN LEAD TO NON-LINEAR CHANGES *DESPITE THE FACT THAT WE'RE USING SIMPLE LINEAR INTERPOLATION*

## Things to perhaps do later:

  * Florian
    * make a wrapper for categorization function (through optional argument) that makes the functions monotonic. do the math.


## CUTS---PLEASE CHECK CAREFULLY AND DELETE THOSE THAT YOU THINK CAN GO



  * And while distinct normalization procedures have been compared to each other [e.g., @adank2004; @hoffmanbion-escudero2007; @kiefte-nearey2019] or against the absence of normalization [@mcmurray-jongman2011], they are rarely compared to the competing hypothesis that changes in representations underlie the effects of recent exposure [for notable exceptions, see @apfelbaum-mcmurray2015; @lehet-holt2020; @xie2021cognition]. ^[It is, however, worth pointing out that the exact nature of normalization remains unclear. This includes questions about the specific transformations that are applied [@REFS] but also questions about whether normalization is best viewed as an autonomous, encapsulated process, or as part of a larger hierarchical inference process that includes inferences at both levels traditionally considered linguistic and levels traditionally considered pre-linguistic. Research on automatic speech recognition, for example, has found that deep neural network models can to some extent remove the need for specialized signal transformations [@deng2016]. Under this view, normalization is itself the result of a hierarchical predictive process that seeks to efficiently predict the incoming signal [for related discussion, see @clark2013; @kell2018; @kleinschmidt-jaeger2015; @kuperberg-jaeger2016].] 


  * <!--leave it here for now, but potentially can be integrated into GD-->An explicit mechanistic account delineating how adaptive changes also strengthens our ability to probe its neural substrates. To date, multiple neurobiological systems have been proposed to support these rapid changes. They range from the primary auditory cortex  [e.g., Heschl’s gyrus] to those that respond to acoustic-phonetic features in speech [e.g., superior temporal gyrus, @Yi2019]. Even wider heterogeneous networks have been implicated in studies assessing brain responses to novel speech input, including those responsible for recognizing a voice/talker [@luthra2020b], attending selectively to a given talker's speech [@wong2004], and correcting prediction errors when there is a discrepancy between a predicted vs. an actual input [e.g., @guediche2015evidence], to name a few. Identifying the roles these brain regions play during adaptive speech perception is not straightforward, as these regions often play multiple roles in cognitive processing. Therefore, the exact interpretation of the findings still depends on the researchers’ hypothesis of the underlying mechanism. For instance, changes in inferior frontal gyri activation in response to accented speech has been interpreted as reflecting greater computational demand [@yi2014neural] or decision related phonetic categorization of ambiguous stimuli [@myers-mesite2014]. A proper theoretical framework that specifies how the alternative mechanisms can account for adaptive changes in speech perception, either in isolation or jointly, will facilitate the design and interpretations of neural investigations.
  
  * This doesn actually seem to be what we do:  Results of our simulations illuminate when---to what stimuli and under what circumstances---the three mechanisms would provide diverging predictions about adaptive changes of perception. We also learn about how much data will be needed to draw reliable statistical inferences from behavioral data. 
  
  * Perhaps better for discussion: Frameworks like ASP that encompasses (A)-(C) thus hold the potential to meaningfully link across multiple lines of behavioral and neuroimaging research [for a related discussion, see @guediche2014, p. 8]. 


<!-- By developing a computational framework that at least acknowledges, and integrates these aspects of speech perception, we take on (small) step towards addressing the challenge identified by @guediche2014: "there is no formal speech perception model that relates activity in the cortical regions identified via neuroimaging to the computational demands of adaptive plasticity in speech perception. Conversely, the classic computational models of speech perception that have attempted to differentiate how the system may meet the computational demands of adaptive plasticity have not made specific predictions of the underlying neural mechanisms".  -->

## Somewhere list all the factors that can determine power during test:

1. the expected change in the categorization function during exposure (describe elsewhere what this is a fucntion of)

2. the location of the test tokens

3. the task used during test
3a. the lapse rate, which is a function of how boring and/or streneous the task is
3b. response biases
3c. response decision criterion relaxation

4. the dependent variable
4a. the measurement noise
4b. the inherent variability

5. anything that undoes the exposure effect, limited the effective sample size.

6. the choice of analysis
6a. validity of assumptions
