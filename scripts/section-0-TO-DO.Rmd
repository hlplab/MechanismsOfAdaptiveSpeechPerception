# THINGS TO DO
Let's delete to-dos after they have been addressed, and post in Slack what we've done. Thanks.

  * Everyone: 
    * Should we make all parameters either 'confidence' / weight of prior knowledge OR learning rates? 

  * **Xin:**
    * Missing Figure in final section of SI
    * remove figure about similar-accuracy in GD
    * align the panels in Figure 27; change caption
    * double check the axis in Figure 27
    * adjusting the code for section 3 (PR) to use repeated sims for the decision-making mode
    * simplify function get_parameters_phonetic_contrast.AA
    

    
  * **Chigusa:**

  * Florian: 
    * CHECK OUT WHETHER CHANGES IN NORMALIZATION CAN LEAD TO NON-LINEAR CHANGES *DESPITE THE FACT THAT WE'RE USING SIMPLE LINEAR INTERPOLATION*

## TO DOs identified in lab mtg

  * In section 2: 
    * Think about a version of Figure 4 that signals how parameters incrementally change with exposure.
  * Eng of section 2: 
  * Can we be clearer that even if one agrees on the three mechanisms, we need to figure out how these mechanisms work. But how do we communicate this? Essentially, it became abundantly clear to us that even minute details can make a big difference in the predictions a model ends up making, so that it’s important to actually implement models. The difference between theory and model. Could go to end of intro or beginning of section 2.
  * When we get to general discussion, be perhaps even clearer what brain-imaging cannot answer. Not only what computations but also that activation evidence has to be interpreted with caution. (perhaps also bring up that imaging doesn’t compare competing accounts of the same type: e.g. no normalization comparison. Why? Too noisy)

 * Send paper out to emily myers and james mcqueen and ask whether they’d be willing to read this in their lab?

## Things to perhaps do later:

  * Florian
    * make a wrapper for categorization function (through optional argument) that makes the functions monotonic. do the math.

  * Move the part about ASP not being a new model to the top of the intro. It may help to signal that we’re aiming to summarize what is known. (Seth)
  * (Shawn) should you highlight “how far can you push normalization before it becomes changes in representations”? (I’m inclined not to get into these nuances in this paper).
