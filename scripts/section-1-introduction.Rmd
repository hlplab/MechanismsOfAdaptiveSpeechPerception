```{r, include=FALSE}
max.f0 <- 350            # max raw f0
min.observation.n <- 25  # min observation per stop
max.p <- .1              # cut-off for rejection of unimodality

d.chodroff_wilson <- read_csv(get_path("../data/Chodroff-Wilson-2018/all_observations_with_non-missing_vot_cog_f0.csv")) %>%
  rename(category = stop, VOT = vot, f0 = usef0, Talker = subj, Word = word, Trial = trial, Vowel = vowel) %>%
  mutate(
    category = 
      plyr::mapvalues(
        category,
        c("B", "D", "G", "P", "T", "K"),
        c("/b/", "/d/", "/g/", "/p/", "/t/", "/k/")),
    gender = factor(
      plyr::mapvalues(
        gender, 
        c("F", "M"),
        c("female", "male")),
      levels = c("male", "female")),
    poa = factor(
      plyr::mapvalues(
        poa, 
        c("lab", "cor", "dor"),
        c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
      levels = c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
    voicing = factor(
      ifelse(category %in% c("/b/", "/d/", "/g/"), "yes", "no"),
      levels = c("yes", "no"))) %>%
  mutate(across(c(Talker, Word, gender, category), factor)) %>%
  select(Talker, Word, Trial, Vowel, gender, category, poa, voicing, VOT, f0)

# Filter f0 above 500 Hz
d.chodroff_wilson %<>%
  filter(f0 < max.f0)

# Keep only subjects with at last n.min observations for each stop
d.chodroff_wilson %<>%
  group_by(Talker, category) %>%
  mutate(n = length(category)) %>%
  group_by(Talker) %>%
  mutate(n = ifelse(any(is.na(n)), 0, min(n))) %>%
  ungroup() %>%
  filter(n > min.observation.n)

# Identify and remove talkers with bimodal f0 distributions
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(f0_Mel = phonR::normMel(f0)) %>%
  group_by(Talker, category) %>%
  mutate(
    f0.multimodal = dip.test(f0)$p.value < .1,
    f0_Mel.multimodal = dip.test(f0_Mel)$p.value < .1) %>%
  filter(!f0.multimodal, !f0.multimodal) %>%
  droplevels()

# Get Mel and Semitones, then C-CuRE
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(
    f0_Mel = phonR::normMel(f0),
    f0_semitones = 12 * log(f0 / mean(f0)) / log(2)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .)))
```

# Introduction
Listeners tend to understand speech from talkers with the same language background without conscious effort. Only when talkers' pronunciations strongly deviate from listeners' expectations, does the challenge of mapping the acoustic input to linguistic categories and meaning become apparent. This might occur, for example, when listening to talkers with strong regional or non-native accents, or patients with apraxia or dysarthria. The same type of problem is, however, present even during seemingly effortless comprehension: even for talkers who share similar language backgrounds, the mapping between the acoustic input and linguistic categories can vary substantially between talkers due to physiological (e.g., vocal tract size and shape) and socio-cultural factors (e.g., social identity). As a consequence, one talker’s pronunciation of, say, the sound category [s] (as in *sip*) can be acoustically more similar to another talker’s production of [`r linguisticsdown::cond_cmpl("ʃ")`] [as in *ship*, Newman et al., -@newman2001]. How we manage to typically understand each other despite such cross-talker differences has remained one of the perennial puzzle in research on speech perception [where it is known as part of the infamous *lack of invariance* problem, @liberman1967]. In the present project, we outline a general computational framework that can help address this question, by---for the first time---implementing competing hypotheses about the mechanisms underlying listeners' ability to accommodate cross-talker variability. We do not, at this stage, test the framework against novel experiments but rather aim to demonstrate why such a framework is critical in moving the field forward, how it works, and what it can offer. But first some background.

Research over the past decades has identified adaptive changes in speech perception to be a key component in listeners' ability to overcome cross-talker differences. Although speech perception can initially be slower and/or less accurate when listeners encounter an unfamiliar talkers with unexpected pronunciations, these processing difficulties tend to reduce with exposure [e.g., @bradlow-bent2008; @nygaard1994; @sidaras2009;@wade2007; @weil2001a; @xie2021jep]. Remarkably, substantial improvements can occur within minutes or less [@clarke-garrett2004; @munro-derwing1995]. Even a context as brief as a single utterance can be sufficient to change perception and segmentation of ambiguous speech [e.g., categorizing a Dutch ambiguous /m?t/ embedded in an utterance with fast or slow speech as either "mat" or "maat" Bosker et al.,-@bosker2017]. <!--[see also @kluender1988; @newman-sawusch1996; @sawusch-newman2000; @sjerps2011]. <!-- TO DO: recognize that this blurs the line between processing and learning --> Understanding these adaptive abilities has led to the formation of many new lines of research, each employing slightly different methodologies, terminology, and theoretical assumptions [for reviews, see @baeseberk2020; @johnson-sjerps2021; @quam-creel2021; @samuel-kraljic2009; @stilp2020; @weatherholtz-jaeger2016]. We illustrate this briefly for two types of paradigms that have been particularly influential, and that are directly relevant for the remainder of this article. 

The first paradigm---variably known as perceptual or phonetic learning, retuning, or recalibration---exposes listeners to artificially manipulated native (L1) pronunciations. In a seminal study, Norris et al. [-@norris2003] exposed Dutch listeners to 20 words that contained an [f] (e.g., "witlo*f*"---chicory) and 20 words that contained an [s] (e.g., "naaldbo*s*"---pine forest), mixed with 160 fillers that did not contain either sound. Between participants, either all instances of [f] *or* all instances of [s] were phonetically manipulated so as to make them perceptually ambiguous between [f] and [s]. The other sound remained unchanged. A third, baseline, condition left both [f] and [s] unchanged. During a subsequent test phase, participants heard instances of an acoustic continuum ranging from clear [`r linguisticsdown::cond_cmpl("ɛ")`f] to clear [`r linguisticsdown::cond_cmpl("ɛ")`s], and responded whether they heard "ef" or "es". <!-- These [`r linguisticsdown::cond_cmpl("ɛ")`f]-[`r linguisticsdown::cond_cmpl("ɛ")`s] stimuli were created using the same acoustic manipulation that was used to make the perceptually ambiguous exposure words.--> Norris and colleagues found that the two groups of participants exhibited strikingly different categorization responses during test (see Figure \@ref(fig:norris-2003)): compared to participants in the baseline condition, participants who were exposed to shifted [f] and typical [s] shifted their categorization boundary (the step along the acoustic [`r linguisticsdown::cond_cmpl("ɛ")`f]–[`r linguisticsdown::cond_cmpl("ɛ")`s] continuum t at which "ef" and "es" responses are equally likely) away from [`r linguisticsdown::cond_cmpl("ɛ")`f] towards [`r linguisticsdown::cond_cmpl("ɛ")`s], so that the participants now categorized more tokens along the [`r linguisticsdown::cond_cmpl("ɛ")`f]–[`r linguisticsdown::cond_cmpl("ɛ")`s] continuum as "ef"; participants who had heard typical [f] and shifted [s] did the opposite, shifting their categorization boundary away from [`r linguisticsdown::cond_cmpl("ɛ")`s] towards [`r linguisticsdown::cond_cmpl("ɛ")`f], categorizing more tokens as "es". These changes after exposure to just 20 shifted word recordings were substantial: e.g., the exact same [`r linguisticsdown::cond_cmpl("ɛ")`f]–[`r linguisticsdown::cond_cmpl("ɛ")`s] recording was categorized as "ef" more than 80% of the time by participants in the [f]-shifted condition but less than 40% of the time by participants in the [s]-shifted condition. This finding has been highly influential, with a recent review citing over 200 studies using variants of this paradigm [@theodore2021]. Boundary shifts like those observed by Norris and colleagues have been demonstrated for an increasing number of phonetic contrasts and languages [e.g., @hanulikova-weber2012; @kraljic-samuel2005; @kraljic-samuel2006; @reinisch2013; @sumner2011; @vroomen2007]. We now know that boundary shifts can occur even under attentional load or distraction [@baart-vroomen2010; @zhang-samuel2014; but see @samuel2016], even when the sounds are embedded in connected speech rather than isolated words [@eisner-mcqueen2006] or in globally accented speech [@reinisch-holt2013], after even fewer shifted tokens [e.g., as few as four, @liu-jaeger2018; @liu-jaeger2019], and that they can persist over hours and days [@eisner-mcqueen2006; @vroomen-baart2009; @saltzman-myers2021] though they eventually decay [@samuel2021]. More recent work has begun to identify the brain regions involved in perceptual recalibration, which range from auditory and superior temporal cortices [@kilianhutten2011; @bonte2017; @myers-mesite2014; @ullas2020], to more frontal and parietal areas [@myers-mesite2014; @ullas2020; @luthra2020; for review, see @guediche2014].

\begin{figure}[h]
\begin{center}
\includegraphics[width=.7\columnwidth]{`r get_path("../figures/diagrams/schertzclare.png")`}
\caption{... PLACEGHOLDER ...}\label{fig:norris-2003}
\end{center}
\end{figure}

A second important paradigm exposes listeners to unfamiliar *naturally* accented speech---e.g., dialectal [@smith2014], varietal [@shaw2018], or second language (L2) accents [@bradlow-bent2008; @eisner2013; @sidaras2009; @weil2001a]---compared to a control condition in which listeners are exposed to a familiar accent (typically a 'standard' variety of listeners' L1). Following exposure, listeners in either group are tested for improved comprehension of the unfamiliar accent. In an influential study, @bradlow-bent2008 had listeners transcribe a total of 160 sentences of either Mandarin-accented English or L1-accented English, distributed over two sessions on two separate days. In a subsequent test phase, both groups transcribed Mandarin-accented sentences. Participants who were first exposed to Mandarin-accented English were significantly more accurate during test (over 90% accuracy compared to about 80%). This finding, too, has since been replicated and extended [for review, see @baeseberk2020]. We now know that substantially shorter exposure can lead to similarly large improvements in accuracy [e.g., 80 sentences in a single session, about 2-5 minutes of speech, @xie2021jep], that these can persist over hours and days [@witteman2015; @xie2018lcn], and that accent adaptation can sometimes generalize across talkers of the same or similar accents [e.g., @baeseberk2013; @tzeng2016; @xie2021jep]. Facilitatory effects of exposure have also been demonstrated in paradigms that tap into online language processing of naturally accented speech, including cross-modal forced-choice tasks [@clarke-garrett2004; @xie2018jasa], phonological priming [@eisner2013; @xie2017], and visual world eye-tracking [@dahan2008; @hanulikova-weber2012; @trude2012talker]. 

In short, there is now a rich literature on the effects of recent exposure on subsequent perception of speech from the same talker.^[Additional lines of work have investigated exposure to synthesized, vocoded, or otherwise distorted speech [e.g., @adank2009; @davis2005; @fenn2003; @mattys2012] or have used distributional learning paradigms [e.g., @clare2019; @clayards2008; @idemaru-holt2011; @maye2008; @schertz2013; @theodore-monto2019; @wade2007]. In the general discussion, we return to distributional learning paradigms, which we believe to hold particular promise in addressing the issues we identify below.] Listeners' ability to adapt based on recent input is now considered a central part of human speech perception, and acknowledged by all major theories of speech perception. Despite these substantial advances, however, it remains unclear *how*---through what mechanisms---exposure comes to facilitate comprehension. Different explanations continue to coexist across different lines of work, each with qualitatively different implications for the cognitive and neural architectures underlying speech perception. Recent reviews highlight this empirical and theoretical indeterminacy as an important unresolved gap in our understanding of speech perception [e.g., @bent-baeseberk2021; @schertz-clare2020; @weatherholtz-jaeger2016; @zheng-samuel2020]. 

This motivates the present study. We introduce a general computational framework that implements competing hypotheses about the effects of recent exposure on subsequent speech perception. Our long-term goal is to contribute to the development of stronger theories that make more specific predictions, facilitating more decisive tests [cf. @platt1964]. Here, we use this framework to demonstrate that the huge majority of existing results do *not* distinguish between three different explanations that have radically different implications for theories of speech perception. To us, and we expect to others in the field, the degree to which this is the case was surprising. Different lines of research---such as work on perceptual recalibration and accent adaptation---seem to have converged on different theoretical assumptions through convention, rather than because of strong evidence in favor of those views (our own work included). 

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\columnwidth]{`r get_path("../figures/diagrams/overview-of-three-mechanisms2.png")`}
\caption{Listeners' recognition of speech categories are typically assumed to involve at least three types of mechanisms: 1) the acoustic input is transformed by low-level normalization processes into the perceptual cues that form the input to categorization, 2) linguistic representations describe the mapping between these perceptual and linguistic categories, and 3) decision-making mechanisms allow additional, stimulus-independent, biases to affect recognition. Any of these three mechanisms can in theory be affected by recent experience. While it is now clear \emph{that} recent experience changes the processing of subsequent speech input, most existing findings leave open \emph{which of the three mechanisms underlie these changes.}}\label{fig:overview}
\end{center}
\end{figure}

Figure \@ref(fig:overview) summarizes the three types of mechanisms we consider, each of which can, in theory, explain why and how recent experience affects speech perception. Research on perceptual recalibration and accent adaptation <!--[e.g., @REFS]--> has focused on the middle layer, changes in linguistic representations. This includes proposals that attribute exposure effects to "boundary re-tuning/shifts” [e.g., @norris2003; @reinisch2011], “perceptual/category recalibration” [e.g., @kraljic-samuel2006; @reinisch-holt2014; @samuel2016; @vroomen-baart2009], "perceptual retuning" [@jesse-mcqueen2011; @mcqueen2006; @mitterer2013], “category expansion" [@schmale2012], “dimension-based statistical learning” [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015], or "criteria relaxation" [@zheng-samuel2020]. While these proposals are often not further formally specified [or modeled; for notable exceptions, see @apfelbaum-mcmurray2015; @clayards2008; @hitczenko-feldman2016; @kleinschmidt-jaeger2015; @lancia-winter2013; @theodore-monto2019; @xie2021cognition], all of them seem to describe types of changes in representations. For example, "category shift" refers to a change in the mean of the cue distribution corresponding to a category, and "category expansion" refers to increases in the variance of that distribution. Either of these changes, along with "cue re-weighting" or "learning of new cues", can be understood as a consequence of the type of distributional learning that is hypothesized in exemplar [@apfelbaum-mcmurray2015; @johnson2006], episodic [@goldinger1998], Bayesian inference [@kleinschmidt-jaeger2015], or neural network models [@lancia-winter2013]. Indeed, recent reviews discuss *what types* of representational changes underlie the effects of recent exposure (e.g., "category expansion" vs. "category shifts"), rather than *whether* representational changes are the actual mechanism underlying the observed results [e.g., @baeseberk2020; @bent-baeseberk2021; @schertz-clare2020]. In our own work, we have sometimes made similar assumptions---e.g., when asking whether representational changes can account for adaptation without considering alternative explanations [@kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012; @kleinschmidt-jaeger2016cogsci; @kurumada2017; @tan2021].

<!--TO DO: INTEGRATE IN  NEXT PARA: “normalization” [@johnson1990; @johnson-sjerps2021; @pisoni1977],-->

There are, however, alternative explanations that have been pointed out in separate lines of work. One possibility is that *pre-linguistic* mechanisms underlie the effects (bottom of Figure \@ref(fig:overview)). Low-level, automatic (involuntary) normalization processes might transform the speech input during the early stages of auditory processing [e.g., @kluender1988; @johnson1990;@pisoni1977;@newman-sawusch1996; @sawusch-newman2000; @sjerps2011; @uddin2020; for discussion, see @barreda2012; @magnuson-nusbaum2007]. These processes are thought to be pre-linguistic in that they do not refer to categories but rather apply to the acoustic or phonetic cues [@johnson-sjerps2021; @stilp2020]. *That* such processes are part of speech perception is widely assumed [@irino-patterson2014]. Their exact contributions to overcoming the lack of invariance problem, however, remain a matter of debate. In particular, normalization is often *not* considered when researchers interpret the results of experiments on accent adaptation, perceptual recalibration, or distributional learning [but see @mullennix-pisoni1990; @sjerps-reinisch2015]. And while normalization procedures have been compared to each other [e.g., @adank2004; @hoffmanbion-escudero2007; @kiefte-nearey2019] or against the absence of normalization [@mcmurray-jongman2011], they are rarely compared to the competing hypothesis that changes in representations underlie the effects of recent exposure [for notable exceptions, see @apfelbaum-mcmurray2015; @lehet-holt2020; @xie2021cognition]. <!-- ^[It is, however, worth pointing out that the exact nature of normalization remains unclear. This includes questions about the specific transformations that are applied [@REFS] but also questions about whether normalization is best viewed as an autonomous, encapsulated process, or as part of a larger hierarchical inference process that includes inferences at both levels traditionally considered linguistic and levels traditionally considered pre-linguistic. Research on automatic speech recognition, for example, has found that deep neural network models can to some extent remove the need for specialized signal transformations [@deng2016]. Under this view, normalization is itself the result of a hierarchical predictive process that seeks to efficiently predict the incoming signal [for related discussion, see @clark2013; @kell2018; @kleinschmidt-jaeger2015; @kuperberg-jaeger2016].] --> 
  
Alternatively, it is possible that *post-linguistic* mechanisms underlie the effects of recent exposure (top of Figure \@ref(fig:overview)). Just like normalization is broadly accepted to be part of speech perception, there is little doubt that changes in response biases can affect listeners' interpretation of speech input, or at least the responses they give within experiments. For instance, Clarke-Davidson et al. [-@clarkedavidson2008, p.605] define a bias as "the increased likelihood to give a particular response---such as /s/---given any acoustic input, or the need for less evidence for a particular response" that "would help participants make faster word decisions in [...] ambiguous cases". Yet, such biases are rarely considered when analyzing the effects of recent exposure. *When* response biases have been considered [e.g., as "response equilibration" in Vroomen and Baart -@vroomen-baart2009], this explanation tends to be dismissed---prematurely, we believe. One reason for this might be particular result patterns---such as boundary shifts in perceptual recalibration or improved categorization or transcription accuracy in accent adaptation---are considered unlikely to be explained by changes in response biases (or normalization, for that matter). It is often assumed, for example, that response biases can only change the relative proportion of answers but cannot explain overall improvements in accuracy. Under this assumption, overall improvements in the perception of an accent would seem to rule out explanations in terms of response biases, at least if the experimenter carefully balanced how often each category occurs during test. This is, however, a common misconception (one that we shared, for what it is worth). As we demonstrate below, changes in response biases can explain findings that are often attributed to changes in representations. The same applies to pre-linguistic normalization: contrary to common intuitions, simple normalization mechanisms can explain seemingly complex changes in listeners' categorization following exposure to L2-accented or otherwise shifted speech (as, e.g., in perceptual recalibration paradigms). 

Yet, which of the three mechanisms in Figure \@ref(fig:overview) underlie changes in speech perception has far reaching consequences for theories of speech perception. It is of theoretical relevance whether listeners can indeed learn and maintain new *representations*---mappings from acoustic or phonetic cues to linguistic categories---for new talkers or types of talkers. The assumption that they can is central to theories of speech perception [e.g., @hay2019; @luthra2020; @kleinschmidt-jaeger2015; @sjerps2019]. Evidence supporting this possibility has helped shape linguistic theories [e.g., @bradlow-bent2008; @baeseberk2013; @goldinger-azuma2004; @hay2019; @magnuson-nusbaum2007; @tzeng2016] as well as theories about the interface between social and linguistic cognition [e.g., @babel2019; @creel-bregman2011; @foulkes-hay2015; @hanulikova2012; @sumner2014]. Even if one were to take for granted that the three mechanisms in Figure \@ref(fig:overview) *jointly* underlie the effects of recent exposure, it remains unclear which of those mechanisms any given results sheds light on. For example, can we conclude from experiments like @clarke-garrett2004 that listeners can learn new representations (or at least select some mixture of existing representations) within two minutes of exposure? Or are those results due to normalization, making them substantially less thought-provoking? Do the results of two different paradigms originate in the same mechanism(s), or do they reflect different mechanisms? The two paradigms discussed above---perceptual recalibration and accent adaptation---both expose different groups of listeners to different types of unfamiliar speech. The two paradigms differ, however, substantially in the types of stimuli and tasks they employ, including difference in the complexity and ecological validity of the speech stimuli. This raises questions about whether the changes observed in the two paradigms originate in the same mechanisms [see also @baeseberk2018; @samuel-kraljic2009; @zheng-samuel2020]. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=.99\columnwidth]{`r get_path("../figures/diagrams/overview-of-changes.png")`}
\caption{Overview of our approach. Experiments on the effect of recent exposure on subsequent speech perception tend to involve two phases. An exposure phase manipulates the statistics of speech input between participants. A subsequent test phase assesses the effects of those manipulations on the interpretation of identical speech input. We seek to understand what type of exposure effects each of the three three mechanisms in Figure \ref{fig:overview} can explain. To this end, we specify both a) a {\em categorization model} that describes the interpretation of speech input at any given moment (vertical information flow) and b) {\em change models} for all three mechanism that describe how these parts of the categorization model change as a function of exposure (horizontal information flow). We then use the different change models to compare the predicted consequences of changes to normalization, representations, or response biases.}\label{fig:overview-change}
\end{center}
\end{figure}

The approach we take in this article is illustrated in Figure \@ref(fig:overview-change). We introduce a general computational framework that encompasses the three levels of perceptual decision-making shown in Figure \@ref(fig:overview). We extend this framework to include *change models* describing how normalization, representations, and responses biases may change with exposure. These extensions, we hope, can also contribute to synthesis between behavioral and neuroimaging research on speech perception [see discussion in @guediche2014, p. 8]. For example, neuroimaging research has linked prefrontal areas [e.g., @binder2004neural; @thompson1997role] as well as the insula and parietal cortex [e.g., @furl2011parietal; @keuken2014] to decision-making, and recent studies have begun to investigate whether adaptive changes in speech perception primarily implicate these areas [@myers-mesite2014; @erb2013brain] or cortical areas associated with phonetic representations [@bonte2017; @luthra2020]. Other lines of research have focused on pre-linguistic signal transformations [@sjerps2011listening; @zhang2016functionally] and sensory adaptation [@guediche2015evidence], including sub-cortical structures [e.g., the brain stem, @skoe2021auditory; and cerebellum, @guediche2014].
<!-- By developing a computational framework that at least acknowledges, and integrates these aspects of speech perception, we take on (small) step towards addressing the challenge identified by @guediche2014: "there is no formal speech perception model that relates activity in the cortical regions identified via neuroimaging to the computational demands of adaptive plasticity in speech perception. Conversely, the classic computational models of speech perception that have attempted to differentiate how the system may meet the computational demands of adaptive plasticity have not made specific predictions of the underlying neural mechanisms".  -->

Here we use this framework to examine whether changes in normalization, representations, or responses biases can explain the signature results of experiments on perceptual recalibration (Case Study 1) and accent adaptation (Case Study 2). <!-- , and (c) unsupervised learning.  -->
These signature results differ across the two paradigms, reflecting differences in the way that data from the two paradigms tend to be analyzed. For perceptual recalibration, researchers tend to analyze the shift in the categorization boundary. For accent adaptation, researchers often analyze improvements in overall accuracy [e.g., @bradlow-bent2008; @sidaras2009] or improvements in the accuracy of specific categories [e.g., @xie2017; @wade2007]. We test whether both types of results can be derived from the same change model(s).

<!-- The computational framework we present aims to capture the central tenets of existing normalization and representational accounts, combining them with a psychometric model that is standard in many parts of the cognitive sciences [e.g., in vision, @REFS] but remains under-utilized in research on speech perception [but see e.g., @clayards2008; @kleinschmidt-jaeger2016cogsci; @REF-lapse]. Our primary goal here is to demonstrate how even such a general model can move the field closer to the ideal of *strong inference*---devising crucial experiments that directly contrast competing hypotheses with the goal of finding the most decisive evidence [@platt1964]. The same framework can, however, also be used to contrast competing representational accounts, and we point to examples of that in some of the studies below. -->

<!--To anticipate, the computational studies we report suggest that the field should change the way we design, analyze, and report our experiments.--> 
To anticipate, the computational studies we report suggest that a deeper understanding of speech perception mechanisms would require new ways to design, analyze, and report our experiments. We find that the signature results of accent adaptation or perceptual recalibration paradigms can be derived from any of the three mechanisms in Figure \@ref(fig:overview). This does not, however, mean that the three theories make identical predictions---they do not. Rather, it is present-day conventions in design and analysis that limit what can be inferred from existing findings. In the general discussion, we recommend changes to these standards. 

All data and code for this article can be downloaded from OSF at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). This article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [R, @R; @RStudio], while changing any of the parameters of our models. Readers can revisit any of the assumptions we make---for example, by substituting alternative models of linguistic representations. The supplementary information (SI, \@ref(sec:SI-software)) lists the software/libraries required to compile this document. Beyond our immediate goals here, we hope that this can be helpful to researchers who are interested in developing more informative experimental designs, and to facilitate the interpretation of existing results [see also @tan2021]. 
