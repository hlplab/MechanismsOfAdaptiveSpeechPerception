\newpage 
<!-- \setcounter{page}{1} -->

```{r, include=FALSE}
max.f0 <- 350            # max raw f0
min.observation.n <- 25  # min observation per stop
max.p <- .1              # cut-off for rejection of unimodality

d.chodroff_wilson <- read_csv(get_path("../data/Chodroff-Wilson-2018/all_observations_with_non-missing_vot_cog_f0.csv")) %>%
  rename(category = stop, VOT = vot, f0 = usef0, Talker = subj, Word = word, Trial = trial, Vowel = vowel) %>%
  mutate(
    category = 
      plyr::mapvalues(
        category,
        c("B", "D", "G", "P", "T", "K"),
        c("/b/", "/d/", "/g/", "/p/", "/t/", "/k/")),
    gender = factor(
      plyr::mapvalues(
        gender, 
        c("F", "M"),
        c("female", "male")),
      levels = c("male", "female")),
    poa = factor(
      plyr::mapvalues(
        poa, 
        c("lab", "cor", "dor"),
        c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
      levels = c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
    voicing = factor(
      ifelse(category %in% c("/b/", "/d/", "/g/"), "yes", "no"),
      levels = c("yes", "no"))) %>%
  mutate(across(c(Talker, Word, gender, category), factor)) %>%
  select(Talker, Word, Trial, Vowel, gender, category, poa, voicing, VOT, f0)

# Filter f0 above 500 Hz
d.chodroff_wilson %<>%
  filter(f0 < max.f0)

# Keep only subjects with at last n.min observations for each stop
d.chodroff_wilson %<>%
  group_by(Talker, category) %>%
  mutate(n = length(category)) %>%
  group_by(Talker) %>%
  mutate(n = ifelse(any(is.na(n)), 0, min(n))) %>%
  ungroup() %>%
  filter(n > min.observation.n)

# Identify and remove talkers with bimodal f0 distributions
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(f0_Mel = phonR::normMel(f0)) %>%
  group_by(Talker, category) %>%
  mutate(
    f0.multimodal = dip.test(f0)$p.value < .1,
    f0_Mel.multimodal = dip.test(f0_Mel)$p.value < .1) %>%
  filter(!f0.multimodal, !f0.multimodal) %>%
  droplevels()

# Get Mel and Semitones, then C-CuRE
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(
    f0_Mel = phonR::normMel(f0),
    f0_semitones = 12 * log(f0 / mean(f0)) / log(2)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .)))
```

# Introduction
How human listeners understand speech is a central question in the cognitive and neurosciences. The computational complexity of spoken language understanding becomes most apparent when a talker's pronunciations---and thus the mapping of acoustic input to linguistic categories and meaning---strongly deviate from listeners' expectations. This might occur, for example, when listening to talkers with unfamiliar regional or non-native accents, or patients with apraxia or dysarthria. The same fundamental challenge is, however, present even during seemingly effortless comprehension: even among talkers who share similar language backgrounds, the mapping between the acoustic input and linguistic categories can vary substantially between talkers due to both physiology (e.g., vocal tract size and shape) and socio-cultural factors (e.g., social identity and language background). As a consequence, one talker’s pronunciation of, say, the sound category [s] (as in *sip*) can be acoustically more similar to another talker’s production of [`r linguisticsdown::cond_cmpl("ʃ")`] [as in *ship*, Newman et al., -@newman2001]. How we manage to typically understand each other despite such cross-talker differences has remained one of the perennial puzzles in research on speech perception [where it is known as part of the infamous *lack of invariance* problem, @liberman1967]. 

This article presents a general computational framework that helps address this question. The framework---which we abbreviate as ASP for *adaptive speech perception*---is the first to formalize and implement three major hypotheses about the mechanisms that allow listeners to overcome cross-talker variability. These three hypotheses make fundamentally different assumptions about the cognitive abilities and neural architectures underlying speech perception, and have far-reaching consequences for theories of speech perception, linguistics, and the malleability of neural representations more generally. These radically different theoretical perspectives have co-existed for several decades, often in separate lines of research and largely in the absence of efforts to directly contrast their empirical adequacy. ASP makes possible well-formed ('apples-to-apples') comparisons between these alternatives. This supports our long-term goal  to contribute to the development of stronger theories that facilitate decisive comparisons between alternative hypotheses about the mechanisms underlying speech perception [in the tradition of "strong inference" approaches to scientific inquiry, @platt1964]. Frameworks like ASP can be used predictively, for example, to inform and revise researchers' intuitions about accounts that otherwise leave too many degrees of freedom to deliver strong predictions, or to guide experimental design, leading to more decisive experiments. ASP can also be used to quantitatively compare the fit of alternative hypotheses about adaptive speech perception against human behavior. This provides an objective way to assess which (combinations of) mechanisms best explains researchers' observations. As such it will help clarify whether impaired adaptation (as seen in chidlren with dyslexia e.g., @gabay2021;@ozernov-palchik2021) arises from auditory, linguistic, or cognitive sources---an important prerequisite for future developments of more effective interventions/treatments. 

For the present article, our three immediate goals are more modest. The first part of this article introduces the general ASP framework, along with visual demonstrations and animations. We take a deliberately tutorial-like approach in order to make the framework accessible to any researcher interested in applying it to their experiments. All data and code for this article can be downloaded from the Open Science Framework at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using the freely available software [@R; @RStudio, see supplementary information for detailed software requirements]. Readers can also revisit any of the assumptions we make---e.g., changing parameterizations of our models, or substituting alternative models [see also @tan2021]. We welcome questions about the code, including the accompanying \texttt{R} library, and hope that it can support the development of experimental designs that decisively distinguish between competing mechanisms, and to facilitate the interpretation of existing results. 

Second, we present two case studies to demonstrate *why* computational frameworks like ASP are needed to move the field forward: we use ASP-based simulations to show that influential and highly-replicated signature findings that have been interpreted as evidence for one of the three hypotheses about adaptive speech perception are, in fact, qualitatively just as compatible with *any* of the hypothesized mechanisms. The degree to which existing findings do not distinguish between competing theories surprised us, and we suspect that we are not alone in this. This second part of the article re-affirms the need identified in recent reviews to better characterize the mechanisms underlying adaptive speech perception [e.g., @baeseberk2020; @kurumada-roettger2021; @johnson-sjerps2021; @quam-creel2021; @stilp2020; @weatherholtz-jaeger2016]. 

Finally, we show that alternative hypotheses about adaptive speech perception *can* be distinguished, provided adequate experimental designs and analyses that are explicit about the linking hypotheses from acoustic properties of the stimulus to participants' responses. While detailed comparisons of specific design proposals are beyond the scope of this article, we close with general recommendations for future experiments. But first some background.

## The state of the field(s)
Research over the past decades has identified adaptive changes in speech perception to be a key component in listeners' ability to overcome cross-talker differences. Although speech perception can initially be slower and/or less accurate when listeners encounter an unfamiliar talkers with unexpected pronunciations, these processing difficulties tend to reduce with exposure [e.g., @bradlow-bent2008; @nygaard1994; @Perrachione2016; @sidaras2009; @wade2007; @weil2001a; @xie2021jep]. Remarkably, substantial improvements---such as reduced processing times or increased accuracy of recognition---can occur within minutes or less [@clarke-garrett2004; @munro-derwing1995; @xie2018jasa]. Even a context as brief as a single utterance can be sufficient to change perception and segmentation of ambiguous speech, blurring the distinction between processing and adaptation [e.g., categorizing a Dutch ambiguous /m?t/ embedded in an utterance with fast or slow speech as either "mat" or "maat" Bosker et al.,-@bosker2017; see also @kluender1988; @newman-sawusch1996; @sawusch-newman2000; @sjerps2011]. <!-- TO-DO: The rest of the para could potentially go? --> Changes in speech perception as a function of recent exposure have now been documented across a broad range of experimental paradigms and tasks [e.g., @bradlow-bent2008; @clayards2008; @eisner2013; @idemaru-holt2011; @norris2003; @vroomen2007], different modes of speech [e.g., "clear" and "conversational" speech, @zhang-samuel2014]; isolated and connected speech, [e.g., @eisner-mcqueen2005; @reinisch-holt2014], and languages [e.g., @chladkova2017; @eisner2013; @hanulikova-weber2012; @sebastian-galles2000; @schertz2015]. Changes can occur even under cognitive load, arguing for a relatively high degree of automaticity of the underlying mechanisms [@zhang-samuel2014; though attentional resources are required for at least some types of changes, @samuel2016]. Yet, even adaptive changes after only a few words of exposure can sometimes last for days if not longer [@eisner-mcqueen2006; @samuel2021; @xie2016; see also @goldinger1996]. 

In short, listeners' ability to adapt based on recent input is now considered a central part of human speech perception, and research over the last few decades has made tremendous progress in identifying its empirical properties and constraints [for comprehensive reviews, see @baeseberk2020; @johnson-sjerps2021; @tzeng2021; @quam-creel2021; @stilp2020]. Despite these substantial advances, however, **it remains unclear *how*---through what mechanisms---recent exposure comes to facilitate spoken language understanding.** This is not for a lack of theoretical proposals. Across the cognitive and neurosciences, there are now dozens of competing perspectives. However, contrastive comparisons between proposals have largely been lacking. <!-- Somewhat curiously, it is precisely comparisons between proposals that differ the most in their theoretical assumptions and consequences that have been absent.--> Two inter-related factors seem to contribute to this: (1) many proposals---including some of our own past work---remain under-developed, constituting informal hypotheses rather than theories or models that lend themselves to strong contrastive tests,<!--TO-DO: this footnote could go?-->^[This has consequences beyond the ones we focus on here. Often multiple labels exist that seem to refer to the same general idea, without attempts to consolidate terminology (e.g., "perceptual recalibration", "phonetic retuning", and related combinations of these terms; "statistical learning" and "distributional learning"). It is also not uncommon that terminological usage seems to blur the distinction between *mechanism* and types of *paradigms* or the *phenomenon* that this paradigm is thought to study (e.g., "dimension-based statistical learning", "perceptual recalibration"). Yet other proposals seem to *describe effects* rather than mechanisms (e.g., "boundary shift") but what type of effect is observed (a boundary shift vs. an increase in accuracy) primarily reflects the researchers' choice of paradigm and analysis, without necessarily being a characteristic property of the underlying mechanisms. In our experience, this lack of clear distinctions between mechanisms and properties of experiments can confound newcomers to the field.] and (2) a tendency, in particular in behavioral research, to focus on identifying properties of adaptive speech perception rather than on distinguishing the underlying cognitive and neural architectures [but see @apfelbaum-mcmurray2015; @chodroff-wilson2020; @kleinschmidt-jaeger2015; @lehet-holt2020; @mcmurray-jongman2011; @xie2021cognition; @adank2004; @hoffmanbion-escudero2007; @kiefte-nearey2019]. As a consequence, separate lines of research often pursue, or simply *assume*, one hypothesis at a time. Reviews of the field are then left with the question whether findings from different paradigms are due to the same underlying mechanisms, and what those mechanisms are [see e.g., @baeseberk2020; @samuel-kraljic2009; @weatherholtz-jaeger2016; @zheng-samuel2020].

The framework we present in Section \@ref(sec:framework) (ASP) is designed to help address these questions. We submit that existing proposals for adaptive speech perception can be grouped into three types of theoretical perspectives (Figure \@ref(fig:overview)). The three proposals share with each other, and with all major theories in speech perception, general assumptions about how the acoustic input supports perception of a speech category. The process begins with (A) the extraction and normalization of acoustic/phonetic cues. These cues are (B) mapped onto and activate linguistic categories (such as phonemes, syllables, and/or words). Finally, (C) decision processes integrate the resulting category activations with contextual support and/or meta-reasoning (e.g., about the task), and recognition takes place. Existing proposals differ, however, in which of (A)-(C) explains *changes* in perception as a function of recent exposure. It is these differences that entail fundamentally different cognitive and neural architectures.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\columnwidth]{`r get_path("../figures/diagrams/overview-of-three-mechanisms.png")`}
\caption{Listeners' recognition of speech categories are typically assumed to involve at least three types of mechanisms: 1) the acoustic input is transformed by low-level normalization processes into the perceptual cues that form the input to categorization, 2) linguistic representations describe the mapping between these perceptual and linguistic categories (here $C_1$ and $C_2$), and 3) decision-making mechanisms allow additional, stimulus-independent, biases to affect recognition. Any of these three mechanisms can in theory be affected by recent experience. While it is now clear \emph{that} recent experience changes the processing of subsequent speech input, most existing findings leave open \emph{which (combination) of the three mechanisms underlie these changes} and \emph{how the involvement of different mechanisms depends on, for instance, stimulus properties, task demands, and individual differences between listeners}.}\label{fig:overview}
\end{center}
\end{figure}

The majority of recent behavioral research on talker-related adaptation has focused on the middle layer, **changes in linguistic representations**. This includes proposals that attribute exposure effects to "boundary re-tuning/shift” [e.g., @norris2003; @reinisch2011], “perceptual/category recalibration” [e.g., @kraljic-samuel2006; @reinisch-holt2014; @samuel2016; @vroomen-baart2009], "perceptual retuning" [@jesse-mcqueen2011; @mcqueen2006; @mitterer2013], "category shift" [@]; “category expansion" [@schmale2012], “dimension-based statistical learning” [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015], or "criteria relaxation" [@zheng-samuel2020]. While these proposals are often not further formally specified or modeled [for notable exceptions, see @apfelbaum-mcmurray2015; @clayards2008; @kleinschmidt-jaeger2015; @lancia-winter2013; @xie2021cognition], they all describe types of changes in representations. For example, "category shift" refers to a change in the mean of the cue distribution corresponding to a category, and "category expansion" refers to increases in the variance of that distribution. 

One reason that the idea of representational changes has received so much attention is that it is of high theoretical relevance: representational changes as a function of recent exposure are predicted by influential exemplar [@apfelbaum-mcmurray2015; @johnson2006], episodic [@goldinger1998], Bayesian inference [@kleinschmidt-jaeger2015], or neural network models [@lancia-winter2013]. All of these theories predict that listeners can learn and store talker- or accent-specific representations [see also @bradlow-bent2008; @baeseberk2013;  @tzeng2016]. Perhaps unsurprisingly, this idea---continued implicit learning of new representations throughout their adult life---has been influential beyond speech perception. The possibility that listeners learn and maintain linguistic representations for multiple types of talkers has influenced psycholinguistic research on, for example, lexical, sentence, and semantic/pragmatic processing [@schuster-degen2020; @fine2013; @fraundorf-jaeger2016; @kaschak2004; @pogue2016; @prasad2021; @ryskin2019; @yildirim2016; @wells2009; for review, see @brown-schmidt2015; @chang2012] and research on second language learning [for reviews, see @kaan-chun2018; @pajak2016]. It has shaped linguistic theories [e.g., @bybee2001; @goldinger-azuma2004; @hay2019; @magnuson-nusbaum2007; @pierrehumbert2001] as well as theories about the interface between social and linguistic cognition [e.g., @babel2019; @creel-bregman2011; @foulkes-hay2015; @hanulikova2012; @sumner2014]. Further illustrating the influence of this idea, several recent reviews go as far as to discuss *what types* of representational changes underlie the effects of recent exposure (e.g., "category expansion" vs. "category shifts"), rather than *whether* representational changes are the actual mechanism underlying the observed results [e.g., @baeseberk2020; @bent-baeseberk2021; @schertz-clare2020]. 

<!--TO DO: INTEGRATE IN  NEXT PARA: “normalization” [@johnson1990; @johnson-sjerps2021; @pisoni1977],-->

In the absence of contrastive tests, however, the same behavioral results that have been interpreted as arguing for representational changes could in principle be explained by computationally simpler mechanisms that do *not* assume changes in linguistic representations. This calls into question whether results that have been described in terms of representational changes really provide evidence for the computational complexity and cognitive abilities implied by those explanations. For instance, proposals dating back to at least the 1970s hold that adaptive changes in perception can be due to low-level, automatic (involuntary) **normalization** during the early stages of auditory processing (bottom of Figure \@ref(fig:overview)). More recently, evidence from neuroimaging studies has lent support to this idea by showing engagement of sub-cortical structures in sensory adaptation [e.g., the brain stem, @skoe2021auditory; and cerebellum, @guediche2015evidence; for review, see @guediche2014]. These processes are thought to be pre-linguistic in that they do not refer to categories but rather apply to the acoustic or phonetic cues [for reviews, @johnson-sjerps2021; @stilp2020]. In contrast to the assumption that cross-talker variability may be learned and stored, which is fundamental to all the theories of representational changes as described above, normalization proposals assume that listeners remove talker variability prior to mapping cues to linguistic categories. 

That normalization helps navigate cross-talker variability is widely assumed both in behavioral and neuroimaging research [e.g., @kluender1988; @irino-patterson2014; @johnson1990;@pisoni1977;@newman-sawusch1996; @sawusch-newman2000; @sjerps2011; @uddin2020; for discussion, see @barreda2012; @magnuson-nusbaum2007; @wong2004]. However, direct comparisons between models of normalization and representational changes have largely been lacking. Only very recently, have studies have begun to do so, each investigating different phonetic contrasts and arriving at different conclusions [@apfelbaum-mcmurray2015; @chodroff-wilson2020; @lehet-holt2020; @xie2021cognition].  More commonly, competing normalization models have been compared to each other [e.g., @adank2004; @hoffmanbion-escudero2007; @kiefte-nearey2019] or against the absence of normalization [@mcmurray-jongman2011], just as representational change models have been compared against the absence of representational changes or different types of representational changes [@clayards2008; @hitczenko-feldman2016; @theodore-monto2019; @lancia-winter2013; including our own work @kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012; @kleinschmidt-jaeger2016cogsci; @kurumada2017; @tan2021]. Typically, the two types of accounts are not even considered as a competing explanation when researchers interpret the results of experiments on accent adaptation, perceptual recalibration, or distributional learning [but see @mullennix-pisoni1990; @sjerps-reinisch2015]. 

Another possible explanation for adaptive speech perception that avoids the need for changes in representations are *post-linguistic* mechanisms (top of Figure \@ref(fig:overview)). Just like normalization, this explanations is computationally more parsimonious than changes in representations. And just like normalization is broadly accepted to be part of speech perception, there is little doubt that **changes in decision and response biases** can affect listeners' interpretation of speech input, or at least the responses they give within experiments. For instance, Clarke-Davidson et al. [-@clarkedavidson2008, p.605] define a bias as "the increased likelihood to give a particular response---such as /s/---given any acoustic input, or the need for less evidence for a particular response" that "would help participants make faster word decisions in [...] ambiguous cases". However, in behavioral research, such biases are rarely considered when analyzing the effects of recent exposure. *When* response biases have been considered [e.g., as "response equilibration" in Vroomen and Baart -@vroomen-baart2009], this explanation tends to be dismissed. One reason for this might be particular result patterns---such as boundary shifts in perceptual recalibration or improved categorization or transcription accuracy in accent adaptation---are considered unlikely to be explained by changes in response biases (or normalization, for that matter). However, to anticipate one of the findings of our case studies in Sections \@ref(sec:PR) and \@ref(sec:AA), we find that response biases can explain more complex changes in behavior than has previously been assumed, including the very signature results that are often assumed to be incompatible with response bias.

In contrast to behavioral research, neuroimaging studies have emphasized the possibility that changes in decision-making contribute to adaptive speech perception. Recent studies have investigated whether adaptive changes in speech perception primarily recruit areas associated with decision-making [@myers-mesite2014; @erb2013brain]---such as prefrontal areas [e.g., @binder2004neural; @thompson1997role] as well as the insula and parietal cortex [e.g., @furl2011parietal; @keuken2014]---as opposed to cortical areas associated with phonetic representations [@bonte2017; @luthra2020a]. For instance, @myers-mesite2014 found that sensitivity to category shifts between [s] and [`r linguisticsdown::cond_cmpl("ʃ")`] emerged in right frontal and middle temporal regions, implicating adjustments of decision-related or attentional criteria downstream from primary auditory cortex. Similarly, a recent work found that adaptation to phoneme category substitutions (e.g., [s] pronounced as [`r linguisticsdown::cond_cmpl("ʃ")`]) engages prefrontal regions, responsible for post-perceptual repair mechanisms [@blanco-elorriera2021]: simply put, listeners seem to initially map the acoustic input onto the wrong category and subsequently corrects the mapping. This was contrasted with the hypothesis that adaptation occurs via retuning of the lower-level functional connections that would be associated with perceptual processing (e.g., between STG and primary auditory cortex). 

In short, different lines of research have focused on different types of explanations for adaptive speech perception. Each of these research lines has found that the mechanisms under investigation can qualitatively explain behavioral and/or neural activation patterns. Comparisons between competing hypotheses are, however, still lacking---in particular, in behavioral research. To the extent that behavioral research has begun to contrast competing hypotheses, these efforts have focused on (A) normalization and (B) representational changes while continuing to mostly ignore the possibility of (C) post-perceptual decision-making. Compared to the behavioral research we reviewed, it is more common in neuroimaging work to directly contrast hypotheses about different mechanisms---operationalized as differential activation of different brain areas and networks that are associated with different functionality. However, in contrast to behavioral work, neuroimaging research tends to not distinguish between (A) and (B), grouping both hypotheses together as functionally distinct from higher-level, decision-related mechanisms further downstream (C). 

This empirical and theoretical indeterminacy motivates the present study. An overview of our approach is provided in Figure \@ref(fig:overview-change). We introduce the ASP framework, along with its separate *change models* for normalization, representational changes, and changes in decision-making. By switching these models off and on, we can simulate the effects of recent exposure during an experiment under competing hypotheses about adaptive speech perception. This allows us to assess whether an experimental finding is compatible with any of the changes models (A)-(C), *and* whether that result is evidence for any of the mechanisms over the other mechanisms---i.e., whether some of the mechanisms can*not* explain the result. Using this approach, we present case studies for two highly influential experimental paradigms: so-called **perceptual recalibration** [e.g., @kraljic-samuel2006; @reinisch-holt2014; @samuel2016; @vroomen-baart2009] and **(non-native) accent adaptation** [e.g., @bradlow-bent2008; @hernandez2019; @tzeng2016; @sidaras2009; @xie2016jep; @zheng-samuel2020]. For each, we find that result patterns taken as signature evidence of one mechanism (e.g., changes of representations) are, in fact, predicted by *any of the three mechanisms*. Second and perhaps more significant, we find that it *is* be possible for future experiments to distinguish between the three mechanisms, provided new design and analysis approaches are employed. We conclude with recommendations for future work that are informed by these simulations, and summarize advantages of ASP that are beyond the scope of the present work (e.g., the ability to derive prediction, or fit human data, for *combinations* of mechanisms; the ability to investigate whether the involvement of different mechanisms depends on certain stimulus properties, task demands, and/or individual differences between listeners).

\begin{figure}[h]
\begin{center}
\includegraphics[width=.99\columnwidth]{`r get_path("../figures/diagrams/overview-of-changes.png")`}
\caption{Overview of our approach. Experiments on the effect of recent exposure on subsequent speech perception tend to involve two phases. An exposure phase manipulates the statistics of speech input between participants. A subsequent test phase assesses the effects of those manipulations on the interpretation of identical speech input. We seek to understand what type of exposure effects each of the three mechanisms in Figure \ref{fig:overview} can explain. To this end, we specify both a) a {\em categorization model} that describes the interpretation of speech input at any given moment (vertical information flow) and b) {\em change models} for all three mechanisms that describe how these parts of the categorization model change as a function of exposure (horizontal information flow). We then use the different change models to compare the predicted consequences of changes to normalization, representations, or response biases.}\label{fig:overview-change}
\end{center}
\end{figure}



