\setcounter{page}{1}

```{r, include=FALSE}
max.f0 <- 350
min.observation.n <- 25
max.p <- .1

d.chodroff_wilson <-
  get_ChodroffWilson_data(
    database_filename = get_path("../data/Chodroff-Wilson-2018/all_observations_with_non-missing_vot_cog_f0.csv"),
    min.n_per_talker_and_stop = min.observation.n,
    limits.VOT = c(-Inf, Inf),
    limits.f0 = c(0, max.f0),
    max.p_for_multimodality = max.p
  ) %>%
  mutate_at(
    c("VOT", "f0", "f0_Mel", "f0_semitones"),
    list("centered" = function(x) apply_ccure(x, data = .)))
```

# Introduction
How human listeners are able to infer meaning from speech is a central question in cognitive and neurosciences. The computational complexity of spoken language understanding becomes most apparent when a talker's pronunciations---and thus the mapping of acoustic input to linguistic categories and meaning---strongly deviate from listeners' expectations. This might occur, for example, when listening to a talker with an unfamiliar regional or non-native accent, or a patient with apraxia or dysarthria. The same fundamental challenge is, however, present even during seemingly effortless comprehension. Even among talkers who share similar language backgrounds, the mapping between the acoustic input and linguistic categories can vary substantially between talkers due to both physiology (e.g., vocal tract size and shape) and socio-cultural factors (e.g., social identity and language background). As a consequence, one talker’s pronunciation of, say, the sound category /s/ (as in *sip*) can be acoustically more similar to another talker’s production of $/`r linguisticsdown::cond_cmpl("ʃ")`/$ [as in *ship*, @newman2001]. How we manage to understand each other despite such cross-talker differences has remained one of the perennial puzzles in research on speech perception [where it is known as part of the infamous *lack of invariance* problem, @liberman1967].

This article presents a general computational framework---which we refer to as *ASP* for *adaptive speech perception*---that helps address this question. ASP grew out of our long-term goal to contribute to the development of stronger theories, affording decisive comparisons between alternative hypotheses about the mechanisms underlying speech perception [in the tradition of "strong inference" approaches to scientific inquiry, @platt1964]. We identify the three most influential hypotheses about the mechanisms that allow listeners to overcome cross-talker variability (Figure \@ref(fig:overview)). These three hypotheses---described below---entail fundamentally different cognitive abilities and neural architectures, with far-reaching consequences for theories of speech perception, linguistics, and the malleability of neural representations more generally. ASP is the first to formalize and implement all three of these mechanisms in a common computational framework. This responds to a need identified in recent reviews to better characterize the mechanisms of adaptive speech perception [e.g., @baeseberk2020; @kurumada-roettger2021; @johnson-sjerps2021; @quam-creel2021; @stilp2020; @weatherholtz-jaeger2016].

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\columnwidth]{`r get_path("../figures/diagrams/overview-of-three-mechanisms.png")`}
\caption{Listeners' recognition of speech categories are typically assumed to involve at least three types of mechanisms: 1) the acoustic input is transformed by low-level normalization processes into the perceptual cues that form the input to categorization, 2) category representations describe the mapping between these perceptual and linguistic categories (here $C_1$ and $C_2$), and 3) decision-making mechanisms allow additional, stimulus-independent, biases to affect recognition. Any of these three mechanisms can in theory be affected by recent experience. It is unknown which (combination) of the three mechanisms underlie these changes and how the engagement of different mechanisms depends on, for instance, stimulus properties, task demands, and individual differences between listeners.}\label{fig:overview}
\end{center}
\end{figure}

While the three hypotheses in Figure \@ref(fig:overview) have largely been pursued in separate lines of research, they are not mutually exclusive. <!-- Indeed, with a phenomenon as complex as human speech perception, it is most plausible that multiple mechanisms are at play.--> This raises questions as to whether adaptive speech perception is the result of *combinations* of these mechanisms, and (if so) how the relative engagement of these mechanisms depends on, for instance, stimulus properties, task demands, or individual differences between listeners. ASP can be used to address these questions. Researchers can, for example, use ASP to predict adaptive changes in speech perception while 'switching off' individual change mechanisms---making it possible to test whether any of the three mechanisms is *sufficient* to explain a given result---or to predict changes from weighted combinations of all three mechanisms. Future work could use ASP, for example, to investigate whether specific instances of impaired adaptation [as seen, e.g., in children with dyslexia, @gabay2021; @ozernov-palchik2021] arise from auditory, linguistic, or cognitive sources---an important prerequisite for devising more effective interventions/treatments.

For the present article, we have three more immediate goals. The first part of this article introduces the ASP framework, along with visual demonstrations and animations. We take a deliberately tutorial-like approach so to help researchers apply ASP to their own experiments. All data and code for this article can be downloaded from the Open Science Framework at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [@R; @RStudio, see supplementary information for detailed software requirements]. Readers can also revisit any of the assumptions we make---e.g., changing parameterizations of our models, or substituting alternative models [see also @tan2021]. We welcome questions about the code, including the accompanying \texttt{R} library.

Second, we demonstrate one way in which computational frameworks like ASP can advance theories of speech perception. We simulate human recognition for two influential experimental paradigms under each of the three hypothesized mechanisms. The first paradigm---*perceptual recalibration* [e.g., @kraljic-samuel2006; @reinisch-holt2013; @samuel2016; @vroomen-baart2009]---tends to employ a small set of exposure and test stimuli that are phonetically manipulated to form a single perceptual continuum. This property makes such experiments comparatively easy to understand and model, and thus suitable as an initial case study. The second paradigm---*adaptation to second language (L2) accents* [e.g., @bradlow-bent2008; @hernandez2019; @tzeng2016; @sidaras2009; @xie2016jep; @zheng-samuel2020]---introduces a greater degree of complexity for both the listener and the researcher. Stimuli for these experiments tend to exhibit the full range of naturally occurring phonetic variability, introducing listeners to novel phonetic cues, differences in cue weightings, and other complex changes in the mapping from acoustics to linguistic categories.

We find that, contrary to common assumptions, the signature findings of both paradigms are qualitatively compatible with *any* of the three hypothesized change mechanisms. Beyond the immediate consequences for research on perceptual recalibration and accent adaptation, this demonstrates how ASP can inform and revise intuitions about the mechanisms underlying adaptive speech perception. The R code for our case studies further serves as a template for experimenters interested in understanding the theoretical implications of their results while reducing the need for ad-hoc reasoning.

Finally, we discuss how computationally-guided behavioral and neuroimaging research can advance future work. We show that the empirical indeterminacy of many existing findings is due to the level of analysis employed in most research on adaptive speech perception. We propose new standards of experimental design and data analysis that will more effectively distinguish between competing hypotheses about the relative engagement of different mechanisms. But first some background.

## The state of the field(s)
Research over the past decades has identified adaptive changes in speech perception to be a key component in listeners' ability to overcome cross-talker differences. Although speech perception can initially be slower and/or less accurate when listeners encounter an unfamiliar talker with unexpected pronunciations, these processing difficulties tend to reduce with exposure [e.g., @bradlow-bent2008; @nygaard1994; @Perrachione2016; @sidaras2009; @wade2007; @weil2001a; @xie2021jep]. Remarkably, substantial improvements---such as reduced processing times or increased accuracy of recognition---can occur within minutes or less [@clarke-garrett2004; @munro-derwing1995; @xie2018jasa]. Even a context as brief as a single utterance can be sufficient to change perception and segmentation of ambiguous speech, blurring the distinction between processing and adaptation [e.g., categorizing a Dutch ambiguous /m?t/ embedded in an utterance with fast or slow speech as either "mat" or "maat" Bosker et al.,-@bosker2017; see also @kluender1988; @newman-sawusch1996; @sawusch-newman2000; @sjerps2011]. Yet, adaptive changes after only a few words of exposure can sometimes last for days, if not longer [@eisner-mcqueen2006; @samuel2021; @xie2016; see also @goldinger1996].

In short, listeners' ability to adapt based on recent input is now considered a central part of human speech perception, and substantial progress has been made in identifying its empirical properties and constraints [for comprehensive reviews, see @baeseberk2020; @johnson-sjerps2021; @tzeng2021; @quam-creel2021; @stilp2020]. Despite these substantial advances, however, **it remains unclear *how*---through what mechanisms---recent exposure comes to facilitate spoken language understanding.** This is not due to a lack of theoretical proposals. Across cognitive sciences and neurosciences, there are now dozens of competing perspectives. However, contrastive comparisons between proposals have largely been lacking. Two inter-related factors seem to contribute to this: (1) many proposals---including some of our own past work---remain under-specified, constituting informal hypotheses rather than theories or models that lend themselves to strong contrastive tests; <!-- ^[This has consequences beyond the ones we focus on here. Often multiple labels exist that seem to refer to the same general idea, without attempts to consolidate terminology (e.g., "perceptual recalibration", "phonetic retuning", and related combinations of these terms; "statistical learning" and "distributional learning"). It is also not uncommon that terminological usage seems to blur the distinction between *mechanism* and types of *paradigms* or the *phenomenon* that this paradigm is thought to study (e.g., "dimension-based statistical learning", "perceptual recalibration"). Yet other proposals seem to *describe effects* rather than mechanisms (e.g., "boundary shift") but what type of effect is observed (a boundary shift vs. an increase in accuracy) primarily reflects the researchers' choice of paradigm and analysis, without necessarily being a characteristic property of the underlying mechanisms. In our experience, this lack of clear distinctions between mechanisms and properties of experiments can confound newcomers to the field.]--> (2) there is a tendency, particularly in behavioral research, to focus on characterizing properties of adaptive speech perception rather than on identifying the underlying cognitive and neural architectures [but see @apfelbaum-mcmurray2015; @chodroff-wilson2020; @harmon2019; @hoffmanbion-escudero2007; @kleinschmidt-jaeger2015; @kiefte-nearey2019; @lehet-holt2020; @mcmurray-jongman2011; @xie2021cognition]. As a consequence, different hypotheses are often *assumed* rather than tested, with different lines of research making different (implicit) assumptions, sometimes co-existing for decades without targeted attempts to contrast the predictions that would follow from those assumptions.

Neuroimaging research has, on the whole, been more invested into contrastive comparisons [e.g., @bonte2017; @erb2013brain; @guediche2015evidence; @myers-mesite2014]: questions about the involvement of different types of information processing---operationalized as differential activation of brain areas and networks that are associated with different functionality---are the bread and butter of neuroimaging. This approach has identified a wide range of brain regions as involved in different aspects of adaptive speech perception, ranging from subcortical areas [e.g., @skoe2021auditory; @guediche2015evidence] to networks associated with post-perceptual decision-making [e.g., @myers-mesite2014; @erb2013brain]. This leaves open, however, what types of computations underlie the observed differential activations, or why different types of exposure lead to different types of behavioral changes. In light of these gaps in our understanding, recent reviews continue to emphasize the need to better characterize the nature of the mechanisms underlying adaptive speech perception [from, e.g., @samuel-kraljic2009; to @weatherholtz-jaeger2016; to @baeseberk2020]. This includes questions about how the task demands of different types of experimental paradigms affect the engagement of different mechanisms [@baeseberk2018; @zheng-samuel2020], and thus also which paradigms are most likely to shed light on the mechanisms affording flexible perception in everyday life.

The ASP framework we present in Section \@ref(sec:framework) is designed to help address these questions. We group existing proposals for adaptive speech perception into three types of theoretical perspectives (Figure \@ref(fig:overview)). The three proposals share with each other, and with all major theories in speech perception, general assumptions about how the acoustic input supports perception of a speech category. The process begins with (A) the extraction and normalization of acoustic/phonetic cues. These cues are (B) mapped onto, and activate, linguistic categories (such as phonemes, syllables, and/or words). Finally, (C) decision processes integrate the resulting category activations with contextual support (e.g., from lexical or sentential context) and/or meta-reasoning (e.g., about the task), and recognition takes place. Existing proposals differ, however, in which of (A)-(C) explains *changes* in speech perception as a function of recent exposure. As we discuss next, it is these differences that entail fundamentally different cognitive and neural architectures.

The majority of recent behavioral research on talker adaptation has focused on the middle layer, **changes in the mapping from acoustic or phonetic cues onto phonological categories (for brevity: changes in category representations)**. This includes proposals that attribute exposure effects to "boundary re-tuning/shift" [e.g., @norris2003; @reinisch2013], "perceptual/category recalibration" [e.g., @kraljic-samuel2006; @reinisch-holt2013; @samuel2016; @vroomen-baart2009], "perceptual retuning" [@jesse-mcqueen2011; @mcqueen2006; @mitterer2013], "category shift" [@lindsay2022; @sawusch-pisoni1976]; "category expansion" [@schmale2012], or "dimension-based statistical learning" [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015]. While these proposals are often not further formally specified or modeled [for notable exceptions, see @apfelbaum-mcmurray2015; @clayards2008; @harmon2019; @hitczenko-feldman2016; @kleinschmidt-jaeger2015; @lancia-winter2013; @xie2021cognition], they all describe types of changes in category representations. For example, "category shift" refers to a change in the mean of the cue distribution corresponding to a category, and "category expansion" refers to increases in the variance of that distribution.

One reason that the idea of representational changes has received so much attention is that it is of high theoretical relevance. Changes in category representations as a function of recent exposure are predicted by influential exemplar [@apfelbaum-mcmurray2015; @johnson2006], episodic [@goldinger1998], Bayesian inference [@kleinschmidt-jaeger2015], reinforcement learning [@harmon2019], and neural network models [@lancia-winter2013]. All of these theories predict that listeners can learn and store talker- or accent-specific representations [see also @bradlow-bent2008; @baeseberk2013;  @tzeng2016]. Perhaps unsurprisingly, this idea---continued implicit learning of new representations throughout adult life---has been influential beyond speech perception. The possibility that listeners learn and maintain category representations for multiple types of talkers has influenced psycholinguistic research on lexical, sentence, and semantic/pragmatic processing [@chang2012; @fine2013; @kaschak2004; @pogue2016; @prasad2021; @ryskin2019; @schuster-degen2020; <!--@yildirim2016; @wells2009;--> for review, see @brown-schmidt2015] and research on second language learning [for reviews, see @kaan-chun2018; @pajak2016]. It has shaped linguistic theories [e.g., @bybee2001; @goldinger-azuma2004; @hay2019; @magnuson-nusbaum2007; @pierrehumbert2001] as well as theories about the interface between social and linguistic cognition [e.g., @babel2019; @creel-bregman2011; @foulkes-hay2015; @hanulikova2012; @sumner2014]. Further illustrating the influence of this idea, several recent reviews have gone as far as to discuss *what types* of representational changes underlie the effects of recent exposure (e.g., "category expansion" vs. "category shifts"), rather than *whether* representational changes are indeed necessary to explain adaptive speech perception [e.g., @baeseberk2020; @bent-baeseberk2021; @schertz-clare2020].

In the absence of contrastive tests, however, the same behavioral results that have been interpreted as arguing for representational changes could in principle be explained by mechanisms that are both computationally and representationally more parsimonious than changes in category representations.^[We use the term *computational parsimony* to refer to the number of parameters that listeners and/or researchers need to estimate (the degrees of freedom of a model), and *representational parsimony* to refer to the amount of information listeners need to keep in memory (for details, see Section \@ref(sec:parsimony)).] For instance, proposals dating back to at least the 1970s hold that adaptive speech perception is due to low-level, automatic (involuntary) **normalization** during the early stages of auditory processing (bottom of Figure \@ref(fig:overview)). These processes are thought to be pre-linguistic in that they do not refer to mapping from cues to categories but rather refer to the *overall* distribution of acoustic or phonetic cues, regardless of the category [for reviews, @johnson-sjerps2021; @stilp2020]. Compared to theories of representational changes, this makes more parsimonious assumptions about listeners' adaptive abilities: when listening to an unfamiliar talker, listeners only need to infer, e.g., the overall mean of phonetic cues rather than the category-specific cue means. Evidence from more recent neuroimaging studies has lent support for low-level normalization mechanisms by showing engagement of subcortical structures in sensory adaptation [e.g., the brain stem, @skoe2021auditory; and cerebellum, @guediche2015evidence; for review, see @guediche2014]. Studies using intracranial electrocorticography have found evidence of talker-normalized (spectral) cues in the auditory cortex before they are mapped onto categories [e.g., @sjerps2019; @Tang2017]. This leaves open, however, whether normalization can indeed explain talker-specific adaptation that has been attributed to changes in representations. If it can, this would remove the necessity for the computational complexity and cognitive abilities implied by theories of representational changes.^[It would also indicate a need to revisit the interpretation of findings that speech perception can be affected by the (inferred) social identity of a talker [e.g., regional origin, @hay-drager2010; @niedzielski1999; sex, @johnson1999; @strand1999; age, @skoogwaller2015; @walker-hay2011; and individual identity, @nygaard1994; @remez2018]. In sociophonetics and related fields, these findings seem to be routinely attributed to talker-specific storage of *category representations*, without considering more parsimonious explanations in terms of talker-specific storage of *marginal cue distributions* (normalization). While early, physiology-focused, normalization accounts indeed cannot explain socially-conditioned pronunciation variation [see, e.g., @johnson2006], modern theories of normalization that we discuss in Section \@ref(sec:framework) are not subject to these constraints [see also @magnuson-nusbaum2007; @pisoni1997].]

Another explanation for adaptive speech perception that does not refer to changes in representations are post-perceptual---'cognitive'--- mechanisms of decision-making (top of Figure \@ref(fig:overview)). Like normalization, this explanation is computationally and representationally more parsimonious than changes in representations. And just like normalization is broadly accepted to be part of speech perception, there is little doubt that **changes in decision and response biases** can affect listeners' interpretation of speech input, or at least the responses they give within experiments. For instance, Clarke-Davidson et al. [-@clarkedavidson2008, p.605] define decision or response biases as "the increased likelihood to give a particular response---such as /s/---given any acoustic input, or the need for less evidence for a particular response" that "would help participants make faster word decisions in [...] ambiguous cases". Changes in decision-making have received substantial attention in neuroimaging research. Studies have investigated whether adaptive changes in speech perception primarily recruit areas associated with decision-making [@erb2013brain; @myers-mesite2014]---such as prefrontal areas [e.g., @binder2004neural; @thompson1997role] as well as the insula and parietal cortex [e.g., @furl2011parietal; @keuken2014]---as opposed to cortical areas associated with phonetic representations [@sohoglu-davis2016; @bonte2017; @luthra2020a]. For instance, @myers-mesite2014 found that sensitivity to category shifts between /s/ and $/`r linguisticsdown::cond_cmpl("ʃ")`/$ emerged in right frontal and middle temporal regions, implicating adjustments of decision-related or attentional criteria [additional analyses further implicated left inferior parietal and right temporal areas, as confirmed by recent MVPA re-analyses of the same data, @luthra2020a]. <!--Xin: can we edit this together?--> 
<!--^[Another recent study [@blanco-elorriera2021] found that adaptation to phoneme category substitutions (e.g., [s] pronounced as [`r linguisticsdown::cond_cmpl("ʃ")`]) engages prefrontal regions, which is responsible for post-perceptual repair mechanisms: listeners seem to initially map the acoustic input onto the wrong category and subsequently corrects the mapping. This was contrasted with the hypothesis that adaptation occurs via retuning of the lower-level functional connections (e.g., between the superior temporal gyrus and primary auditory cortex) that would be associated with perceptual processing. Whether these findings generalize to adaptive perception of *phonetic* differences between talkers is an open question.]-->
In behavioral research, however, changes in response biases continue to be rarely considered when interpreting the effects of recent exposure. *When* they have been considered [e.g., as "response equilibration" in Vroomen and Baart -@vroomen-baart2009], this explanation tends to be dismissed. One reason for this might be that particular result patterns---such as boundary shifts in perceptual recalibration or improved categorization or transcription accuracy in accent adaptation---are taken to rule out changes in response biases (or normalization, for that matter). However, to anticipate one take-home point from the case studies we present below, we find that response biases can explain more complex changes in behavior than has previously been assumed, including the very signature results that are often assumed to be incompatible with changes in response bias.

In summary, behavioral and neuroimaging research has identified multiple potential candidates for the mechanisms underlying adaptive speech perception. However, contrastive comparisons of competing hypotheses about these mechanisms---and the computations they imply---remain lacking. As a result, we often do not know whether the results of a specific paradigm clearly support one mechanism over another, or whether the results of different paradigms all point to the same (combination of) mechanisms. The computational framework of adaptive speech perception (ASP) we present next provides a way for future work to address these questions.
