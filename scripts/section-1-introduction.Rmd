\newpage 
\setcounter{page}{1}

```{r, include=FALSE}
max.f0 <- 350            # max raw f0
min.observation.n <- 25  # min observation per stop
max.p <- .1              # cut-off for rejection of unimodality

d.chodroff_wilson <- read_csv(get_path("../data/Chodroff-Wilson-2018/all_observations_with_non-missing_vot_cog_f0.csv")) %>%
  rename(category = stop, VOT = vot, f0 = usef0, Talker = subj, Word = word, Trial = trial, Vowel = vowel) %>%
  mutate(
    category = 
      plyr::mapvalues(
        category,
        c("B", "D", "G", "P", "T", "K"),
        c("/b/", "/d/", "/g/", "/p/", "/t/", "/k/")),
    gender = factor(
      plyr::mapvalues(
        gender, 
        c("F", "M"),
        c("female", "male")),
      levels = c("male", "female")),
    poa = factor(
      plyr::mapvalues(
        poa, 
        c("lab", "cor", "dor"),
        c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
      levels = c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
    voicing = factor(
      ifelse(category %in% c("/b/", "/d/", "/g/"), "yes", "no"),
      levels = c("yes", "no"))) %>%
  mutate(across(c(Talker, Word, gender, category), factor)) %>%
  select(Talker, Word, Trial, Vowel, gender, category, poa, voicing, VOT, f0)

# Filter f0 above 500 Hz
d.chodroff_wilson %<>%
  filter(f0 < max.f0)

# Keep only subjects with at last n.min observations for each stop
d.chodroff_wilson %<>%
  group_by(Talker, category) %>%
  mutate(n = length(category)) %>%
  group_by(Talker) %>%
  mutate(n = ifelse(any(is.na(n)), 0, min(n))) %>%
  ungroup() %>%
  filter(n > min.observation.n)

# Identify and remove talkers with bimodal f0 distributions
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(f0_Mel = phonR::normMel(f0)) %>%
  group_by(Talker, category) %>%
  mutate(
    f0.multimodal = dip.test(f0)$p.value < .1,
    f0_Mel.multimodal = dip.test(f0_Mel)$p.value < .1) %>%
  filter(!f0.multimodal, !f0.multimodal) %>%
  droplevels()

# Get Mel and Semitones, then C-CuRE
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(
    f0_Mel = phonR::normMel(f0),
    f0_semitones = 12 * log(f0 / mean(f0)) / log(2)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .)))
```

# Introduction
How human listeners are able to infer meaning from speech is a central question in the cognitive and neurosciences. The computational complexity of spoken language understanding becomes most apparent when a talker's pronunciations---and thus the mapping of acoustic input to linguistic categories and meaning---strongly deviate from listeners' expectations. This might occur, for example, when listening to a talker with an unfamiliar regional or non-native accent, or a patient with apraxia or dysarthria. The same fundamental challenge is, however, present even during seemingly effortless comprehension: even among talkers who share similar language backgrounds, the mapping between the acoustic input and linguistic categories can vary substantially between talkers due to both physiology (e.g., vocal tract size and shape) and socio-cultural factors (e.g., social identity and language background). As a consequence, one talker’s pronunciation of, say, the sound category [s] (as in *sip*) can be acoustically more similar to another talker’s production of [`r linguisticsdown::cond_cmpl("ʃ")`] [as in *ship*, Newman et al., -@newman2001]. How we manage to typically understand each other despite such cross-talker differences has remained one of the perennial puzzles in research on speech perception [where it is known as part of the infamous *lack of invariance* problem, @liberman1967]. 

This article presents a general computational framework---which we will refer to as *ASP* for *adaptive speech perception*---that helps address this question. We identify three influential hypotheses about the mechanisms that allow listeners to overcome cross-talker variability. ASP, for the first time, formalizes and implements all three of these mechanisms. The three hypotheses (described below) entail fundamentally different cognitive abilities and neural architectures, and have far-reaching consequences for theories of speech perception, linguistics, and the malleability of neural representations more generally. Yet, tests that effectively contrast these different theoretical perspectives have been lacking for decades, with separate lines of research often *assuming* rather than testing that their paradigms tap into one of the three mechanisms. ASP will facilitate systematic well-formed comparisons between these alternatives, within and across different experimental paradigms. This supports our long-term goal to contribute to the development of stronger theories, facilitating decisive comparisons between alternative hypotheses about the mechanisms underlying speech perception [in the tradition of "strong inference" approaches to scientific inquiry, @platt1964]. ASP also provides an objective way to assess how the relative engagement of the three mechanisms depends task demands, stimulus properties, or individual differences between listeners. This ability to study how the three mechanisms *combine* is something that we expect to become increasingly important. It will, for example, allow future applications of ASP to help clarify whether specific instances of impaired adaptation [as seen, e.g., in children with dyslexia, @gabay2021; @ozernov-palchik2021] arise from auditory, linguistic, or cognitive sources---an important prerequisite for future developments of more effective interventions/treatments. <!--To think about: the last point seems to be orthogonal to understaning the combination of mechanisms. (If reduced adaptation in dyslexia is purely due tom say, a failure in phonological learning, that's very informative.) Should we move this last sentence up?--> 

For the present article, we have three more immediate goals. The first part of this article introduces the ASP framework, along with visual demonstrations and animations. We take a deliberately tutorial-like approach in order to make the framework accessible to researchers interested in applying it to their experiments. All data and code for this article can be downloaded from the Open Science Framework at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using the freely available software [@R; @RStudio, see supplementary information for detailed software requirements]. Readers can also revisit any of the assumptions we make---e.g., changing parameterizations of our models, or substituting alternative models [see also @tan2021]. We welcome questions about the code, including the accompanying \texttt{R} library, and hope that it can support the development of experimental designs that decisively distinguish between competing mechanisms, and to facilitate the interpretation of existing results. 

Second, we provide an initial demonstration of *how* computational frameworks like ASP can move the field forward. We use ASP-based simulations to show that the highly-replicated signature findings of two influential experimental paradigms---*perceptual recalibration* paradigms [e.g., @kraljic-samuel2006; @reinisch-holt2013; @samuel2016; @vroomen-baart2009] and experiments on *(non-native) accent adaptation* [e.g., @bradlow-bent2008; @hernandez2019; @tzeng2016; @sidaras2009; @xie2016jep; @zheng-samuel2020]---are qualitatively compatible with *any* of the three hypothesized change mechanisms. The degree to which existing findings from these paradigms do not distinguish between competing theories surprised us, and we suspect that we are not alone in this. These case studies demonstrate how ASP can be used to inform and revise intuitions about the mechanisms underlying adaptive speech perception, and re-affirms the need identified in recent reviews to better characterize these mechanisms [e.g., @baeseberk2020; @kurumada-roettger2021; @johnson-sjerps2021; @quam-creel2021; @stilp2020; @weatherholtz-jaeger2016]. Beyond the immediate implications of our findings for research on perceptual recalibration and accent adaptation, we hope that our case studies can serve as a template for other researchers [see, e.g., @hitzcenko-feldman2016 or @theodore-monto2019 for an application of the representational change model].

Finally, we discuss how computationally-guided behavioral research---in combination with neuroimaging---can be employed in future work to more productively distinguish between alternative (combinations of) hypotheses about adaptive speech perception. We show that the empirical indeterminacy of many existing findings is due to the level of analysis chosen in those studies: with adequate designs and analyses, it will become possible to distinguish between competing hypotheses, and to estimate the involvement of multiple mechanisms more effectively. But first some background.

## The state of the field(s)
Research over the past decades has identified adaptive changes in speech perception to be a key component in listeners' ability to overcome cross-talker differences. Although speech perception can initially be slower and/or less accurate when listeners encounter an unfamiliar talkers with unexpected pronunciations, these processing difficulties tend to reduce with exposure [e.g., @bradlow-bent2008; @nygaard1994; @Perrachione2016; @sidaras2009; @wade2007; @weil2001a; @xie2021jep]. Remarkably, substantial improvements---such as reduced processing times or increased accuracy of recognition---can occur within minutes or less [@clarke-garrett2004; @munro-derwing1995; @xie2018jasa]. Even a context as brief as a single utterance can be sufficient to change perception and segmentation of ambiguous speech, blurring the distinction between processing and adaptation [e.g., categorizing a Dutch ambiguous /m?t/ embedded in an utterance with fast or slow speech as either "mat" or "maat" Bosker et al.,-@bosker2017; see also @kluender1988; @newman-sawusch1996; @sawusch-newman2000; @sjerps2011]. <!-- TO-DO: The rest of the para could potentially go? --> Changes in speech perception as a function of recent exposure have now been documented across a broad range of experimental paradigms and tasks [e.g., @bradlow-bent2008; @clayards2008; @eisner2013; @idemaru-holt2011; @norris2003; @vroomen2007], different modes of speech [e.g., "clear" and "conversational" speech, @zhang-samuel2014]; isolated and connected speech, [e.g., @eisner-mcqueen2005; @reinisch-holt2013], and languages [e.g., @chladkova2017; @eisner2013; @hanulikova-weber2012; @sebastian-galles2000; @schertz2015]. Changes can occur even under cognitive load, arguing for a relatively high degree of automaticity of some of the underlying mechanisms [@zhang-samuel2014; though both top-down attention-direction and interference can modulate adaptation, @mcauliffe-babel2016; @samuel2016]. Yet, adaptive changes after only a few words of exposure can sometimes last for days if not longer [@eisner-mcqueen2006; @samuel2021; @xie2016; see also @goldinger1996]. 

In short, listeners' ability to adapt based on recent input is now considered a central part of human speech perception, and research over the last few decades has made tremendous progress in identifying its empirical properties and constraints [for comprehensive reviews, see @baeseberk2020; @johnson-sjerps2021; @tzeng2021; @quam-creel2021; @stilp2020]. Despite these substantial advances, however, **it remains unclear *how*---through what mechanisms---recent exposure comes to facilitate spoken language understanding.** This is not for a lack of theoretical proposals. Across the cognitive and neurosciences, there are now dozens of competing perspectives. However, contrastive comparisons between proposals have largely been lacking. Two inter-related factors seem to contribute to this: (1) many proposals---including some of our own past work---remain under-developed, constituting informal hypotheses rather than theories or models that lend themselves to strong contrastive tests,<!--TO-DO/DONE: comment out this footnote? ^[This has consequences beyond the ones we focus on here. Often multiple labels exist that seem to refer to the same general idea, without attempts to consolidate terminology (e.g., "perceptual recalibration", "phonetic retuning", and related combinations of these terms; "statistical learning" and "distributional learning"). It is also not uncommon that terminological usage seems to blur the distinction between *mechanism* and types of *paradigms* or the *phenomenon* that this paradigm is thought to study (e.g., "dimension-based statistical learning", "perceptual recalibration"). Yet other proposals seem to *describe effects* rather than mechanisms (e.g., "boundary shift") but what type of effect is observed (a boundary shift vs. an increase in accuracy) primarily reflects the researchers' choice of paradigm and analysis, without necessarily being a characteristic property of the underlying mechanisms. In our experience, this lack of clear distinctions between mechanisms and properties of experiments can confound newcomers to the field.]--> and (2) a tendency, in particular in behavioral research, to focus on identifying properties of adaptive speech perception rather than on distinguishing the underlying cognitive and neural architectures [but see @apfelbaum-mcmurray2015; @chodroff-wilson2020;  @hoffmanbion-escudero2007; @kleinschmidt-jaeger2015; @kiefte-nearey2019; @lehet-holt2020; @mcmurray-jongman2011; @xie2021cognition]. Separate lines of research often pursue, or simply (often implicitly) *assume*, one hypothesis at a time. In light of this, it is not surprising that reviews have for some time emphasized the need to better characterize the nature of the mechanisms underlying adaptive speech perception [see e.g., @baeseberk2020; @samuel-kraljic2009; @weatherholtz-jaeger2016]. This includes, in particular, questions about how the tasks demands inherent in different types of experimental paradigms---e.g., depending on whether experiments investigate relative simple phonetic manipulations or the full complexity of natural accents---affect the engagement of different mechanisms [@baeseberk2018; @zheng-samuel2020], and thus also which paradigms are most likely to shed light on the mechanisms affording flexible perception in everyday life.

The ASP framework presented in Section \@ref(sec:framework) is designed to help address these questions. We group existing proposals for adaptive speech perception into three types of theoretical perspectives (Figure \@ref(fig:overview)). The three proposals share with each other, and with all major theories in speech perception, general assumptions about how the acoustic input supports perception of a speech category. The process begins with (A) the extraction and normalization of acoustic/phonetic cues. These cues are (B) mapped onto and activate linguistic categories (such as phonemes, syllables, and/or words). Finally, (C) decision processes integrate the resulting category activations with contextual support and/or meta-reasoning (e.g., about the task), and recognition takes place. Existing proposals differ, however, in which of (A)-(C) explains *changes* in perception as a function of recent exposure. As we discuss next, it is these differences that entail fundamentally different cognitive and neural architectures. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\columnwidth]{`r get_path("../figures/diagrams/overview-of-three-mechanisms.png")`}
\caption{Listeners' recognition of speech categories are typically assumed to involve at least three types of mechanisms: 1) the acoustic input is transformed by low-level normalization processes into the perceptual cues that form the input to categorization, 2) linguistic representations describe the mapping between these perceptual and linguistic categories (here $C_1$ and $C_2$), and 3) decision-making mechanisms allow additional, stimulus-independent, biases to affect recognition. Any of these three mechanisms can in theory be affected by recent experience. While it is now clear \emph{that} recent experience changes the processing of subsequent speech input, most existing findings leave open \emph{which (combination) of the three mechanisms underlie these changes} and \emph{how the involvement of different mechanisms depends on, for instance, stimulus properties, task demands, and individual differences between listeners}.}\label{fig:overview}
\end{center}
\end{figure}

The majority of recent behavioral research on talker-related adaptation has focused on the middle layer, **changes in linguistic representations**. This includes proposals that attribute exposure effects to "boundary re-tuning/shift” [e.g., @norris2003; @reinisch2013], “perceptual/category recalibration” [e.g., @kraljic-samuel2006; @reinisch-holt2013; @samuel2016; @vroomen-baart2009], "perceptual retuning" [@jesse-mcqueen2011; @mcqueen2006; @mitterer2013], "category shift" [@lindsay2022; @sawusch-pisoni1976]; “category expansion" [@schmale2012], “dimension-based statistical learning” [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015], or "criteria relaxation" [@zheng-samuel2020]. While these proposals are often not further formally specified or modeled [for notable exceptions, see @apfelbaum-mcmurray2015; @clayards2008; @kleinschmidt-jaeger2015; @lancia-winter2013; @xie2021cognition], they all describe types of changes in representations. For example, "category shift" refers to a change in the mean of the cue distribution corresponding to a category, and "category expansion" refers to increases in the variance of that distribution. 

One reason that the idea of representational changes has received so much attention is that it is of high theoretical relevance: representational changes as a function of recent exposure are predicted by influential exemplar [@apfelbaum-mcmurray2015; @johnson2006], episodic [@goldinger1998], Bayesian inference [@kleinschmidt-jaeger2015], or neural network models [@lancia-winter2013]. All of these theories predict that listeners can learn and store talker- or accent-specific representations [see also @bradlow-bent2008; @baeseberk2013;  @tzeng2016]. Perhaps unsurprisingly, this idea---continued implicit learning of new representations throughout their adult life---has been influential beyond speech perception. The possibility that listeners learn and maintain linguistic representations for multiple types of talkers has influenced psycholinguistic research on, for example, lexical, sentence, and semantic/pragmatic processing [@schuster-degen2020; @fine2013; @fraundorf-jaeger2016; @kaschak2004; @pogue2016; @prasad2021; @ryskin2019; @yildirim2016; @wells2009; for review, see @brown-schmidt2015; @chang2012] and research on second language learning [for reviews, see @kaan-chun2018; @pajak2016]. It has shaped linguistic theories [e.g., @bybee2001; @goldinger-azuma2004; @hay2019; @magnuson-nusbaum2007; @pierrehumbert2001] as well as theories about the interface between social and linguistic cognition [e.g., @babel2019; @creel-bregman2011; @foulkes-hay2015; @hanulikova2012; @sumner2014]. Further illustrating the influence of this idea, several recent reviews go as far as to discuss *what types* of representational changes underlie the effects of recent exposure (e.g., "category expansion" vs. "category shifts"), rather than *whether* representational changes are the actual mechanism underlying the observed results [e.g., @baeseberk2020; @bent-baeseberk2021; @schertz-clare2020].

<!--TO DO: INTEGRATE IN  NEXT PARA: “normalization” [@johnson1990; @pisoni1977],-->
In the absence of contrastive tests, however, the same behavioral results that have been interpreted as arguing for representational changes could *in principle* be explained by computationally more parsimonious mechanisms that do *not* assume changes in linguistic representations. For instance, proposals dating back to at least the 1970s hold that adaptive changes in perception can be due to low-level, automatic (involuntary) **normalization** during the early stages of auditory processing (bottom of Figure \@ref(fig:overview)). These processes are thought to be pre-linguistic in that they do not refer to categories but rather apply to the acoustic or phonetic cues [for reviews, @johnson-sjerps2021; @stilp2020]. In contrast to the assumption that cross-talker variability may be learned and stored, which is fundamental to all the theories of representational changes as described above, normalization proposals assume that listeners remove talker variability prior to mapping cues to linguistic categories. More recently, evidence from neuroimaging studies has lent support for low-level normalization mechanisms by showing engagement of sub-cortical structures in sensory adaptation [e.g., the brain stem, @skoe2021auditory; and cerebellum, @guediche2015evidence; for review, see @guediche2014]. This leaves open, however, whether normalization can indeed explain the type of phenomena that have been attributed to changes in representations. If it can, this would undermine the empirical foundation for the computational complexity and cognitive abilities implied by theories of representational changes.^[It would also indicate a need to revisit the interpretation of findings that speech perception can be affected by the (inferred) social identity of a talker [e.g., regional origin, @hay-drager2010; @niedzielski1999; sex, @johnson1999; @strand1999; age, @skoogwaller2015; @walker-hay2011; and individual identity, @nygaard1994; @remez2018]. In sociophonetics and related fields, these findings are routinely attributed to talker-specific storage of *linguistic representations*, without considering alternative explanations in terms of normalization. While early normalization accounts that were limited to correction for physiology were rather convincingly rejected by cross-linguistic comparisons [@johnson2006], modern theories of normalization like the ones we discuss in Section \@ref(sec:framework) are not subject to those arguments.] <!-- TO DO: Xin, Chigusa, I moved this here from the GD. Basically, just to get it out of the way since I felt it was stranded where it was. If that's ok, can you delete this comment? -->

Another alternative explanation for adaptive speech perception that does not refer to changes in representations are *post-linguistic* mechanisms (top of Figure \@ref(fig:overview)). Just like normalization, this explanation is computationally more parsimonious than changes in representations. And just like normalization is broadly accepted to be part of speech perception, there is little doubt that **changes in decision and response biases** can affect listeners' interpretation of speech input, or at least the responses they give within experiments. For instance, Clarke-Davidson et al. [-@clarkedavidson2008, p.605] define a bias as "the increased likelihood to give a particular response---such as /s/---given any acoustic input, or the need for less evidence for a particular response" that "would help participants make faster word decisions in [...] ambiguous cases". In behavioral research, such biases continue to be rarely considered when interpreting the effects of recent exposure. *When* response biases have been considered [e.g., as "response equilibration" in Vroomen and Baart -@vroomen-baart2009], this explanation tends to be dismissed. One reason for this might be that particular result patterns---such as boundary shifts in perceptual recalibration or improved categorization or transcription accuracy in accent adaptation---are taken to rule out changes in response biases (or normalization, for that matter). However, to anticipate one take-home point from the case studies we present below, we find that response biases can explain more complex changes in behavior than has previously been assumed, including the very signature results that are often assumed to be incompatible with response bias.

In contrast to behavioral research, changes in decision-making have received substantial attention in neuroimaging research. Recent studies have investigated whether adaptive changes in speech perception primarily recruit areas associated with decision-making [@myers-mesite2014; @erb2013brain]---such as prefrontal areas [e.g., @binder2004neural; @thompson1997role] as well as the insula and parietal cortex [e.g., @furl2011parietal; @keuken2014]---as opposed to cortical areas associated with phonetic representations [@bonte2017; @luthra2020a]. For instance, @myers-mesite2014 found that sensitivity to category shifts between [s] and [`r linguisticsdown::cond_cmpl("ʃ")`] emerged in right frontal and middle temporal regions, implicating adjustments of decision-related or attentional criteria downstream from functions performed by primary auditory cortex. Similarly, a recent study found that adaptation to phoneme category substitutions (e.g., [s] pronounced as [`r linguisticsdown::cond_cmpl("ʃ")`]) engages prefrontal regions, responsible for post-perceptual repair mechanisms [@blanco-elorriera2021]: simply put, listeners seem to initially map the acoustic input onto the wrong category and subsequently corrects the mapping. This was contrasted with the hypothesis that adaptation occurs via retuning of the lower-level functional connections that would be associated with perceptual processing (e.g., between the superior temporal gyrus and primary auditory cortex). 

In summary, different lines of research have focused on different types of explanations for adaptive speech perception observed in different types of paradigms. Each of these research lines has found that the mechanisms under investigation can qualitatively explain behavioral and/or neural activation patterns observed in *some* paradigm. Comparisons between competing hypotheses---within or across paradigms---are, however, still lacking.^[To the extent that behavioral research has begun to contrast competing hypotheses, these efforts have focused on (A) normalization and (B) representational changes while mostly ignoring the possibility of (C) post-perceptual decision-making. Compared to the behavioral research, it is more common in neuroimaging work to directly contrast hypotheses about different mechanisms---operationalized as differential activation of different brain areas and networks that are associated with different functionality. In contrast to behavioral work, neuroimaging research tends to not group hypotheses (A) and (B) together as functionally distinct from higher-level, decision-related mechanisms further downstream (C). <!-- Neuroimaging work therefore does not (yet) distinguish between hypothesis (A) and (B). -->] The upshot of this is that we neither know for any given paradigm whether its results clearly argue for one over another mechanisms, nor whether the results of different paradigms all point to the same (combination of) mechanisms. 

<!-- TO DO: how much of this next para do we really need? It's still a bit redundant with things we have already said above -->
The computational framework that we introduce next (ASP) provides a way to address these questions (Section \@ref(sec:framework)). We demonstrate the use of ASP in two case studies (Sections \@ref(sec:PR) and \@ref(sec:AA)). These case studies show---we believe, for the first time---that, at the present level of data analysis, some of the most commonly used paradigms in speech perception do not distinguish between fundamentally different hypotheses about the mechanisms underlying adaptive speech perception. Our case studies deliberately only consider ASP models that exclusively employ one of the three change mechanisms at a time---i.e., models that assess the *sufficiency* of each mechanism. In the general discussion (Section \@ref(sec:general-discussion)), we return to this point. We first discuss how future work can take advantage of the respective computational limitations of each mechanism (derived in Section \@ref(sec:framework)) to provide more decisive tests of each mechanism's sufficiency. However, it is possible---and, arguably, likely---that adaptive speech perception draws on all three mechanisms. We thus close by discussing how future ASP modeling can help determine how the relative engagement of the three mechanisms depends on, e.g., task demands (e.g., attentional load), stimulus properties (e.g., whether an L2 accent involves simple shifts relative to the L1 accent, or the learning of new features), and individual differences between listeners. 