\newpage 
\setcounter{page}{1}

```{r, include=FALSE}
max.f0 <- 350            # max raw f0
min.observation.n <- 25  # min observation per stop
max.p <- .1              # cut-off for rejection of unimodality

d.chodroff_wilson <- read_csv(get_path("../data/Chodroff-Wilson-2018/all_observations_with_non-missing_vot_cog_f0.csv")) %>%
  rename(category = stop, VOT = vot, f0 = usef0, Talker = subj, Word = word, Trial = trial, Vowel = vowel) %>%
  mutate(
    category = 
      plyr::mapvalues(
        category,
        c("B", "D", "G", "P", "T", "K"),
        c("/b/", "/d/", "/g/", "/p/", "/t/", "/k/")),
    gender = factor(
      plyr::mapvalues(
        gender, 
        c("F", "M"),
        c("female", "male")),
      levels = c("male", "female")),
    poa = factor(
      plyr::mapvalues(
        poa, 
        c("lab", "cor", "dor"),
        c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
      levels = c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
    voicing = factor(
      ifelse(category %in% c("/b/", "/d/", "/g/"), "yes", "no"),
      levels = c("yes", "no"))) %>%
  mutate(across(c(Talker, Word, gender, category), factor)) %>%
  select(Talker, Word, Trial, Vowel, gender, category, poa, voicing, VOT, f0)

# Filter f0 above 500 Hz
d.chodroff_wilson %<>%
  filter(f0 < max.f0)

# Keep only subjects with at last n.min observations for each stop
d.chodroff_wilson %<>%
  group_by(Talker, category) %>%
  mutate(n = length(category)) %>%
  group_by(Talker) %>%
  mutate(n = ifelse(any(is.na(n)), 0, min(n))) %>%
  ungroup() %>%
  filter(n > min.observation.n)

# Identify and remove talkers with bimodal f0 distributions
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(f0_Mel = phonR::normMel(f0)) %>%
  group_by(Talker, category) %>%
  mutate(
    f0.multimodal = dip.test(f0)$p.value < .1,
    f0_Mel.multimodal = dip.test(f0_Mel)$p.value < .1) %>%
  filter(!f0.multimodal, !f0.multimodal) %>%
  droplevels()

# Get Mel and Semitones, then C-CuRE
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(
    f0_Mel = phonR::normMel(f0),
    f0_semitones = 12 * log(f0 / mean(f0)) / log(2)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .)))
```

# Introduction
How human listeners are able to infer meaning from speech is a central question in the cognitive and neurosciences. The computational complexity of spoken language understanding becomes most apparent when a talker's pronunciations---and thus the mapping of acoustic input to linguistic categories and meaning---strongly deviate from listeners' expectations. This might occur, for example, when listening to a talker with an unfamiliar regional or non-native accent, or a patient with apraxia or dysarthria. The same fundamental challenge is, however, present even during seemingly effortless comprehension: even among talkers who share similar language backgrounds, the mapping between the acoustic input and linguistic categories can vary substantially between talkers due to both physiology (e.g., vocal tract size and shape) and socio-cultural factors (e.g., social identity and language background). As a consequence, one talker’s pronunciation of, say, the sound category [s] (as in *sip*) can be acoustically more similar to another talker’s production of [`r linguisticsdown::cond_cmpl("ʃ")`] [as in *ship*, Newman et al., -@newman2001]. How we manage to typically understand each other despite such cross-talker differences has remained one of the perennial puzzles in research on speech perception [where it is known as part of the infamous *lack of invariance* problem, @liberman1967]. 

This article presents a general computational framework that helps address this question. The framework---which we will refer to as *ASP* for *adaptive speech perception*---is the first to formalize and implement all three major hypotheses about the mechanisms that allow listeners to overcome cross-talker variability. These three hypotheses entail fundamentally different cognitive abilities and neural architectures, and have far-reaching consequences for theories of speech perception, linguistics, and the malleability of neural representations more generally. Yet, tests that effectively contrast these different theoretical perspectives have been lacking for decades, with separate lines of research often *assuming* rather than testing that their paradigms tap into one of the three mechanisms. ASP facilitates systematic well-formed comparisons between these alternatives, within and across different experimental paradigms. This supports our long-term goal to contribute to the development of stronger theories, facilitating decisive comparisons between alternative hypotheses about the mechanisms underlying speech perception [in the tradition of "strong inference" approaches to scientific inquiry, @platt1964]. ASP can be used, for example, to quantitatively compare the fit of alternative hypotheses about adaptive speech perception against human behavior. Critically, this also provides an objective way to assess (1) which *combinations* of the three mechanisms best explains researchers' observation and (2) how the relative engagement of the three mechanisms depends task demands, stimulus properties, or individual differences between listeners. This should allow future applications of ASP to help clarify whether specific instances of impaired adaptation [as seen, e.g., in children with dyslexia, @gabay2021; @ozernov-palchik2021] arise from auditory, linguistic, or cognitive sources---an important prerequisite for future developments of more effective interventions/treatments. 

For the present article, we have three more immediate goals. The first part of this article introduces the ASP framework, along with visual demonstrations and animations. We take a deliberately tutorial-like approach in order to make the framework accessible to researchers interested in applying it to their experiments. All data and code for this article can be downloaded from the Open Science Framework at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using the freely available software [@R; @RStudio, see supplementary information for detailed software requirements]. Readers can also revisit any of the assumptions we make---e.g., changing parameterizations of our models, or substituting alternative models [see also @tan2021]. We welcome questions about the code, including the accompanying \texttt{R} library, and hope that it can support the development of experimental designs that decisively distinguish between competing mechanisms, and to facilitate the interpretation of existing results. 

Second, we present two case studies to provide an initial demonstratation *how* computational frameworks like ASP can move the field forward: we use ASP-based simulations to show that influential and highly-replicated signature findings previously interpreted as evidence for one of the three hypotheses about adaptive speech perception are, in fact, qualitatively just as compatible with *any* of the hypothesized mechanisms. The degree to which existing findings do not distinguish between competing theories surprised us, and we suspect that we are not alone in this. This demonstrates how ASP can also be used to inform and revise intuitions about the mechanisms underlying adaptive speech perception, and re-affirms the need identified in recent reviews to better characterize these mechanisms [e.g., @baeseberk2020; @kurumada-roettger2021; @johnson-sjerps2021; @quam-creel2021; @stilp2020; @weatherholtz-jaeger2016]. 

Finally, we discuss how computationally-guided behavioral research---in combination with neuroimaging---can be employed in future work to more productively decisively between alternative (combinations of) hypotheses about adaptive speech perception. We show that the empirical indeterminacy of many existing findings is due to the level of analysis chosen in those studies: with adequate designs and analyses, it will become possible to distinguish between competing hypotheses, and to estimate the involvement of multiple mechanisms more effectively. But first some background.

## The state of the field(s)
Research over the past decades has identified adaptive changes in speech perception to be a key component in listeners' ability to overcome cross-talker differences. Although speech perception can initially be slower and/or less accurate when listeners encounter an unfamiliar talkers with unexpected pronunciations, these processing difficulties tend to reduce with exposure [e.g., @bradlow-bent2008; @nygaard1994; @Perrachione2016; @sidaras2009; @wade2007; @weil2001a; @xie2021jep]. Remarkably, substantial improvements---such as reduced processing times or increased accuracy of recognition---can occur within minutes or less [@clarke-garrett2004; @munro-derwing1995; @xie2018jasa]. Even a context as brief as a single utterance can be sufficient to change perception and segmentation of ambiguous speech, blurring the distinction between processing and adaptation [e.g., categorizing a Dutch ambiguous /m?t/ embedded in an utterance with fast or slow speech as either "mat" or "maat" Bosker et al.,-@bosker2017; see also @kluender1988; @newman-sawusch1996; @sawusch-newman2000; @sjerps2011]. <!-- TO-DO: The rest of the para could potentially go? --> Changes in speech perception as a function of recent exposure have now been documented across a broad range of experimental paradigms and tasks [e.g., @bradlow-bent2008; @clayards2008; @eisner2013; @idemaru-holt2011; @norris2003; @vroomen2007], different modes of speech [e.g., "clear" and "conversational" speech, @zhang-samuel2014]; isolated and connected speech, [e.g., @eisner-mcqueen2005; @reinisch-holt2013], and languages [e.g., @chladkova2017; @eisner2013; @hanulikova-weber2012; @sebastian-galles2000; @schertz2015]. Changes can occur even under cognitive load, arguing for a relatively high degree of automaticity of some of the underlying mechanisms [@zhang-samuel2014; though both top-down attention-direction and interference can modulate adaptation, @mcaucliffe-babel2014; @samuel2016]. Yet, adaptive changes after only a few words of exposure can sometimes last for days if not longer [@eisner-mcqueen2006; @samuel2021; @xie2016; see also @goldinger1996]. 

In short, listeners' ability to adapt based on recent input is now considered a central part of human speech perception, and research over the last few decades has made tremendous progress in identifying its empirical properties and constraints [for comprehensive reviews, see @baeseberk2020; @johnson-sjerps2021; @tzeng2021; @quam-creel2021; @stilp2020]. Despite these substantial advances, however, **it remains unclear *how*---through what mechanisms---recent exposure comes to facilitate spoken language understanding.** This is not for a lack of theoretical proposals. Across the cognitive and neurosciences, there are now dozens of competing perspectives. However, contrastive comparisons between proposals have largely been lacking. Two inter-related factors seem to contribute to this: (1) many proposals---including some of our own past work---remain under-developed, constituting informal hypotheses rather than theories or models that lend themselves to strong contrastive tests,<!--TO-DO/DONE: comment out this footnote? ^[This has consequences beyond the ones we focus on here. Often multiple labels exist that seem to refer to the same general idea, without attempts to consolidate terminology (e.g., "perceptual recalibration", "phonetic retuning", and related combinations of these terms; "statistical learning" and "distributional learning"). It is also not uncommon that terminological usage seems to blur the distinction between *mechanism* and types of *paradigms* or the *phenomenon* that this paradigm is thought to study (e.g., "dimension-based statistical learning", "perceptual recalibration"). Yet other proposals seem to *describe effects* rather than mechanisms (e.g., "boundary shift") but what type of effect is observed (a boundary shift vs. an increase in accuracy) primarily reflects the researchers' choice of paradigm and analysis, without necessarily being a characteristic property of the underlying mechanisms. In our experience, this lack of clear distinctions between mechanisms and properties of experiments can confound newcomers to the field.]--> and (2) a tendency, in particular in behavioral research, to focus on identifying properties of adaptive speech perception rather than on distinguishing the underlying cognitive and neural architectures [but see @apfelbaum-mcmurray2015; @chodroff-wilson2020;  @hoffmanbion-escudero2007; @kleinschmidt-jaeger2015; @kiefte-nearey2019; @lehet-holt2020; @mcmurray-jongman2011; @xie2021cognition]. Separate lines of research often pursue, or simply (often implicitly) *assume*, one hypothesis at a time. In light of this, it is not surprising that reviews have for some time emphasized the need to better characterize the nature of the mechanisms underlying adaptive speech perception [see e.g., @baeseberk2020; @samuel-kraljic2009; @weatherholtz-jaeger2016]. This includes, in particular, questions about how the tasks demands inherent in different types of experimental paradigms---e.g., depending on whether experiments investigate relative simple phonetic manipulations or the full complexity of natural accents---affect the engagement of different mechanisms [@baeseberk2018; @zheng-samuel2020], and thus also which paradigms are most likely to shed light on the mechanisms affording flexible perception in everyday life.

The ASP framework presented in Section \@ref(sec:framework) is designed to help address these questions. We group existing proposals for adaptive speech perception into three types of theoretical perspectives (Figure \@ref(fig:overview)). The three proposals share with each other, and with all major theories in speech perception, general assumptions about how the acoustic input supports perception of a speech category. The process begins with (A) the extraction and normalization of acoustic/phonetic cues. These cues are (B) mapped onto and activate linguistic categories (such as phonemes, syllables, and/or words). Finally, (C) decision processes integrate the resulting category activations with contextual support and/or meta-reasoning (e.g., about the task), and recognition takes place. Existing proposals differ, however, in which of (A)-(C) explains *changes* in perception as a function of recent exposure. As we discuss next, it is these differences that entail fundamentally different cognitive and neural architectures. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\columnwidth]{`r get_path("../figures/diagrams/overview-of-three-mechanisms.png")`}
\caption{Listeners' recognition of speech categories are typically assumed to involve at least three types of mechanisms: 1) the acoustic input is transformed by low-level normalization processes into the perceptual cues that form the input to categorization, 2) linguistic representations describe the mapping between these perceptual and linguistic categories (here $C_1$ and $C_2$), and 3) decision-making mechanisms allow additional, stimulus-independent, biases to affect recognition. Any of these three mechanisms can in theory be affected by recent experience. While it is now clear \emph{that} recent experience changes the processing of subsequent speech input, most existing findings leave open \emph{which (combination) of the three mechanisms underlie these changes} and \emph{how the involvement of different mechanisms depends on, for instance, stimulus properties, task demands, and individual differences between listeners}.}\label{fig:overview}
\end{center}
\end{figure}

The majority of recent behavioral research on talker-related adaptation has focused on the middle layer, **changes in linguistic representations**. This includes proposals that attribute exposure effects to "boundary re-tuning/shift” [e.g., @norris2003; @reinisch2013], “perceptual/category recalibration” [e.g., @kraljic-samuel2006; @reinisch-holt2013; @samuel2016; @vroomen-baart2009], "perceptual retuning" [@jesse-mcqueen2011; @mcqueen2006; @mitterer2013], "category shift" [@lindsay2022; @sawusch-pisoni1976]; “category expansion" [@schmale2012], “dimension-based statistical learning” [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015], or "criteria relaxation" [@zheng-samuel2020]. While these proposals are often not further formally specified or modeled [for notable exceptions, see @apfelbaum-mcmurray2015; @clayards2008; @kleinschmidt-jaeger2015; @lancia-winter2013; @xie2021cognition], they all describe types of changes in representations. For example, "category shift" refers to a change in the mean of the cue distribution corresponding to a category, and "category expansion" refers to increases in the variance of that distribution. 

One reason that the idea of representational changes has received so much attention is that it is of high theoretical relevance: representational changes as a function of recent exposure are predicted by influential exemplar [@apfelbaum-mcmurray2015; @johnson2006], episodic [@goldinger1998], Bayesian inference [@kleinschmidt-jaeger2015], or neural network models [@lancia-winter2013]. All of these theories predict that listeners can learn and store talker- or accent-specific representations [see also @bradlow-bent2008; @baeseberk2013;  @tzeng2016]. Perhaps unsurprisingly, this idea---continued implicit learning of new representations throughout their adult life---has been influential beyond speech perception. The possibility that listeners learn and maintain linguistic representations for multiple types of talkers has influenced psycholinguistic research on, for example, lexical, sentence, and semantic/pragmatic processing [@schuster-degen2020; @fine2013; @fraundorf-jaeger2016; @kaschak2004; @pogue2016; @prasad2021; @ryskin2019; @yildirim2016; @wells2009; for review, see @brown-schmidt2015; @chang2012] and research on second language learning [for reviews, see @kaan-chun2018; @pajak2016]. It has shaped linguistic theories [e.g., @bybee2001; @goldinger-azuma2004; @hay2019; @magnuson-nusbaum2007; @pierrehumbert2001] as well as theories about the interface between social and linguistic cognition [e.g., @babel2019; @creel-bregman2011; @foulkes-hay2015; @hanulikova2012; @sumner2014]. Further illustrating the influence of this idea, several recent reviews go as far as to discuss *what types* of representational changes underlie the effects of recent exposure (e.g., "category expansion" vs. "category shifts"), rather than *whether* representational changes are the actual mechanism underlying the observed results [e.g., @baeseberk2020; @bent-baeseberk2021; @schertz-clare2020].

<!--TO DO: INTEGRATE IN  NEXT PARA: “normalization” [@johnson1990; @pisoni1977],-->
In the absence of contrastive tests, however, the same behavioral results that have been interpreted as arguing for representational changes could *in principle* be explained by computationally more parsimonious mechanisms that do *not* assume changes in linguistic representations. For instance, proposals dating back to at least the 1970s hold that adaptive changes in perception can be due to low-level, automatic (involuntary) **normalization** during the early stages of auditory processing (bottom of Figure \@ref(fig:overview)). These processes are thought to be pre-linguistic in that they do not refer to categories but rather apply to the acoustic or phonetic cues [for reviews, @johnson-sjerps2021; @stilp2020]. In contrast to the assumption that cross-talker variability may be learned and stored, which is fundamental to all the theories of representational changes as described above, normalization proposals assume that listeners remove talker variability prior to mapping cues to linguistic categories. More recently, evidence from neuroimaging studies has lent support for low-level normalization mechanisms by showing engagement of sub-cortical structures in sensory adaptation [e.g., the brain stem, @skoe2021auditory; and cerebellum, @guediche2015evidence; for review, see @guediche2014]. This leaves open, however, whether normalization can indeed explain the type of phenomena that have been attributed to changes in representations. If it can, this would undermine the empirical foundation for the computational complexity and cognitive abilities implied by theories of representational changes.^[It would also indicate a need to revisit the interpretation of findings that speech perception can be affected by the (inferred) social identity of a talker [e.g., regional origin, @hay-drager2010; @niedzielski1999; sex, @johnson1999; @strand1999; age, @skoogwaller2015; @walker-hay2011; and individual identity, @nygaard1994; @remez2018]. In sociophonetics and related fields, these findings are routinely attributed to talker-specific storage of *linguistic representations*, without considering alternative explanations in terms of normalization. While early normalization accounts that were limited to correction for physiology were rather convincingly rejected by cross-linguistic comparisons [@johnson2006], modern theories of normalization like the ones we discuss in Section \@ref(sec:framework) are not subject to those arguments.] <!-- TO DO: Xin, Chigusa, I moved this here from the GD. Basically, just to get it out of the way since I felt it was stranded where it was. If that's ok, can you delete this comment? -->

Another alternative explanation for adaptive speech perception that does not refer to changes in representations are *post-linguistic* mechanisms (top of Figure \@ref(fig:overview)). Just like normalization, this explanation is computationally more parsimonious than changes in representations. And just like normalization is broadly accepted to be part of speech perception, there is little doubt that **changes in decision and response biases** can affect listeners' interpretation of speech input, or at least the responses they give within experiments. For instance, Clarke-Davidson et al. [-@clarkedavidson2008, p.605] define a bias as "the increased likelihood to give a particular response---such as /s/---given any acoustic input, or the need for less evidence for a particular response" that "would help participants make faster word decisions in [...] ambiguous cases". In behavioral research, such biases continue to be rarely considered when interpreting the effects of recent exposure. *When* response biases have been considered [e.g., as "response equilibration" in Vroomen and Baart -@vroomen-baart2009], this explanation tends to be dismissed. One reason for this might be that particular result patterns---such as boundary shifts in perceptual recalibration or improved categorization or transcription accuracy in accent adaptation---are taken to rule out changes in response biases (or normalization, for that matter). However, to anticipate one take-home point from the case studies we present below, we find that response biases can explain more complex changes in behavior than has previously been assumed, including the very signature results that are often assumed to be incompatible with response bias.

In contrast to behavioral research, changes in decision-making have received substantial attention in neuroimaging research. Recent studies have investigated whether adaptive changes in speech perception primarily recruit areas associated with decision-making [@myers-mesite2014; @erb2013brain]---such as prefrontal areas [e.g., @binder2004neural; @thompson1997role] as well as the insula and parietal cortex [e.g., @furl2011parietal; @keuken2014]---as opposed to cortical areas associated with phonetic representations [@bonte2017; @luthra2020a]. For instance, @myers-mesite2014 found that sensitivity to category shifts between [s] and [`r linguisticsdown::cond_cmpl("ʃ")`] emerged in right frontal and middle temporal regions, implicating adjustments of decision-related or attentional criteria downstream from functions performed by primary auditory cortex. Similarly, a recent study found that adaptation to phoneme category substitutions (e.g., [s] pronounced as [`r linguisticsdown::cond_cmpl("ʃ")`]) engages prefrontal regions, responsible for post-perceptual repair mechanisms [@blanco-elorriera2021]: simply put, listeners seem to initially map the acoustic input onto the wrong category and subsequently corrects the mapping. This was contrasted with the hypothesis that adaptation occurs via retuning of the lower-level functional connections that would be associated with perceptual processing (e.g., between the superior temporal gyrus and primary auditory cortex). 

In summary, different lines of research have focused on different types of explanations for adaptive speech perception observed in different types of paradigms. Each of these research lines has found that the mechanisms under investigation can qualitatively explain behavioral and/or neural activation patterns observed in *some* paradigm. Comparisons between competing hypotheses---within or across paradigms---are, however, still lacking.^[To the extent that behavioral research has begun to contrast competing hypotheses, these efforts have focused on (A) normalization and (B) representational changes while mostly ignoring the possibility of (C) post-perceptual decision-making. Compared to the behavioral research, it is more common in neuroimaging work to directly contrast hypotheses about different mechanisms---operationalized as differential activation of different brain areas and networks that are associated with different functionality. In contrast to behavioral work, neuroimaging research tends to not group hypotheses (A) and (B) together as functionally distinct from higher-level, decision-related mechanisms further downstream (C). <!-- Neuroimaging work therefore does not (yet) distinguish between hypothesis (A) and (B). -->] The upshot of this is that we neither know for any given paradigm whether its results clearly argue for one over another mechanisms, nor whether the results of different paradigms all point to the same (combination of) mechanisms. 

One of the immediate goals of the present study is to demonstrate how frameworks like ASP can help advance research on adaptive speech perception. An overview of our approach is provided in Figure \@ref(fig:overview-change). We introduce the ASP framework, along with its separate *change models* for normalization, representational changes, and changes in decision-making. By switching these models off and on, we can simulate the effects of recent exposure during an experiment under competing hypotheses about adaptive speech perception. This allows us to assess whether an experimental finding is compatible with any of the changes models (A)-(C), and whether that result is evidence for any of the mechanisms over the other mechanisms---i.e., whether some of the mechanisms can*not* explain the result. Using this approach, we present case studies for two highly influential experimental paradigms: **perceptual recalibration** paradigms [e.g., @kraljic-samuel2006; @reinisch-holt2013; @samuel2016; @vroomen-baart2009] and experiments on **(non-native) accent adaptation** [e.g., @bradlow-bent2008; @hernandez2019; @tzeng2016; @sidaras2009; @xie2016jep; @zheng-samuel2020]. For each, we find that result patterns often taken as signature evidence of one mechanism (e.g., changes of representations) are, in fact, predicted by *any of the three mechanisms*. This shows---we believe, for the first time---that, at the present level of data analysis, some of the most commonly used paradigms in speech perception do not distinguish between fundamentally different hypotheses about the mechanisms underlying adaptive speech perception. We conclude with recommendations for future work that are informed by these simulations, and anticipate future uses of ASP that are beyond the scope of the present work. For example, our case studies deliberately only consider ASP models that exclusively employ one of the three change mechanisms at a time. It is, however, also possible---or, in fact, likely---that adaptive speech perception draws on *combinations* of all three mechanisms, and that the engagement of the three mechanisms depends on a variety of factors, including stimulus properties (e.g., whether an L2 accent involves simple shifts relative to the L1 accent, or the learning of new features), task demands (e.g., attentional load), and individual differences between listeners. ASP can facilitate future research on these questions.

\begin{figure}[h]
\begin{center}
\includegraphics[width=.99\columnwidth]{`r get_path("../figures/diagrams/overview-of-changes.png")`}
\caption{Overview of our approach. Experiments on the effect of recent exposure on subsequent speech perception tend to involve two phases. An exposure phase manipulates the statistics of speech input between participants. A subsequent test phase assesses the effects of those manipulations on the interpretation of identical speech input. We seek to understand what type of exposure effects each of the three mechanisms in Figure \ref{fig:overview} can explain. To this end, we specify both a) a {\em categorization model} that describes the interpretation of speech input at any given moment (vertical information flow) and b) {\em change models} for all three mechanisms that describe how these parts of the categorization model change as a function of exposure (horizontal information flow). We then use the different change models to compare the predicted consequences of changes to normalization, representations, or response biases.}\label{fig:overview-change}
\end{center}
\end{figure}



