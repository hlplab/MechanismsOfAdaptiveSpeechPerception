\newpage 
\setcounter{page}{1}

```{r, include=FALSE}
max.f0 <- 350            # max raw f0
min.observation.n <- 25  # min observation per stop
max.p <- .1              # cut-off for rejection of unimodality

d.chodroff_wilson <- read_csv(get_path("../data/Chodroff-Wilson-2018/all_observations_with_non-missing_vot_cog_f0.csv")) %>%
  rename(category = stop, VOT = vot, f0 = usef0, Talker = subj, Word = word, Trial = trial, Vowel = vowel) %>%
  mutate(
    category = 
      plyr::mapvalues(
        category,
        c("B", "D", "G", "P", "T", "K"),
        c("/b/", "/d/", "/g/", "/p/", "/t/", "/k/")),
    gender = factor(
      plyr::mapvalues(
        gender, 
        c("F", "M"),
        c("female", "male")),
      levels = c("male", "female")),
    poa = factor(
      plyr::mapvalues(
        poa, 
        c("lab", "cor", "dor"),
        c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
      levels = c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
    voicing = factor(
      ifelse(category %in% c("/b/", "/d/", "/g/"), "yes", "no"),
      levels = c("yes", "no"))) %>%
  mutate(across(c(Talker, Word, gender, category), factor)) %>%
  select(Talker, Word, Trial, Vowel, gender, category, poa, voicing, VOT, f0)

# Filter f0 above 500 Hz
d.chodroff_wilson %<>%
  filter(f0 < max.f0)

# Keep only subjects with at last n.min observations for each stop
d.chodroff_wilson %<>%
  group_by(Talker, category) %>%
  mutate(n = length(category)) %>%
  group_by(Talker) %>%
  mutate(n = ifelse(any(is.na(n)), 0, min(n))) %>%
  ungroup() %>%
  filter(n > min.observation.n)

# Identify and remove talkers with bimodal f0 distributions
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(f0_Mel = phonR::normMel(f0)) %>%
  group_by(Talker, category) %>%
  mutate(
    f0.multimodal = dip.test(f0)$p.value < .1,
    f0_Mel.multimodal = dip.test(f0_Mel)$p.value < .1) %>%
  filter(!f0.multimodal, !f0.multimodal) %>%
  droplevels()

# Get Mel and Semitones, then C-CuRE
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(
    f0_Mel = phonR::normMel(f0),
    f0_semitones = 12 * log(f0 / mean(f0)) / log(2)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .)))
```

# Introduction
How human listeners are able to infer meaning from speech is a central question in the cognitive and neurosciences. The computational complexity of spoken language understanding becomes most apparent when a talker's pronunciations---and thus the mapping of acoustic input to linguistic categories and meaning---strongly deviate from listeners' expectations. This might occur, for example, when listening to a talker with an unfamiliar regional or non-native accent, or a patient with apraxia or dysarthria. The same fundamental challenge is, however, present even during seemingly effortless comprehension. Even among talkers who share similar language backgrounds, the mapping between the acoustic input and linguistic categories can vary substantially between talkers due to both physiology (e.g., vocal tract size and shape) and socio-cultural factors (e.g., social identity and language background). As a consequence, one talker’s pronunciation of, say, the sound category [s] (as in *sip*) can be acoustically more similar to another talker’s production of [`r linguisticsdown::cond_cmpl("ʃ")`] [as in *ship*, Newman et al., -@newman2001]. How we manage to understand each other despite such cross-talker differences has remained one of the perennial puzzles in research on speech perception [where it is known as part of the infamous *lack of invariance* problem, @liberman1967]. 

This article presents a general computational framework---which we refer to as *ASP* for *adaptive speech perception*---that helps address this question. ASP grew out of our long-term goal to contribute to the development of stronger theories, affording decisive comparisons between alternative hypotheses about the mechanisms underlying speech perception [in the tradition of "strong inference" approaches to scientific inquiry, @platt1964]. We identify the three most influential hypotheses about the mechanisms that allow listeners to overcome cross-talker variability (Figure \@ref(fig:overview)). The three hypotheses---described below---entail fundamentally different cognitive abilities and neural architectures, with far-reaching consequences for theories of speech perception, linguistics, and the malleability of neural representations more generally. ASP is the first to formalize and implement all three of these mechanisms in a common computational framework. This responds to a strong need identified in recent reviews to better characterize the mechanisms of adaptive speech perception [e.g., @baeseberk2020; @kurumada-roettger2021; @johnson-sjerps2021; @quam-creel2021; @stilp2020; @weatherholtz-jaeger2016]. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\columnwidth]{`r get_path("../figures/diagrams/overview-of-three-mechanisms.png")`}
\caption{Listeners' recognition of speech categories are typically assumed to involve at least three types of mechanisms: 1) the acoustic input is transformed by low-level normalization processes into the perceptual cues that form the input to categorization, 2) linguistic representations describe the mapping between these perceptual and linguistic categories (here $C_1$ and $C_2$), and 3) decision-making mechanisms allow additional, stimulus-independent, biases to affect recognition. Any of these three mechanisms can in theory be affected by recent experience. While it is now clear \emph{that} recent experience changes the processing of subsequent speech input, most existing findings leave open \emph{which (combination) of the three mechanisms underlie these changes} and \emph{how the involvement of different mechanisms depends on, for instance, stimulus properties, task demands, and individual differences between listeners}.}\label{fig:overview}
\end{center}
\end{figure}

While the three hypotheses in Figure \@ref(fig:overview) have largely been pursued in separate lines of research, they are not mutually exclusive. <!-- Indeed, with a phenomenon as complex as human speech perception, it is most plausible that multiple mechanisms are at play.--> This raises questions as to whether adaptive speech perception is the result of multiple mechanisms, and how the relative engagement of these mechanisms depends on task demands, stimulus properties, or individual differences between listeners. Just as ASP can be used for *sufficiency tests*---whether any of the three mechanisms is sufficient to explain adaptive speech perception (or types thereof)---it can also be used to assess the *relative engagement of the three mechanisms*---i.e., what weighted combination of the three mechanisms best explains a given phenomenon. We anticipate that, ultimately, the latter capability of ASP will be critical in advancing the field. Future work could use the ASP framework to, for example, investigate whether specific instances of impaired adaptation [as seen, e.g., in children with dyslexia, @gabay2021; @ozernov-palchik2021] arise from auditory, linguistic, or cognitive sources---an important prerequisite for devising more effective interventions/treatments. 

For the present article, we have three more immediate goals. The first part of this article introduces the ASP framework, along with visual demonstrations and animations. We take a deliberately tutorial-like approach so to help researchers apply ASP to their own experiments. All data and code for this article can be downloaded from the Open Science Framework at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). The article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [@R; @RStudio, see supplementary information for detailed software requirements]. Readers can also revisit any of the assumptions we make---e.g., changing parameterizations of our models, or substituting alternative models [see also @tan2021]. We welcome questions about the code, including the accompanying \texttt{R} library. 

Second, we demonstrate one way in which computational frameworks like ASP can advance theories of speech perception. We simulate human recognition for two influential experimental paradigms---*perceptual recalibration* paradigms [e.g., @kraljic-samuel2006; @reinisch-holt2013; @samuel2016; @vroomen-baart2009] and experiments on *(non-native) accent adaptation* [e.g., @bradlow-bent2008; @hernandez2019; @tzeng2016; @sidaras2009; @xie2016jep; @zheng-samuel2020]---under each of the three hypothesized mechanisms. We find that, contrary to common assumptions, the signature findings of these two paradigms are qualitatively compatible with *any* of the three hypothesized change mechanisms. Beyond the immediate consequences for research on perceptual recalibration and accent adaptation, this demonstrates how ASP can inform and revise intuitions about the mechanisms underlying adaptive speech perception. The R code for our case studies further serves as a template for experimenters interested in understanding the theoretical implications of their results while reducing the need for ad-hoc reasoning. <!-- TO DO: too aggressive? -->

Finally, we discuss how computationally-guided behavioral research---in combination with neuroimaging---can be employed in future work. We show that the empirical indeterminacy of many existing findings is due to the level of analysis chosen in those studies. We propose new standards of experimental design and data analysis that will more effectively distinguish between competing hypotheses about the relative involvement of different mechanisms. But first some background.

## The state of the field(s)
Research over the past decades has identified adaptive changes in speech perception to be a key component in listeners' ability to overcome cross-talker differences. Although speech perception can initially be slower and/or less accurate when listeners encounter an unfamiliar talker with unexpected pronunciations, these processing difficulties tend to reduce with exposure [e.g., @bradlow-bent2008; @nygaard1994; @Perrachione2016; @sidaras2009; @wade2007; @weil2001a; @xie2021jep]. Remarkably, substantial improvements---such as reduced processing times or increased accuracy of recognition---can occur within minutes or less [@clarke-garrett2004; @munro-derwing1995; @xie2018jasa]. Even a context as brief as a single utterance can be sufficient to change perception and segmentation of ambiguous speech, blurring the distinction between processing and adaptation [e.g., categorizing a Dutch ambiguous /m?t/ embedded in an utterance with fast or slow speech as either "mat" or "maat" Bosker et al.,-@bosker2017; see also @kluender1988; @newman-sawusch1996; @sawusch-newman2000; @sjerps2011]. Yet, adaptive changes after only a few words of exposure can sometimes last for days if not longer [@eisner-mcqueen2006; @samuel2021; @xie2016; see also @goldinger1996].
<!-- Changes in speech perception as a function of recent exposure have now been documented across a broad range of experimental paradigms and tasks [e.g., @bradlow-bent2008; @clayards2008; @eisner2013; @idemaru-holt2011; @norris2003; @vroomen2007]. different modes of speech [e.g., "clear" and "conversational" speech, @zhang-samuel2014], isolated and connected speech, [e.g., @eisner-mcqueen2005; @reinisch-holt2013], and languages [e.g., @chladkova2017; @eisner2013; @hanulikova-weber2012; @sebastian-galles2000; @schertz2015]. Changes can occur even under cognitive load, arguing for a relatively high degree of automaticity of some of the underlying mechanisms [@zhang-samuel2014; though both top-down attention-direction and interference can modulate adaptation, @mcauliffe-babel2016; @samuel2016]. Adaptive changes after only a few words of exposure can sometimes last for days if not longer [@eisner-mcqueen2006; @samuel2021; @xie2016; see also @goldinger1996]. -->

Listeners' ability to adapt based on recent input is now considered a central part of human speech perception, and research over the last few decades has made substantial progress in identifying its empirical properties and constraints [for comprehensive reviews, see @baeseberk2020; @johnson-sjerps2021; @tzeng2021; @quam-creel2021; @stilp2020]. Despite these substantial advances, however, **it remains unclear *how*---through what mechanisms---recent exposure comes to facilitate spoken language understanding.** This is not due to a lack of theoretical proposals. Across cognitive sciences and neurosciences, there are now dozens of competing perspectives. However, contrastive comparisons between proposals have largely been lacking. Two inter-related factors seem to contribute to this: (1) many proposals---including some of our own past work---remain under-specified, constituting informal hypotheses rather than theories or models that lend themselves to strong contrastive tests; <!-- ^[This has consequences beyond the ones we focus on here. Often multiple labels exist that seem to refer to the same general idea, without attempts to consolidate terminology (e.g., "perceptual recalibration", "phonetic retuning", and related combinations of these terms; "statistical learning" and "distributional learning"). It is also not uncommon that terminological usage seems to blur the distinction between *mechanism* and types of *paradigms* or the *phenomenon* that this paradigm is thought to study (e.g., "dimension-based statistical learning", "perceptual recalibration"). Yet other proposals seem to *describe effects* rather than mechanisms (e.g., "boundary shift") but what type of effect is observed (a boundary shift vs. an increase in accuracy) primarily reflects the researchers' choice of paradigm and analysis, without necessarily being a characteristic property of the underlying mechanisms. In our experience, this lack of clear distinctions between mechanisms and properties of experiments can confound newcomers to the field.]--> (2) there is a tendency, particularly in behavioral research, to focus on characterizing properties of adaptive speech perception rather than on identifying the underlying cognitive and neural architectures [but see @apfelbaum-mcmurray2015; @chodroff-wilson2020;  @hoffmanbion-escudero2007; @kleinschmidt-jaeger2015; @kiefte-nearey2019; @lehet-holt2020; @mcmurray-jongman2011; @xie2021cognition]. As a consequence, different hypotheses are often *assumed* rather than tested, with different lines of research making different (implicit) assumptions, sometimes co-existing for decades without targeted attempts to contrast the predictions that would follow from those assumptions. <!-- This bias away from theory-contrasting tests is also reflected in the titles of papers, which tend to name the *phenomenon* or *paradigm* ("dimension-based statistical learning", "phonetic retuning", "selective adaptation", "accent adaptation", etc.), rather than competing hypotheses about mechanism. -->
Neuroimaging research has, on the whole, been more invested into contrastive comparisons [e.g., @bonte2017; @erb2013brain; @guediche2015evidence; @myers-mesite2014]: questions about the involvement of different types of information processing---operationalized as differential activation of different brain areas and networks that are associated with different functionality---are the bread and butter of neuroimaging. This approach has identified a wide range of brain regions as involved in different aspects of adaptive speech perception, ranging from subcortical areas [@REFS] to networks associated with decision-making [@REFS]. This leaves open, however, what types of computations underlie the observed differential activations, or why different types of exposure lead to different types of behavioral changes. In light of these gaps in our understanding, recent reviews continue to emphasize the need to better characterize the nature of the mechanisms underlying adaptive speech perception [from, e.g., @samuel-kraljic2009; to @weatherholtz-jaeger2016; to @baeseberk2020]. This includes questions about how the tasks demands of different types of experimental paradigms affect the engagement of different mechanisms [@baeseberk2018; @zheng-samuel2020], and thus also which paradigms are most likely to shed light on the mechanisms affording flexible perception in everyday life.

The ASP framework we present in Section \@ref(sec:framework) is designed to help address these questions. We group existing proposals for adaptive speech perception into three types of theoretical perspectives (Figure \@ref(fig:overview)). The three proposals share with each other, and with all major theories in speech perception, general assumptions about how the acoustic input supports perception of a speech category. The process begins with (A) the extraction and normalization of acoustic/phonetic cues. These cues are (B) mapped onto and activate linguistic categories (such as phonemes, syllables, and/or words). Finally, (C) decision processes integrate the resulting category activations with contextual support and/or meta-reasoning (e.g., about the task), and recognition takes place. Existing proposals differ, however, in which of (A)-(C) explains *changes* in perception as a function of recent exposure. As we discuss next, it is these differences that entail fundamentally different cognitive and neural architectures. 

The majority of recent behavioral research on talker-related adaptation has focused on the middle layer, **changes in linguistic representations**. This includes proposals that attribute exposure effects to "boundary re-tuning/shift" [e.g., @norris2003; @reinisch2013], "perceptual/category recalibration" [e.g., @kraljic-samuel2006; @reinisch-holt2013; @samuel2016; @vroomen-baart2009], "perceptual retuning" [@jesse-mcqueen2011; @mcqueen2006; @mitterer2013], "category shift" [@lindsay2022; @sawusch-pisoni1976]; "category expansion" [@schmale2012], "dimension-based statistical learning" [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015], or "criteria relaxation" [@zheng-samuel2020]. While these proposals are often not further formally specified or modeled [for notable exceptions, see @apfelbaum-mcmurray2015; @clayards2008; @kleinschmidt-jaeger2015; @lancia-winter2013; @xie2021cognition], they all describe types of changes in representations. For example, "category shift" refers to a change in the mean of the cue distribution corresponding to a category, and "category expansion" refers to increases in the variance of that distribution. 

One reason the idea of representational changes has received so much attention is that it is of high theoretical relevance. Representational changes as a function of recent exposure are predicted by influential exemplar [@apfelbaum-mcmurray2015; @johnson2006], episodic [@goldinger1998], Bayesian inference [@kleinschmidt-jaeger2015], or neural network models [@lancia-winter2013]. All of these theories predict that listeners can learn and store talker- or accent-specific representations [see also @bradlow-bent2008; @baeseberk2013;  @tzeng2016]. Perhaps unsurprisingly, this idea---continued implicit learning of new representations throughout their adult life---has been influential beyond speech perception. The possibility that listeners learn and maintain linguistic representations for multiple types of talkers has influenced psycholinguistic research on lexical, sentence, and semantic/pragmatic processing [@chang2012; @fine2013; @kaschak2004; @pogue2016; @prasad2021; @ryskin2019; @schuster-degen2020; <!--@yildirim2016; @wells2009;--> for review, see @brown-schmidt2015] and research on second language learning [for reviews, see @kaan-chun2018; @pajak2016]. It has shaped linguistic theories [e.g., @bybee2001; @goldinger-azuma2004; @hay2019; @magnuson-nusbaum2007; @pierrehumbert2001] as well as theories about the interface between social and linguistic cognition [e.g., @babel2019; @creel-bregman2011; @foulkes-hay2015; @hanulikova2012; @sumner2014]. Further illustrating the influence of this idea, several recent reviews have gone as far as to discuss *what types* of representational changes underlie the effects of recent exposure (e.g., "category expansion" vs. "category shifts"), rather than *whether* representational changes are indeed necessary explain adaptive speech perception [e.g., @baeseberk2020; @bent-baeseberk2021; @schertz-clare2020].

In the absence of contrastive tests, however, the same behavioral results that have been interpreted as arguing for representational changes could in principle be explained by computationally more parsimonious mechanisms that do *not* assume changes in linguistic representations. For instance, proposals dating back to at least the 1970s hold that adaptive speech perception is due to low-level, automatic (involuntary) **normalization** during the early stages of auditory processing (bottom of Figure \@ref(fig:overview)). These processes are thought to be pre-linguistic in that they do not refer to categories but rather apply to the acoustic or phonetic cues [for reviews, @johnson-sjerps2021; @stilp2020]. In contrast to the assumption that cross-talker variability may be learned and stored, which is fundamental to all the theories of representational changes as described above, normalization proposals assume that listeners remove talker variability prior to mapping cues to linguistic categories. More recently, evidence from neuroimaging studies has lent support for low-level normalization mechanisms by showing engagement of subcortical structures in sensory adaptation [e.g., the brain stem, @skoe2021auditory; and cerebellum, @guediche2015evidence; for review, see @guediche2014]. This leaves open, however, whether normalization can indeed explain talker-related adaptation that have been attributed to changes in representations. If it can, this would undermine the empirical foundation for the computational complexity and cognitive abilities implied by theories of representational changes.^[It would also indicate a need to revisit the interpretation of findings that speech perception can be affected by the (inferred) social identity of a talker [e.g., regional origin, @hay-drager2010; @niedzielski1999; sex, @johnson1999; @strand1999; age, @skoogwaller2015; @walker-hay2011; and individual identity, @nygaard1994; @remez2018]. In sociophonetics and related fields, these findings are routinely attributed to talker-specific storage of *linguistic representations*, without considering alternative explanations in terms of normalization. While early normalization accounts that were limited to correction for physiology were rather convincingly rejected by cross-linguistic comparisons [@johnson2006], modern theories of normalization like the ones we discuss in Section \@ref(sec:framework) are not subject to those arguments.] 

A third explanation for adaptive speech perception that does not refer to changes in representations are *post-linguistic* mechanisms (top of Figure \@ref(fig:overview)). Just like normalization, this explanation is computationally more parsimonious than changes in representations. And just like normalization is broadly accepted to be part of speech perception, there is little doubt that **changes in decision and response biases** can affect listeners' interpretation of speech input, or at least the responses they give within experiments. For instance, Clarke-Davidson et al. [-@clarkedavidson2008, p.605] define a bias as "the increased likelihood to give a particular response---such as /s/---given any acoustic input, or the need for less evidence for a particular response" that "would help participants make faster word decisions in [...] ambiguous cases". In behavioral research, such biases continue to be rarely considered when interpreting the effects of recent exposure. *When* response biases have been considered [e.g., as "response equilibration" in Vroomen and Baart -@vroomen-baart2009], this explanation tends to be dismissed. One reason for this might be that particular result patterns---such as boundary shifts in perceptual recalibration or improved categorization or transcription accuracy in accent adaptation---are taken to rule out changes in response biases (or normalization, for that matter). However, to anticipate one take-home point from the case studies we present below, we find that response biases can explain more complex changes in behavior than has previously been assumed, including the very signature results that are often assumed to be incompatible with changes in response bias.

In contrast to behavioral research, changes in decision-making have received substantial attention in neuroimaging research. Recent studies have investigated whether adaptive changes in speech perception primarily recruit areas associated with decision-making [@myers-mesite2014; @erb2013brain]---such as prefrontal areas [e.g., @binder2004neural; @thompson1997role] as well as the insula and parietal cortex [e.g., @furl2011parietal; @keuken2014]---as opposed to cortical areas associated with phonetic representations [@bonte2017; @luthra2020a]. For instance, @myers-mesite2014 found that sensitivity to category shifts between [s] and [`r linguisticsdown::cond_cmpl("ʃ")`] emerged in right frontal and middle temporal regions, implicating adjustments of decision-related or attentional criteria downstream from functions performed by primary auditory cortex.^[Another recent study found that adaptation to phoneme category substitutions (e.g., [s] pronounced as [`r linguisticsdown::cond_cmpl("ʃ")`]) engages prefrontal regions, which is responsible for post-perceptual repair mechanisms [@blanco-elorriera2021]: listeners seem to initially map the acoustic input onto the wrong category and subsequently corrects the mapping. This was contrasted with the hypothesis that adaptation occurs via retuning of the lower-level functional connections that would be associated with perceptual processing (e.g., between the superior temporal gyrus and primary auditory cortex). Whether these findings generalize to adaptive perception of *phonetic* differences between talkers is an open question.]

In summary, different lines of research have focused on different types of explanations for adaptive speech perception observed in different types of paradigms. Comparisons between competing hypotheses about the types of computations that underlie adaptive speech perception---within or across paradigms---remain lacking.<!--^[To the extent that behavioral research has begun to contrast competing hypotheses, these efforts have been limited to (A) normalization and (B) representational changes while ignoring the possibility of (C) post-perceptual decision-making. In contrast to behavioral work, neuroimaging research has focused more on the contrast between (C) vs. (A) and (B), grouping the latter together as functionally distinct from higher-level decision-related processes [but see @REF-XIN-DOES-SOMETHING-COME-TO-MIND].]--> The upshot of this is that we often neither know whether the results of a given paradigm clearly argue for one over another mechanisms, nor whether the results of different paradigms all point to the same (combination of) mechanisms. The computational framework that we introduce next provides a way for future work to address these questions. 

<!--The computational framework that we introduce next (ASP) provides a way to address these questions (Section \@ref(sec:framework)). We demonstrate the use of ASP in two case studies (Sections \@ref(sec:PR) and \@ref(sec:AA)).  These case studies show---we believe, for the first time---that, at the present level of data analysis, some of the most commonly used paradigms in speech perception indeed do not distinguish between fundamentally different hypotheses about the mechanisms underlying adaptive speech perception. Our case studies deliberately only consider ASP models that exclusively employ one of the three change mechanisms at a time---i.e., models that assess the *sufficiency* of each mechanism. In the general discussion (Section \@ref(sec:general-discussion)), we return to this point and discuss how future work can take advantage of the respective computational limitations of each mechanism (derived in Section \@ref(sec:framework)) to provide more decisive tests of each mechanism's sufficiency. However, it is possible---or rather, *expected* given a careful review of existing findings---that adaptive speech perception draws on all three mechanisms. We thus close by discussing how future ASP modeling can help determine how the relative engagement of the three mechanisms depends on, e.g., task demands (e.g., attentional load), stimulus properties (e.g., whether an L2 accent involves simple shifts relative to the L1 accent, or the learning of new features), and individual differences between listeners. -->
