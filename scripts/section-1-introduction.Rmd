```{r, include=FALSE}
max.f0 <- 350            # max raw f0
min.observation.n <- 25  # min observation per stop
max.p <- .1              # cut-off for rejection of unimodality

d.chodroff_wilson <- read_csv(get_path("../data/Chodroff-Wilson-2018/all_observations_with_non-missing_vot_cog_f0.csv")) %>%
  rename(category = stop, VOT = vot, f0 = usef0, Talker = subj, Word = word, Trial = trial, Vowel = vowel) %>%
  mutate(
    category = 
      plyr::mapvalues(
        category,
        c("B", "D", "G", "P", "T", "K"),
        c("/b/", "/d/", "/g/", "/p/", "/t/", "/k/")),
    gender = factor(
      plyr::mapvalues(
        gender, 
        c("F", "M"),
        c("female", "male")),
      levels = c("male", "female")),
    poa = factor(
      plyr::mapvalues(
        poa, 
        c("lab", "cor", "dor"),
        c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
      levels = c("/b/-/p/", "/d/-/t/", "/g/-/k/")),
    voicing = factor(
      ifelse(category %in% c("/b/", "/d/", "/g/"), "yes", "no"),
      levels = c("yes", "no"))) %>%
  mutate(across(c(Talker, Word, gender, category), factor)) %>%
  select(Talker, Word, Trial, Vowel, gender, category, poa, voicing, VOT, f0)

# Filter f0 above 500 Hz
d.chodroff_wilson %<>%
  filter(f0 < max.f0)

# Keep only subjects with at last n.min observations for each stop
d.chodroff_wilson %<>%
  group_by(Talker, category) %>%
  mutate(n = length(category)) %>%
  group_by(Talker) %>%
  mutate(n = ifelse(any(is.na(n)), 0, min(n))) %>%
  ungroup() %>%
  filter(n > min.observation.n)

# Identify and remove talkers with bimodal f0 distributions
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(f0_Mel = phonR::normMel(f0)) %>%
  group_by(Talker, category) %>%
  mutate(
    f0.multimodal = dip.test(f0)$p.value < .1,
    f0_Mel.multimodal = dip.test(f0_Mel)$p.value < .1) %>%
  filter(!f0.multimodal, !f0.multimodal) %>%
  droplevels()

# Get Mel and Semitones, then C-CuRE
d.chodroff_wilson %<>%
  group_by(Talker) %>%
  mutate(
    f0_Mel = phonR::normMel(f0),
    f0_semitones = 12 * log(f0 / mean(f0)) / log(2)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .)))
```

# Introduction
How human listeners understand speech is a central question in cognitive science. The computational complexity of spoken language understanding becomes most apparent when a talker's pronunciations---and thus the mapping of acoustic input to linguistic categories and meaning---strongly deviate from listeners' expectations. This might occur, for example, when listening to talkers with unfamiliar regional or non-native accents, or patients with apraxia or dysarthria. The same fundamental challenge is, however, present even during seemingly effortless comprehension: even among talkers who share similar language backgrounds, the mapping between the acoustic input and linguistic categories can vary substantially between talkers due to both physiology (e.g., vocal tract size and shape) and socio-cultural factors (e.g., social identity and language background). As a consequence, one talker’s pronunciation of, say, the sound category [s] (as in *sip*) can be acoustically more similar to another talker’s production of [`r linguisticsdown::cond_cmpl("ʃ")`] [as in *ship*, Newman et al., -@newman2001]. How we manage to typically understand each other despite such cross-talker differences has remained one of the perennial puzzles in research on speech perception [where it is known as part of the infamous *lack of invariance* problem, @liberman1967]. In the present project, we outline a general computational framework that can help address this question, by---for the first time---implementing competing hypotheses about the mechanisms underlying listeners' ability to accommodate cross-talker variability. Our goal is to demonstrate how the framework works, what it offers, and why a computational approach like this is needed. We use the framework to show that we know much *less* about the perceptual mechanisms underlying speech perception than is often assumed. We then discuss how to design more decisive behavioral experiments that are more suited to distinguish between competing hypotheses about these mechanisms.

<!--1.	state at high-level and provide a quick summary of adaptation literature--> Research over the past decades has identified adaptive changes in speech perception to be a key component in listeners' ability to overcome cross-talker differences. Although speech perception can initially be slower and/or less accurate when listeners encounter an unfamiliar talkers with unexpected pronunciations, these processing difficulties tend to reduce with exposure [e.g., @bradlow-bent2008; @nygaard1994; @Perrachione2016; @sidaras2009; @wade2007; @weil2001a; @xie2021jep]. Remarkably, substantial improvements can occur within minutes or less [@clarke-garrett2004; @munro-derwing1995; @xie2018jasa]. Even a context as brief as a single utterance can be sufficient to change perception and segmentation of ambiguous speech [e.g., categorizing a Dutch ambiguous /m?t/ embedded in an utterance with fast or slow speech as either "mat" or "maat" Bosker et al.,-@bosker2017]. [see also @kluender1988; @newman-sawusch1996; @sawusch-newman2000; @sjerps2011].Evidence of rapid behavioral improvements---often marked by faster processing and more accurate recognition---have been reported across a variety of phonetic categories in multiple languages [@kraljic-samuel2007; @bradlow-bent2008; @xie2021jep; @reinisch-holt2014]. 

<!--Understanding these adaptive abilities has led to the formation of many new lines of research, each employing slightly different methodologies, terminology, and theoretical assumptions [for reviews, see @baeseberk2020; @johnson-sjerps2021; @quam-creel2021; @samuel-kraljic2009; @stilp2020; @weatherholtz-jaeger2016].--> 

<!--
\begin{figure}[h]
\begin{center}
\includegraphics[width=.7\columnwidth]{`r get_path("../figures/diagrams/schertzclare.png")`}
\caption{... PLACEGHOLDER ...}\label{fig:norris-2003}
\end{center}
\end{figure}
--> 

<!--knowledge gap： what do we not know and why should we care about it? --> In short, listeners' ability to adapt based on recent input is now considered a central part of human speech perception. Explaining how it can be done has motivated major theories of speech perception and comprehension.^[Additional lines of work have investigated exposure to synthesized, vocoded, or otherwise distorted speech [e.g., @adank2009; @davis2005; @fenn2003; @mattys2012] or have used distributional learning paradigms [e.g., @clare2019; @clayards2008; @idemaru-holt2011; @maye2008; @schertz2013; @theodore-monto2019; @wade2007].<!--TO_DO:this footnote should perhaps be moved elsehwere now that we do not talk about the two paradigms here--> In the general discussion, we return to distributional learning paradigms, which we believe to hold particular promise in addressing the issues we identify below.] Existing theories in speech perception share general assumptions about how the acoustic input supports perception of a speech category (\@ref(fig:overview)). The process begins with (A) the extraction and normalization of acoustic/phonetic cues. These cues are (B) mapped onto and activate linguistic categories (such as phonemes, syllables, and/or words). Finally, (C) decision processes integrate the resulting category activations with contextual support and/or meta-reasoning (e.g., about the task), and then recognition takes place. Despite these substantial advances, however, mechanistic origins of the adaptivity are not yet well understood. In particular, the rapidity of adaptive changes blurs the line between processing and learning.  Different explanations continue to coexist across different lines of work, each with qualitatively different implications for the cognitive and neural accounts of how—through what mechanisms—exposure comes to facilitate comprehension [for reviews, see @baeseberk2020; @johnson-sjerps2021; @quam-creel2021; @samuel-kraljic2009; @stilp2020; @weatherholtz-jaeger2016]. We currently do not know which of the mechanism(s) is/are responsible for its *adaptive changes* according to recent exposure. 

<!--Why do we not know--> This gap in the knowledge, we argue, is due primarily to the fact that empirical investigations have typically examined one mechanism at a time. Very few studies have directly contrasted predictions of two mechanisms [@apfelbaum-mcmurray2015; @lehet-holt2020; @xie2021cognition]. None that we know has contrasted all three. Results are consequentially interpreted as evidence in support of a particular mechanism without explicit rejection of alternative explanations. There is currently no strong evidence that any of the mechanisms is actually involved in adaptive changes of perception. <!--In our review of the literature, there have been very few efforts that aim to contrast any two of the mechanisms directly; none contrasts all three. Empirical investigations that assess adaptation properties are often framed in a way that vaguely appeal to one of the mechanisms; results are consequentially interpreted as evidence in support of a particular mechanism without comparing against alternative explanations.--> 

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\columnwidth]{`r get_path("../figures/diagrams/overview-of-three-mechanisms2.png")`}
\caption{Listeners' recognition of speech categories are typically assumed to involve at least three types of mechanisms: 1) the acoustic input is transformed by low-level normalization processes into the perceptual cues that form the input to categorization, 2) linguistic representations describe the mapping between these perceptual and linguistic categories, and 3) decision-making mechanisms allow additional, stimulus-independent, biases to affect recognition. Any of these three mechanisms can in theory be affected by recent experience. While it is now clear \emph{that} recent experience changes the processing of subsequent speech input, most existing findings leave open \emph{which of the three mechanisms underlie these changes.}}\label{fig:overview}
\end{center}
\end{figure}

<!--Figure \@ref(fig:overview) summarizes the three types of mechanisms we consider, each of which can, in theory, explain why and how recent experience affects speech perception. -->

The majority of recent research on talker-related adaptation has focused on the middle layer, changes in linguistic representations. This includes proposals that attribute exposure effects to "boundary re-tuning/shifts” [e.g., @norris2003; @reinisch2011], “perceptual/category recalibration” [e.g., @kraljic-samuel2006; @reinisch-holt2014; @samuel2016; @vroomen-baart2009], "perceptual retuning" [@jesse-mcqueen2011; @mcqueen2006; @mitterer2013], “category expansion" [@schmale2012], “dimension-based statistical learning” [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015], or "criteria relaxation" [@zheng-samuel2020]. While these proposals are often not further formally specified or modeled [for notable exceptions, see @apfelbaum-mcmurray2015; @clayards2008; @hitczenko-feldman2016; @kleinschmidt-jaeger2015; @lancia-winter2013; @theodore-monto2019; @xie2021cognition], all of them seem to describe types of changes in representations. For example, "category shift" refers to a change in the mean of the cue distribution corresponding to a category, and "category expansion" refers to increases in the variance of that distribution. Either of these changes, along with "cue re-weighting" or "learning of new cues", can be understood as a consequence of the type of distributional learning that is hypothesized in exemplar [@apfelbaum-mcmurray2015; @johnson2006], episodic [@goldinger1998], Bayesian inference [@kleinschmidt-jaeger2015], or neural network models [@lancia-winter2013]. Indeed, recent reviews discuss *what types* of representational changes underlie the effects of recent exposure (e.g., "category expansion" vs. "category shifts"), rather than *whether* representational changes are the actual mechanism underlying the observed results [e.g., @baeseberk2020; @bent-baeseberk2021; @schertz-clare2020]. In our own work, we have sometimes made similar assumptions---e.g., when asking whether representational changes can account for adaptation without considering alternative explanations [@kleinschmidt-jaeger2011; @kleinschmidt-jaeger2012; @kleinschmidt-jaeger2016cogsci; @kurumada2017; @tan2021].

<!--TO DO: INTEGRATE IN  NEXT PARA: “normalization” [@johnson1990; @johnson-sjerps2021; @pisoni1977],-->

An absence of contrastive tests, however, means that the same behavioral results could in principle be explained by an alternative mechanism that assumes *no change of linguistic representations*. For instance, if is possible that adaptive changes of responses can be due to low-level, automatic (involuntary) normalization during the early stages of auditory processing (bottom of Figure \@ref(fig:overview)) These processes are thought to be pre-linguistic in that they do not refer to categories but rather apply to the acoustic or phonetic cues [@johnson-sjerps2021; @stilp2020]. In contrast to the assumption that cross-talker variability may be learned and stored, which is fundamental to all the theories of representational changes as described above, theories in normalization assume that listeners remove (or discard) talker variability prior to mapping cues to linguistic categories. *That* such processes help navigate cross-talker variability is widely assumed both in behavioral and neuroimaging research [e.g., @kluender1988; @irino-patterson2014; @johnson1990;@pisoni1977;@newman-sawusch1996; @sawusch-newman2000; @sjerps2011; @uddin2020; for discussion, see @barreda2012; @magnuson-nusbaum2007; @wong2004]. However, distinct normalization procedures have mostly been compared to each other [e.g., @adank2004; @hoffmanbion-escudero2007; @kiefte-nearey2019] or against the absence of normalization [@mcmurray-jongman2011]. They are often not considered as a competing explanation when researchers interpret the results of experiments on accent adaptation, perceptual recalibration, or distributional learning [but see @mullennix-pisoni1990; @sjerps-reinisch2015]. Only a handful of recent studies have begun to do so for specific contrast types. With some (e.g., English fricatives [@chodroff-wilson2020]), low-level auditory normalization has been found to explain human perception of fricatives better than changes in representations. <!-- And while distinct normalization procedures have been compared to each other [e.g., @adank2004; @hoffmanbion-escudero2007; @kiefte-nearey2019] or against the absence of normalization [@mcmurray-jongman2011], they are rarely compared to the competing hypothesis that changes in representations underlie the effects of recent exposure [for notable exceptions, see @apfelbaum-mcmurray2015; @lehet-holt2020; @xie2021cognition]. ^[It is, however, worth pointing out that the exact nature of normalization remains unclear. This includes questions about the specific transformations that are applied [@REFS] but also questions about whether normalization is best viewed as an autonomous, encapsulated process, or as part of a larger hierarchical inference process that includes inferences at both levels traditionally considered linguistic and levels traditionally considered pre-linguistic. Research on automatic speech recognition, for example, has found that deep neural network models can to some extent remove the need for specialized signal transformations [@deng2016]. Under this view, normalization is itself the result of a hierarchical predictive process that seeks to efficiently predict the incoming signal [for related discussion, see @clark2013; @kell2018; @kleinschmidt-jaeger2015; @kuperberg-jaeger2016].] --> 
  

Yet alternatively, it is possible that *post-linguistic* mechanisms underlie the effects of recent exposure (top of Figure \@ref(fig:overview)). Just like normalization is broadly accepted to be part of speech perception, there is little doubt that changes in response biases can affect listeners' interpretation of speech input, or at least the responses they give within experiments. For instance, Clarke-Davidson et al. [-@clarkedavidson2008, p.605] define a bias as "the increased likelihood to give a particular response---such as /s/---given any acoustic input, or the need for less evidence for a particular response" that "would help participants make faster word decisions in [...] ambiguous cases". However, such biases are rarely considered in the behavioral literature when analyzing the effects of recent exposure. *When* response biases have been considered [e.g., as "response equilibration" in Vroomen and Baart -@vroomen-baart2009], this explanation tends to be dismissed---prematurely, we believe. One reason for this might be particular result patterns---such as boundary shifts in perceptual recalibration or improved categorization or transcription accuracy in accent adaptation---are considered unlikely to be explained by changes in response biases (or normalization, for that matter). <!--It is often assumed, for example, that response biases can only change the relative proportion of answers but cannot explain overall improvements in accuracy. Under this assumption, overall improvements in the perception of an accent would seem to rule out explanations in terms of response biases, at least if the experimenter carefully balanced how often each category occurs during test. This is, however, a common misconception (one that we shared, for what it is worth). As we demonstrate below, changes in response biases can explain findings that are often attributed to changes in representations. The same applies to pre-linguistic normalization: contrary to common intuitions, simple normalization mechanisms can explain seemingly complex changes in listeners' categorization following exposure to L2-accented or otherwise shifted speech (as, e.g., in perceptual recalibration paradigms).-->  

In investigations into neural systems, in contrast, it is commonplace to isolate prefrontal areas [e.g., @binder2004neural; @thompson1997role] as well as the insula and parietal cortex [e.g., @furl2011parietal; @keuken2014] as responsible for perceptual decision-making. Recent studies have begun to investigate whether adaptive changes in speech perception primarily recruit these areas [@myers-mesite2014; @erb2013brain] as opposed to cortical areas associated with phonetic representations [@bonte2017; @luthra2020a]. For instance, @myers-mesite2014 have suggested that sensitivity to boundary shifts between [s] and [`r linguisticsdown::cond_cmpl("ʃ")`] emerged in right frontal and middle temporal regions, implicating adjustments of decision-related or attentional criteria downstream from primary auditory cortex. Similarly, a recent work on adaptation to phoneme category substitutions (e.g., [s] pronounced as [`r linguisticsdown::cond_cmpl("ʃ")`]) found evidence that adaptive processing of such phoneme substitutions engages prefrontal regions, responsible for post-perceptual *repair* mechanisms [@blanco-elorriera2021]: simply put, listeners seemed to initially map the acoustic input onto the wrong category and subsequently corrects the mapping. This was contrasted with the hypothesis that adaptation occurs via returning of the lower-level functional connections (e.g., between STG and primary auditory cortex).^[We note that other lines of imaging research have focused on pre-linguistic signal transformations [@sjerps2011listening; @zhang2016functionally] and sensory adaptation [@guediche2015evidence], including sub-cortical structures [e.g., the brain stem, @skoe2021auditory; and cerebellum, @guediche2014]. Compared to the behavioral research we reviewed, it is more common to directly contrast hypotheses that are associated with functionally distinct areas and networks. Like the behavioral research, however, no study to date has examined the three mechanisms in a coherent framework. Studies often adopt stimuli and experimental design that are strongly constrained by particular hypotheses researchers intend to test. The recommendations we provide in General Discussion will help facilitate direct tests across multiple mechanisms.]

Across the behavioral and neuroimaging work, the past research has thus split the underlying process of speech perception (Figure \@ref(fig:overview)) in distinct ways. Behavioral studies have begun to distinguish between the (A) normalization and (B) representational changes while continuing to treat (C) post-perceptual decision-making as a factor external to perception. In contrast, the neuroimaging work tends to group (A) and (B) together as happening early in the neural processing stream, functionally distinct from (C) i.e., higher-level, decision-related mechanisms further downstream. To meaningfully link these lines of work, and to better elucidate how human speech perception operates over variable acoustic input, we need a framework that can encompass (A)-(C) [for a related discussion, see @guediche2014, p. 8]. More specifically, we need a framework to formalize and implement the competing hypotheses about the effects of recent exposure on subsequent speech perception. By testing these predictions using a coherent experimental design, we will be able to evaluate if any of the mechanisms (A)-(C), and *only* that mechanism, can account for adaptive changes seen in human responses.

<!-- Recent reviews highlight this empirical and theoretical indeterminacy as an important unresolved gap in our understanding of speech perception [e.g., @bent-baeseberk2021; @schertz-clare2020; @weatherholtz-jaeger2016; @zheng-samuel2020]. Identifying the exact mechanisms of adaptive changes perception not only contributes to our basic understanding of speech perception, it is also important for developing treatments for impaired adaptation. which may arise from multiple auditory, linguistic and cognitive sources.--> 

<!--critical need: We therefore need to implement the intuitions that allows contrastive tests; we also need ways to assess how the interaction of the multiple mechanisms might depend on types of exposure and task demands. Our response: we propose a formalized approach to really distinguish between them. The very first contribution of the current paper. A large part of this paper is to spell out a general-purpose framework that integrates these mechanisms in a unified framework. This builds and expands a few notable efforts.-->


This motivates the present study. Our long-term goal is to contribute to the development of stronger theories that facilitate decisive comparisons between alternative hypotheses about the mechanisms underlying speech perception [in the tradition of "strong inference" approaches to scientific inquiry, @platt1964]. As a first step, we develop a general, unified framework that implements the three distinct theories of adaptive speech perception (i.e., (A)-(C) in Figure \@ref(fig:overview)). To anticipate our results, our simulations will demonstrate that the vast majority of existing results do *not* distinguish between three different explanations. We will show this in two highly influential paradigms: so-called **perceptual recalibration** [e.g., @kraljic-samuel2006; @reinisch-holt2014; @samuel2016; @vroomen-baart2009] and **(foreign) accent adaptation** [e.g., @bradlow-bent2008; @hernandez2019; @tzeng2016; @sidaras2009; @xie2016jep; @zheng-samuel2020]. In each, result patterns taken as signature evidence of one mechanism (e.g., changes of representations) could be predicted by *any of the three mechanisms*. To us, and we expect to others in the field, the degree to which this is the case was surprising. These separate lines of research (our own work included) seem to have drawn theoretical conclusions through convention, rather than through strong empirical evidence in favor of a particular mechanism. 

As we discussed above, the assumption that our *linguistic representations* adapt to recent exposure has been central to recent breakthroughs in speech perception research [e.g., @hay2019; @luthra2020a; @kleinschmidt-jaeger2015; @sjerps2019]. Evidence supporting this assumption has helped shape linguistic theories [e.g., @bradlow-bent2008; @baeseberk2013; @goldinger-azuma2004; @hay2019; @magnuson-nusbaum2007; @tzeng2016] as well as theories about the interface between social and linguistic cognition [e.g., @babel2019; @creel-bregman2011; @foulkes-hay2015; @hanulikova2012; @sumner2014]. The empirical indeterminacy that we present calls for a scrutiny of this assumption. Even if one were to take for granted that the three mechanisms in Figure \@ref(fig:overview) *jointly* underlie the effects of recent exposure, it remains unclear which of those mechanisms any given results sheds light on. For example, can we conclude from experiments like @clarke-garrett2004 that listeners can learn new representations (or at least select some mixture of existing representations) within two minutes of exposure? Or are those results due to normalization, and if so, where should we draw the line between processing and learning? Also, do the results of perceptual recalibration and accent adaptation stem from the same mechanism(s), or do they reflect different mechanisms [@baeseberk2018; @samuel-kraljic2009; @zheng-samuel2020]? Our comprehensive framework will make it possible to empirically test whether two paradigms that differ substantially in their design, tasks, and ecological validity of the speech stimuli would in fact engage the same mechanism(s). 

<!--leave it here for now, but potentially can be integrated into GD-->An explicit mechanistic account delineating how adaptive changes also strengthens our ability to probe its neural substrates. To date, multiple neurobiological systems have been proposed to support these rapid changes [e.g., @adank2015]. They range from the primary auditory cortex  [e.g., Heschl’s gyrus, @Perrachione2007] to those that respond to acoustic-phonetic features in speech [e.g., superior temporal gyrus, @Yi2019]. Even wider heterogeneous networks have been implicated in studies assessing brain responses to novel speech input, including those responsible for recognizing a voice/talker [@luthra2020b], attending selectively to a given talker's speech [@wong2004], and correcting prediction errors when there is a discrepancy between a predicted vs. an actual input [e.g., @guediche2015evidence], to name a few. Identifying the roles these brain regions play during adaptive speech perception is not straightforward, as these regions often play multiple roles in cognitive processing. Therefore, the exact interpretation of the findings still depends on the researchers’ hypothesis of the underlying mechanism. For instance, changes in inferior frontal gyri activation in response to accented speech has been interpreted as reflecting greater computational demand [@yi2014neural] or decision related phonetic categorization of ambiguous stimuli [@myers-mesite2014]. A proper theoretical framework that specifies how the alternative mechanisms can account for adaptive changes in speech perception, either in isolation or jointly, will facilitate the design and interpretations of neural investigations.

\begin{figure}[h]
\begin{center}
\includegraphics[width=.99\columnwidth]{`r get_path("../figures/diagrams/overview-of-changes.png")`}
\caption{Overview of our approach. Experiments on the effect of recent exposure on subsequent speech perception tend to involve two phases. An exposure phase manipulates the statistics of speech input between participants. A subsequent test phase assesses the effects of those manipulations on the interpretation of identical speech input. We seek to understand what type of exposure effects each of the three mechanisms in Figure \ref{fig:overview} can explain. To this end, we specify both a) a {\em categorization model} that describes the interpretation of speech input at any given moment (vertical information flow) and b) {\em change models} for all three mechanisms that describe how these parts of the categorization model change as a function of exposure (horizontal information flow). We then use the different change models to compare the predicted consequences of changes to normalization, representations, or response biases.}\label{fig:overview-change}
\end{center}
\end{figure}
  

The approach we take in this article is illustrated in Figure \@ref(fig:overview-change). We extend the three-pronged process of speech perception (Figure \@ref(fig:overview)) to implement *change models* describing how normalization, representations, and response biases may change with exposure. By training these change models on large-scale production corpora, we simulate adaptive changes of human responses predicted under a given hypothesized mechanism. Our main finding is two fold. First, as we stated above, existing data do not decisively separate their predictions. Present-day conventions in experimental design and analysis severely limit the discriminatory power of behavioral results over the underlying mechanisms. Second, perhaps more significant, it *will* be possible to distinguish the mechanisms from one another if new standards are applied for selection of adaptor/test stimuli as well as for statistical tests. Results of our simulations illuminate when---to what stimuli and under what circumstances---the three mechanisms would provide diverging predictions about adaptive changes of perception. We also learn about how much data will be needed to draw reliable statistical inferences from behavioral data. The general discussion will elaborate on these new standards and how they can be achieved.


<!-- By developing a computational framework that at least acknowledges, and integrates these aspects of speech perception, we take on (small) step towards addressing the challenge identified by @guediche2014: "there is no formal speech perception model that relates activity in the cortical regions identified via neuroimaging to the computational demands of adaptive plasticity in speech perception. Conversely, the classic computational models of speech perception that have attempted to differentiate how the system may meet the computational demands of adaptive plasticity have not made specific predictions of the underlying neural mechanisms".  -->

<!--Here we use this framework to examine whether changes in normalization, representations, or response biases can explain the signature results of experiments on perceptual recalibration (Case Study 1) and accent adaptation (Case Study 2). --> <!-- , and (c) unsupervised learning.  -->
<!--These signature results differ across the two paradigms, reflecting differences in the way that data from the two paradigms tend to be analyzed. For perceptual recalibration, researchers tend to analyze the shift in the categorization boundary. For accent adaptation, researchers often analyze improvements in overall accuracy [e.g., @bradlow-bent2008; @sidaras2009] or improvements in the accuracy of specific categories [e.g., @xie2017; @wade2007]. We test whether both types of results can be derived from the same change model(s).-->

<!-- The computational framework we present aims to capture the central tenets of existing normalization and representational accounts, combining them with a psychometric model that is standard in many parts of the cognitive sciences [e.g., in vision, @REFS] but remains under-utilized in research on speech perception [but see e.g., @clayards2008; @kleinschmidt-jaeger2016cogsci; @REF-lapse]. Our primary goal here is to demonstrate how even such a general model can move the field closer to the ideal of *strong inference*---devising crucial experiments that directly contrast competing hypotheses with the goal of finding the most decisive evidence [@platt1964]. The same framework can, however, also be used to contrast competing representational accounts, and we point to examples of that in some of the studies below. -->

<!--To anticipate, the computational studies we report suggest that the field should change the way we design, analyze, and report our experiments.--> 

<!--What does it leave us? As we clarify below, this empirical indeterminacy is not inevitable. The field needs to change the way it designs the experiments and how the results are analyzed. We provide some initial suggestions about how the designs need to change. We also discuss how the framework we introduced here can be ultimately used to inform the designs and conduct the analyses. -->


All data and code for this article can be downloaded from OSF at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). This article is written in R markdown, allowing readers to replicate our analyses with the press of a button using freely available software [R, @R; @RStudio], while changing any of the parameters of our models. Readers can revisit any of the assumptions we make---for example, by substituting alternative models of linguistic representations. The supplementary information (SI, \@ref(sec:SI-software)) lists the software/libraries required to compile this document. Beyond our immediate goals here, we hope that this can be helpful to researchers who are interested in developing more informative experimental designs, and to facilitate the interpretation of existing results [see also @tan2021]. 
