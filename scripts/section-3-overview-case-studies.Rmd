# Next in this theater: overview of case studies
Next, we present two case studies that illustrate the predictions of the three change models for two types of paradigms that have been particularly influential: perceptual recalibration (Case Study 1) and accent adaptation (Case Study 2). Figure \@ref(fig:ASP-predictions) summarizes how we will use ASP in these two case studies to derive predictions about listeners' behavior.

\begin{figure}[h]
\begin{center}
\includegraphics[width=.99\columnwidth]{`r get_path("../figures/diagrams/predictive-power-simulations.png")`}
\caption{The ASP framework can be used to derive predictions, or to fit to human behavior. ASP's change models describe how ASP's categorization model is predicted to change based on the inputs experienced during exposure. The updated categorization model can then be used to predict listeners' response distributions for any test token (as we do in Case Study 1), and/or to predict listeners' recognition accuracy (as we do in Case Study 2). As described in the text, researchers can choose to set some of ASP's parameters based on phonetic databases. By doing so, researchers commit to the assumptions (i) that listeners learn and store some statistics of previously experienced speech input, and (ii) that the database is a sufficiently good approximation of those statistics for an average listener in the experiment. Setting parameters based on phonetic databases reduces the degrees of freedom and functional flexibility of ASP's change models, leading to stronger (more easily falsifiable) predictions. This is the approach we employ in our case studies.}\label{fig:ASP-predictions}
\end{center}
\end{figure}

ASP can be used to predict changes in perception from *weighted combinations* of all three change mechanisms. In the general discussion, we outline why this ability to make predictions based on combinations of mechanisms will likely be critical in moving the field forward. In that context, we discuss how future work can use ASP to address more advanced questions about the factors that determine the relative engagement of the three mechanisms---by investigating how the ASP parameters ($\kappa_0$, $\kappa_{c,0}$, $\nu_{c,0}$, and $\beta_{\pi}$) that best explain human behavior depend on factors such as stimulus properties, task demands, and individual differences between listeners.

For our two case studies, however, we deliberately only consider ASP models that employ one of the three change mechanisms at a time. This allows us to assess which of the three mechanisms are *sufficient*, and which ones are *insufficient*, to explain the signature results of the two paradigms. Of particular theoretical interest is whether it is indeed the case that the computationally least parsimonious change model, changes in category representations, is the only model that can explain the signature results of both perceptual recalibration and accent adaptation paradigms (as seems to be often assumed). 

Beyond this specific question, we use the two case studies to (i) illustrate how intuitions about the types of results each mechanism can(not) account for can be misleading, and (ii) to show how ASP can be used to inform researchers' intuitions, by predicting the effects of recent exposure on subsequent speech perception. For these reasons, both case studies focus on *qualitative comparison* of ASP's predictions against the signature results from each paradigm, documented in previously published perception experiments (in the general discussion, we show how ASP can be used to compare change models in terms of their quantitative fit against listeners' behavior).

The two case studies build on each other, each introducing new insights. Experiments on perceptual recalibration use comparatively simple stimuli designs, intended to elicit simple boundary shifts along a uni-dimensional perceptual continuum. And while the specific tasks employed during exposure and test can vary across studies, the general stimulus design is remarkably similar across experiments. This allows us to use Case Study 1 to introduce many of the relevant concepts while keeping the paradigm comparatively simple. Case Study 2 then introduces additional complexities that come with modeling *natural* accents, which can deviate from listeners' expectations in heterogeneous ways across multiple acoustic-phonetic dimensions. This allows us to work our way to an important insight: while all three change mechanisms predict that exposure should facilitate---or at least not hinder---subsequent processing of the same accent, the exact effects of exposure are predicted to depend on the specific accent properties and how they relate to listeners' prior expectations.

```{r MVG-make-models}
cues <- c("VOT_centered", "f0_Mel_centered")

items <- c("DOLLARS","TOPIC","DOES","TUNNELS","DAUGHTER","TALKING","DIED","TIME","DEFINITELY", "TELL", "DAY","TAKE", "DEAL", "TEACHER","DON'T","TOTALLY","DO","TOO")

# sample a balanced set of voiced and voiceless tokens for each place of articulation
# the specific number is determined by the min number of tokens per category for that contrast
d.chodroff_wilson.selected <-
  d.chodroff_wilson %>%
  filter(Word %in% items) %>%
  group_by(Talker, category) %>%
  mutate(n = n()) %>%
  group_by(Talker) %>%
  # subsample n tokens, as determined by category with fewer tokens
  mutate(
    n_min = min(n),
    n_category = n_distinct(category)) %>%
  # select talkers with both /d/ and /t/ observations
  filter(n_category == 2) %>%
  group_by(Talker, category) %>%
  sample_n(size = first(n_min)) %>%
  ungroup() %>%
  mutate_at(
      c("VOT", "f0", "f0_Mel", "f0_semitones"),
      list("centered" = function(x) apply_ccure(x, data = .)))

prior_marginal_VOT_f0_stats <-
  d.chodroff_wilson.selected %>%
  group_by(Talker) %>%
  summarise(across(c(VOT, f0_Mel), mean)) %>%
  ungroup() %>%
  summarise(
    x_mean = list(c(VOT = mean(VOT), f0 = mean(f0_Mel))),
    x_var_VOT = var(VOT),
    x_var_f0 = var(f0_Mel),
    x_cov = list(cov(cbind(VOT, f0_Mel))))

m.VOT_f0_MVG <-
  make_MVG_from_data(
  data = d.chodroff_wilson.selected,
  category = "category",
  cues = cues)
```
