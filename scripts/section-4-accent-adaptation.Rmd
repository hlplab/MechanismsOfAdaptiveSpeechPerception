# Case Study 2: accent adaptation {#sec:AA}
The second paradigm we consider focuses on *naturally* accented speech---e.g., dialectal [@smith2014], varietal [@shaw2018], or second language (L2) accents [@bradlow-bent2008; @eisner2013; @sidaras2009; @weil2001a]. Typically, exposure to an unfamiliar accent is compared to a control condition in which listeners are exposed to a familiar accent, most often the 'standard' variety of listeners' L1. Following exposure, listeners in either group are tested on the unfamiliar accent. In an influential study, @bradlow-bent2008 had listeners transcribe a total of 160 sentences of either Mandarin-accented English or L1-accented English, distributed over two sessions on two separate days. In a subsequent test phase, both groups transcribed Mandarin-accented sentences. Participants who were first exposed to Mandarin-accented English were significantly more accurate during test (over 90% accuracy compared to about 80%). This finding has since been replicated and extended [for review, see @baeseberk2020]. We now know that substantially shorter exposure can lead to similarly large improvements in accuracy [e.g., 80 sentences in a single session, about 2-5 minutes of speech, @xie2021jep], that these can persist over hours and days [@witteman2015; @xie2018lcn], and that accent adaptation can sometimes generalize across talkers of the same or similar accents [e.g., @baeseberk2013; @tzeng2016; @xie2021jep]. <!-- Facilitatory effects of exposure have also been demonstrated in tasks that tap into online processing of naturally accented speech, including cross-modal matching [@clarke-garrett2004; @xie2018jasa], phonological priming [@eisner2013; @xie2017], and visual world eye-tracking [@dahan2008; @trude2012talker].-->

Critically, naturally accented speech typically differs from listeners' expectations in ways that can be considerably more complex than the types of manipulation studied in perceptual recalibration experiments. It is thus not clear whether the same mechanisms that underlie perceptual recalibration also underlie adaptation to natural accents. This question continues to attract attention [see recent reviews, @baeseberk2018; @bent-baeseberk2021; @zheng-samuel2020], because its answer determines whether perceptual recalibration and related paradigms---which afford increased control over stimuli characteristics but come at the risk of decreased ecological validity---can shed light on how listeners overcome the challenge of cross-talker variability in everyday speech.

As in the case of perceptual recalibration, accent adaptation is often attributed to changes in category representations [e.g., @bent-baeseberk2021; @sidaras2009; @eisner2013; @sumner2009; @tzeng2016; @xie2016jep]. This might be in part due to the (false) assumption that changes in response biases can only explain trade-offs in accuracy: as the accuracy for one category improves, it has to inevitably decrease for the other category. Under this assumption, overall improvements in recognition accuracy could not possibly be explained by changes in decision-making. Similarly, pre-linguistic normalization has rarely been considered as a possible mechanism for accent adaptation---perhaps because the complexities of L2 accents make it counter-intuitive that simple centering would be sufficient to explain adaptation to L2-accented speech. However, to the best of our knowledge, previous work has never actually put these intuitions to the test. Case Study 2 thus assesses whether the signature results of experiments on accent adaptation necessarily distinguish between the three change mechanisms. 

Compared to perceptual recalibration paradigms, experiments on accent adaptation exhibit more heterogeneity in their designs and tasks. Case Study 2 focuses on experiments in which treatment exposure employs a single talker with the unfamiliar accent, and the test phase heard by both groups of participants employs previously unheard stimuli from the same accented talker [e.g., @eisner2013; @schertz2015; @xie2017].^[The approach we take here can also be applied to cross-talker generalization---i.e., paradigms in which speech from different talkers of the same (or different) accent as the exposure talker is used during test [@baeseberk2013; @bradlow-bent2008; @sidaras2009].] For example, in an exposure-test experiment, @xie2016jep exposed two groups of L1-English listeners to Mandarin-accented speech. In exposure, participants in the treatment group heard 30 words ending in /d/ (e.g., _overload_) together with 60 word fillers and 90 nonword fillers. Participants in the control group did not hear any words that contain /d/. After exposure, participants in both conditions categorized recordings of 60 minimal /d/-/t/ pairs (e.g., _seed_ vs. _seat_) spoken by the same Mandarin-accented talker. Participants who heard /d/-final words during exposure were more likely to categorize /d/-final words correctly during test, compared to the control condition. At the same time, a non-significant numerical *de*crease in accuracy was observed for /t/-final words [see also @xie2018lcn]. Related paradigms have yielded similar results for other contrasts and other L1-L2 pairs [e.g., @zheng-samuel2020; @eisner2013].

For Case Study 2, we construct a hypothetical experiment that closely follows this type of design. For the sake of continuity, we focus on the same syllable-initial /d/-/t/ contrast used in Case Study 1. Like in Case Study 1, we analyze the data following the conventions of the field. For experiments on accent adaptation, this typically means that the facilitatory effects of L2-accented exposure are assessed through improvements in categorization accuracy during test.

As in Case Study 1, we ask whether any of the three change mechanisms can *qualitatively* explain the signature results found in @xie2016jep and other similar work. The simulation process requires us to be specific about the properties of the L1 and L2 accents---and therefore their difference---at the level of acoustic-phonetic distributions. These properties---although well characterized in studies on L2 production---are rarely ever reported in the literature on adaptive speech perception, and their predicted consequences for perception are even less commonly modeled (for an exception, see Tan et al., 2021). In the discussion, we consider their influence on the model behaviors and further reflect on how the systematic patterns of differences between L1- and L2-accents provide an opportunity to distinguish between the three change mechanisms.

## Data
The stimulus generation procedure is described in detail in the SI (\@ref(sec:SI-AA)).

```{r study-AA-setup, message=FALSE}
# Set plotting aesthetics
geom_text.size <- theme_get()$text$size *5/14 # change from mm to point scale; keeping the size of geom_text the same as the rest of text in ggplot
pos <- position_dodge(.6)

# Random seed for this study
set.seed(123498765)

# Which categories is this experiment about?
categories.AA <- c("/d/", "/t/")
conditions.AA <- c("L1-accented", "L2-accented")
colors.category <- colors.condition <- colors.voicing
shapes.category <- shapes.condition <- c(15, 17)
linetypes.condition <- c(1,2)

# Number of subjects
n.subject <- 1

# Number of exposure tokens for each category, shifted and typical
n.exposure.token <- 30

# Multiple category variance by a constant; var.ratio = 1 means no change in category variance
var.ratio = 1
# How separable the categories are in the L2 accent?
# e.g., dist.L2_category.cue1  -- The ratio of category mean difference along the primary cue in L2 to that in L1 (the primary cue does not have to be the same cue dimension across accents); 1 means the distance is equal to that in L1 accent.

# Case 1: presented in the main text, with clear cue-weighting (e.g., word-initial stops in Korean-accented English).
dist.L2_category.cue1 <- 0.5
dist.L2_category.cue2 <- 0.7
shift.cue1 <- 0
shift.cue2 <- 0

# arbitrarily assume that the variance of the speech stimuli in the experiment is $\frac{1}{`r my_experimenter.variability_reduction`}$th of that observed in natural productions (applying to L1-accented speech only)
my_experimenter.variability_reduction <- 1

# Number of test blocks
n.test_block <- 1
# Number of test tokens per category
n.test.token <- 60

# Set the minimal values for randomly generated test token cue values to make sure it is physiologically possible
min_VOT = 5
min_f0_Mel = 20

m.io.VOT_f0.AA <-
  m.VOT_f0_MVG  %>%
  filter(category %in% categories.AA) %>%
  droplevels() %>%
  make_stop_VOTf0_ideal_observer() %>%
  arrange(category)

m.ia.VOT_f0.AA <-
  crossing(
    prior_kappa = 4^(1:6),
    prior_nu = 4^(1:6)) %>%
  rowwise() %>%
  mutate(ideal_adaptor = map2(prior_kappa, prior_nu, ~ make_stop_VOTf0_ideal_adaptor(m = m.io.VOT_f0.AA, kappa = .x, nu = .y))) %>%
  unnest(ideal_adaptor) %>%
  arrange(category)
```

### Exposure phase
Figure \@ref(fig:study-AA-exposure-test-plot)A shows the stimuli for the exposure and test phases of the experiment. In the L2-accented exposure condition, listeners hear 30 word recordings containing L2-accented initial /d/ and 30 word recordings containing L1-accented initial /t/. In the control condition with L1-accented exposure, listeners hear the same words but from an L1-accented talker.^[The use of L1-accented exposure, rather than L2-accented exposure without /d/, as control follows a typical design in accent adaptation studies [e.g., @bradlow-bent2008], rather than @xie2016jep. Xie and colleagues instead employed L2-accented exposure without /d/, using the same L2-accented talker as during test [see also @eisner2013]. For the present purpose, both types of control conditions lead to identical predictions (as long as the L2-accented control exposure successfully avoids conveying non-negligible amount of information about the categories considered during the test phase) since the current implementation of the three change models do not consider talker-switch costs.] For L1-accented exposure, the category likelihoods were set to match those observed in @chodroff-wilson2018, after C-CuRE normalization (i.e., the distributions shown in Figure \@ref(fig:demonstrate-normalization)B). This follows the same approach we took for the typical tokens in Case Study 1. For L2-accented exposure, the /t/ was given the exact same distribution as the L1-accented /t/, whereas the L2-accented /d/ differed from the L1-accented /d/. We simulated the /d/ distribution such that the primary cue for L1 listeners (VOT) becomes the secondary cue in the L2-accented speech while preserving the relative ordering between categories (i.e., shorter VOT and lower f0 signaling a /d/ than a /t/ category). This simulates a common type of situation that occurs in L2 accents. For example, Korean-accented English relies more on f0 rather than VOT to signal word-initial /d/-/t/ contrasts, compared to L1-accented English [e.g., @schertz2015].

```{r study-AA-exposure-functions}
# Get acoustic locations that correspond to targeted response proportions.
get_parameters_phonetic_contrast.AA <- function(ideal.observer,
                                                category_dist_ratio1,
                                                category_dist_ratio2,
                                                shift_mean_1 = 0,
                                                shift_mean_2 = 0) {

   # native categories
   mu_native_A <- ideal.observer %>% filter(category == "/d/") %>% pull(mu) %>% .[[1]]
   Sigma_native_A <- ideal.observer %>% filter(category == "/d/") %>% pull(Sigma) %>% .[[1]]
   mu_native_B <- ideal.observer %>% filter(category == "/t/") %>% pull(mu) %>% .[[1]]
   Sigma_native_B <- ideal.observer %>% filter(category == "/t/") %>% pull(Sigma) %>% .[[1]]

   # nonnative categories
   mu_nonnative_B <- mu_native_B
   Sigma_nonnative_B <- matrix(c(Sigma_native_B[1]*var.ratio, Sigma_native_B[2], Sigma_native_B[3], Sigma_native_B[4]), nrow=2)

   # The larger the category_dist_ratio, the greater distance along that cue dimension (1 = VOT, 2 = f0) between the two categories in the L2 accent.
   # If category_dist_ratio1 = category_dist_ratio2 = 0, then the two categories overlap entirely in the L2 accent.
   # If category_dist_ratio1 = category_dist_ratio2 = 1, then /d/-/t/ will have the same distance as that in L1-accent along the primary cue dimension.
   mu_nonnative_A <- c(mu_nonnative_B[1] * (1 + category_dist_ratio1 * (mu_native_A[1]- mu_native_B[1])/mu_native_B[1]),
                       mu_nonnative_B[2] * (1 + category_dist_ratio2 * (mu_native_A[1]- mu_native_B[1])/mu_native_B[1]))

   Omega_native_A <- cov2cor(Sigma_native_A)
   Omega_nonnative_A <- matrix(c(Omega_native_A[1], Omega_native_A[2], Omega_native_A[3], Omega_native_A[4]), nrow=2) # change omega if orientation of category dispersion changes
   Sigma_nonnative_A <- cor2cov(Omega_nonnative_A, sqrt(diag(Sigma_native_A))*var.ratio)
    
   # Allow a shift of nonnative category means
    shift1 <- (mu_nonnative_A[1]+ mu_nonnative_B[1])/2*shift_mean_1
    shift2 <- (mu_nonnative_A[2]+ mu_nonnative_B[2])/2*shift_mean_2
    mu_nonnative_A <- c(mu_nonnative_A[1]+ shift1,
                        mu_nonnative_A[2]+ shift2)
    mu_nonnative_B <- c(mu_nonnative_B[1]+ shift1,
                        mu_nonnative_B[2]+ shift2)
    
    tau_native_A <-  sqrt(diag(Sigma_native_A))
    Omega_native_A <- cov2cor(Sigma_native_A)
    tau_native_B <-  sqrt(diag(Sigma_native_B))
    Omega_native_B <- cov2cor(Sigma_native_B)
    tau_nonnative_A <-  sqrt(diag(Sigma_nonnative_A))
    Omega_nonnative_A <- cov2cor(Sigma_nonnative_A)
    tau_nonnative_B <-  sqrt(diag(Sigma_nonnative_B))
    Omega_nonnative_B <- cov2cor(Sigma_nonnative_B)

    # plot and check whether the two categories are simulated as intended
    d.model <- tibble(
      speech = c(rep("native", 2), rep("nonnative", 2)),
      category = rep(c("A", "B"), 2),
      mu = list(mu_native_A, mu_native_B, mu_nonnative_A, mu_nonnative_B),
      tau = list(tau_native_A, tau_native_B, tau_nonnative_A, tau_nonnative_B),
      Omega = list(Omega_native_A, Omega_native_B, Omega_nonnative_A, Omega_nonnative_B),
      Sigma = list(Sigma_native_A, Sigma_native_B, Sigma_nonnative_A, Sigma_nonnative_B)) %>%
      group_by(speech, category) %>%
      mutate(Model = list(list(
        mu = Reduce("+", mu) / length(mu),
        Sigma = Reduce("+", Sigma) / length(Sigma))))

    return(d.model)
}

# make the design of an exposure phase
make_accent_adaptation_exposure_design = function(
  # Ideal observer describing the true perceptual system for two categories.
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,

  # How separate are the two categories in L2 accent?
  category_dist_ratio1 = dist.L2_category.cue1,
  category_dist_ratio2 = dist.L2_category.cue2,
  shift_mean_1 = 0,
  shift_mean_2 = 0,
  exposure.tokens.L1.n = n.exposure.token,
  exposure.tokens.L2.n = n.exposure.token,
  quiet = F
) {
  if (!quiet) message(
    paste0("Making exposure design with ", exposure.tokens.L1.n, " L1-accented tokens and ", exposure.tokens.L2.n, " L2-accented tokens."))

  d.model.both <- 
    get_parameters_phonetic_contrast.AA(
      experimenter.ideal_observer,
      category_dist_ratio1,
      category_dist_ratio2,
      shift_mean_1,
      shift_mean_2)
  
  d.model.native <- d.model.both %>%
    filter(speech == "native") %>%
    droplevels()
  d.model.nonnative <- d.model.both %>%
    filter(speech == "nonnative") %>%
    droplevels()

  # Create basic tibble
  tibble(
    Phase = "exposure",
    ItemID = as.character(1:(exposure.tokens.L1.n + exposure.tokens.L2.n)),
    Item.Type = factor(c(rep("L1-accented", exposure.tokens.L1.n), rep("L2-accented", exposure.tokens.L2.n)))) %>%
    # Add all unique design combinations
    crossing(
      Item.Category = factor(experimenter.ideal_observer$category)) %>%
    # Get cue value of item: if L2-accented then use L2 category parameters. If not sample from L1-accented category distribution
    mutate(
      Condition = Item.Type,
      x = map2(
        Item.Type,
        Item.Category,
        ~ case_when(
          .x == "L2-accented" & .y == experimenter.ideal_observer$category[[1]] ~ rmvnorm(1, d.model.nonnative$mu[[1]], d.model.nonnative$Sigma[[1]] / experimenter.variability_reduction),
          .x == "L2-accented" & .y == experimenter.ideal_observer$category[[2]] ~ rmvnorm(1, d.model.nonnative$mu[[2]], d.model.nonnative$Sigma[[2]] / experimenter.variability_reduction),
          .x == "L1-accented" & .y == experimenter.ideal_observer$category[[1]] ~ rmvnorm(1, d.model.native$mu[[1]], d.model.native$Sigma[[1]] / experimenter.variability_reduction),
          .x == "L1-accented" & .y == experimenter.ideal_observer$category[[2]] ~ rmvnorm(1, d.model.native$mu[[2]], d.model.native$Sigma[[2]] / experimenter.variability_reduction),
          T ~ NA_real_))) %>%
    mutate(
      VOT = unlist(map(x, ~ max(min_VOT, .x[1]))), 
      f0_Mel = unlist(map(x, ~ max(min_f0_Mel, .x[2]))),  
      x = map2(VOT, f0_Mel, ~ c("VOT" = .x, "f0_Mel" = .y)))
}

#<!--TO DO remove this function if already existed in main file; leaving it here now so that this section can be knitted independently--!>
add_subjects_to_exposure <- function(
  d,
  n.subject,
  quiet = F
) {
  assert_that(!("Subject" %in% names(d)),
              msg = "This data frame already seems to contain subjects")
  if (!quiet) message(paste("Adding", n.subject, "subjects per exposure condition."))

  d %>%
    group_by(Condition) %>%
    crossing(Subject = factor(1:n.subject)) %>%
    mutate(Subject = paste(Condition, Subject, sep = ".")) %>%
    select(Condition, Subject, Phase, ItemID, Item.Category, Item.Type, x, everything()) %>%
    ungroup()
}
```

```{r study-AA-exposure-make-data, message=FALSE}
d.AA.exposure <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)

# calculate sample variance along each cue dimension for each exposure condition
sample_var <- d.AA.exposure %>%
  group_by(Condition, Item.Category) %>%
  summarise(s_var.f0 = var(f0_Mel),
    s_var.vot = var(VOT))

# get prior variance along each cue dimension for each exposure condition
population_var <- data.frame(
  c("/d/", "/d/", "/t/", "/t/"),
  c("VOT", "f0", "VOT", "f0"),
  c(m.VOT_f0_MVG$Sigma[2][[1]][1], m.VOT_f0_MVG$Sigma[2][[1]][4],
    m.VOT_f0_MVG$Sigma[6][[1]][1], m.VOT_f0_MVG$Sigma[6][[1]][4]))
colnames(population_var) = c("category", "cue", "prior_variance")
```

### Test phase
During test, listeners from both exposure conditions hear the same recordings from the same L2-accented talker used in the L2-accented exposure condition. 60 test tokens are randomly sampled from the distribution of each L2-accented category, half from the /d/ category and half from the /t/ category (Figure \@ref(fig:study-AA-exposure-test-plot)B).

```{r study-AA-test-functions}
# TO DO: Xin, why do the test conditions have a different name than the exposure conditions? 
# also, why are there even two test conditions?? 
# (this an other questions might need to be revisited for the PR section, too, but that section also has other requirements)

# make the design of an test phase
make_accent_adaptation_test_design = function(
  d,
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,
  category_dist_ratio1 = dist.L2_category.cue1,
  category_dist_ratio2 = dist.L2_category.cue2,
  shift_mean_1 = 0,
  shift_mean_2 = 0,
  test.n_block = n.test_block, 
  n_test = n.test.token     
) {
  d.model.both <- 
    get_parameters_phonetic_contrast.AA(
      experimenter.ideal_observer,
      category_dist_ratio1,
      category_dist_ratio2 ,
      shift_mean_1,
      shift_mean_2)
  
  ## --------------------------------------------------------------------------------
  # create a *natural* non-native test set
  ## --------------------------------------------------------------------------------
  x <- 
    rbind(
      rmvnorm(
        n_test,
        d.model.both %>% filter(speech == "nonnative", category == "A") %>% pull(mu) %>% .[[1]],
        d.model.both %>% filter(speech == "nonnative", category == "A") %>% pull(Sigma) %>% .[[1]] / experimenter.variability_reduction),
      rmvnorm(
        n_test,
        d.model.both %>% filter(speech == "nonnative", category == "B") %>% pull(mu) %>% .[[1]],
        d.model.both %>% filter(speech == "nonnative", category == "B") %>% pull(Sigma) %>% .[[1]] / experimenter.variability_reduction))
  
  d.test <-
    tibble(
      Item.Intended_category = sort(rep(c("A", "B"), n_test)),
      VOT = unlist(map(x[,1], ~ max(min_VOT, .x))), # cue 1
      f0_Mel = unlist(map(x[,2], ~ max(min_f0_Mel, .x))), # cue 2
      x = map2(VOT, f0_Mel, ~ c("VOT" = .x, "f0_Mel" =  .y))) %>%
    mutate(
      Item.Intended_category = ifelse(Item.Intended_category == "A", "/d/", "/t/"),
      Phase = "test",
      ItemID = row_number(),
      Item.Type = "test",
      Item.Category = NA,
    ) %>%
    crossing(  
     # Condition = factor(paste0(paste0(.$Item.Intended_category, "_"), d$Condition)),
       Condition = factor(d$Condition),
      Block = 1:test.n_block) %>%
    # filter((Item.Intended_category == "/d/" & grepl("/d/", Condition)) | (Item.Intended_category == "/t/" & grepl("/t/", Condition))) %>%
    droplevels()
}

add_subjects_to_test <- function(
  d,
  n.subject,
  quiet = F
) {
  assert_that(!("Subject" %in% names(d)),
              msg = "This data frame already seems to contain subjects")
  if (!quiet) message(paste("Adding", n.subject, "subjects per exposure condition."))

  d %>%
    group_by(Condition) %>%
    crossing(Subject = factor(1:n.subject)) %>%
    mutate(Subject = paste(Condition, Subject, sep = ".")) %>%
    select(Condition, Subject, Phase, ItemID, Item.Category,Item.Intended_category, Item.Type, x, everything()) %>%
    ungroup()
}
```

```{r study-AA-test-make-data}
d.AA.test <- make_accent_adaptation_test_design(d.AA.exposure, experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)
```

(ref:study-AA-exposure-test-plot) **Panel A - Exposure:** Distribution of the stimuli used during the exposure phase of the accent adaptation experiment (`r n.exposure.token` tokens each of `r conditions.AA[1]` and `r conditions.AA[2]` `r paste(categories.AA, collapse = " and ")`, respectively). **Panel B - Test:**  Distribution of the stimuli used during the test phase of the accent adaptation experiment. The test tokens come from L2-accented speech (`r n.test.token` tokens per category) and are identical for the two exposure conditions. Ellipses show the 95% probability mass for the two categories in L2-accented exposure speech.

```{r plot-AA-exposure-test-functions}
# Function to plot exposure and test data
make_exposure_test_plot <- function(exposure.data, test.data){
  p.AA.exposure <- 
    exposure.data %>%
    mutate(Condition = paste0("Exposure: ", Condition)) %>%
    ggplot(aes(x = VOT, y = f0_Mel, color = Item.Category)) +
    geom_point(alpha = 0.5) +
    geom_rug(data = . %>%
                 group_by(Condition, Item.Category) %>%
                 summarise(VOT = mean(VOT), f0_Mel = mean(f0_Mel)), show.legend = FALSE) +
    scale_x_continuous(expression("VOT (ms)"), 
                        expand = expansion(mult = .1, add = 0)) +
    scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0), breaks=seq(0, 500, by = 100)) +
    coord_cartesian(ylim = c(0, 500)) +
    scale_color_manual("Category", breaks = categories.AA, values = colors.category) +
    scale_fill_manual("Category", breaks = categories.AA, values = colors.category) +
    facet_grid(~ Condition)
  
  limits <- get_plot_limits(p.AA.exposure)
  breaks <- get_plot_breaks(p.AA.exposure)

  p.AA.test <- 
    test.data %>%
    # TO DO: Xin, this looks like you are plotting the points for both test conditions 
    # (whatever the heck they are, which I still don't understand), and then are plotting
    # the ellipses for just the L2-accented test condition. If we remove the L1-accented 
    # test condition to begin with, I think we can simplify this code (and likely elsewhere below).
    mutate(Speech = ifelse(grepl("L1", Condition), "L1-accented", "L2-accented")) %>%
    filter(Speech == "L2-accented") %>%
    mutate(Speech = paste0("Test: ", Speech)) %>%
    ggplot(aes(x = VOT, y = f0_Mel, label = as.numeric(factor(ItemID)))) +
    geom_point(aes(color = Item.Intended_category), alpha = 0.5) +
    stat_ellipse(
      data = 
        exposure.data %>% 
        filter(Condition == "L2-accented"), 
      mapping = aes(fill = Item.Category), 
      level = .95, alpha = 0.3, geom = "polygon") +
    scale_x_continuous(
      expression("VOT (ms)"),
      expand = expansion(mult = .1, add = 0),
      breaks = c(breaks$xbreaks),
      limits = c(limits$xmin, limits$xmax)) +
    scale_y_continuous(
      expression("f0 (Mel)"),
      breaks = c(breaks$ybreaks),
      expand = expansion(mult = .1, add = 0),
      limits = c(limits$ymin, limits$ymax)) +
    scale_color_manual("Category", values = colors.category) +
    scale_fill_manual("Category", values = colors.category, guide = "none") +
    coord_cartesian(xlim = c(limits$xmin, limits$xmax), ylim = c(limits$ymin, limits$ymax), expand = FALSE) +
    facet_grid(~ Speech)

  prow = plot_grid(
    p.AA.exposure + theme(legend.position="none"),
    p.AA.test + theme(legend.position="none"),
    labels = c('A)', 'B)'), nrow = 1, rel_widths = c(2,1.1))
  
  legend_prow <- 
    get_legend(
      p.AA.exposure +
        guides(color = guide_legend(title.position = "left", nrow = 1)))

  p.output <-  plot_grid(legend_prow, prow, ncol = 1, rel_heights = c(.1, 1))
  return(p.output)
}

appender <- function(string, prefix = "Changes in\n") paste0(prefix, string)
```

```{r study-AA-exposure-test-plot, fig.width=base.width*3 + .5, fig.height=base.height + .5, fig.cap="(ref:study-AA-exposure-test-plot)"}
make_exposure_test_plot(exposure.data = d.AA.exposure, test.data = d.AA.test)
```

```{r add-subjects-AA-exposure-test-data}
d.AA.exposure %<>% add_subjects_to_exposure(n.subject = n.subject)
d.AA.test %<>% add_subjects_to_test(n.subject = n.subject)
```


## Results
Paralleling Case Study 1, we ask which of the three change models can account for the signature results of accent adaptation experiments. Specifically, we assess for each change model whether it can explain the two types of signature results observed in, for example, Xie et al. [-@xie2016jep]: (1) for L1-accented exposure, /d/ test tokens are recognized with lower accuracy than /t/ test tokens, since the difference between the two accents is more pronounced for /d/ than for /t/, creating a larger deviation from L1 listeners' expectations; and (2) L2-accented exposure should lead to a significant increase in recognition accuracy for /d/ test tokens without an equivalent decrease in the accuracy for /t/ test tokens, resulting in increased overall accuracy. 

Below we focus on whether each change model can *qualitatively* account for these signature results, rather than the specific magnitude of accuracy improvements. We do so, because the predicted improvements do not only depend on each change model's parameters, but also the statistics of the L2-accented exposure and test tokens. For a different set of (real or simulated) stimuli, the predicted magnitude of accuracy improvements after L2-accented exposure can thus differ. This is an important consideration to keep in mind, and we return to it in the discussion. 

As in Case Study 1, we first model changes in representations, and then compare them against change models in decision-making and normalization. We consider a larger range of parameterizations than in Case Study 1, because the models with the highest accuracy for L2-accented exposure (henceforth, the best-performing parameterization) in some cases fell outside of the range of parameter values considered in Case Study 1. These best-performing parameterizations were determined by bounded quasi-Newton optimization of the recognition accuracy during test after L2-accented exposure [@byrd1995, implemented in function \texttt{optim()} in R], and are indicated in all result plots we present below. Table \@ref(tab:XXX) summarizes the parameter ranges considered by the optimization algorithm.

<!-- TO DO: Xin, have a table here that summarizes the five parameters for the three change models, along with the min and max values considered for them. -->
<!-- For changes in representations, the optimization procedure searched values from the smallest possible value (1 for $\kappa_{0,c}$ and 4 for $\nu_{0,c}$) to 10000 (virtually no changes in representations). -->

### Changes in representations

```{r study-AA-models-changes-in-representations, echo=FALSE, message=FALSE, warning=FALSE}
range.prior_kappa <- c(1, 10000)
range.prior_nu <- c(4, 10000) 

# This and the next function collect code chunks that are repeated a few times below
# (if test is null it won't be added and it's assumed that there already is a nested
# column of test tokens called  x and another column indicating the mapping from x 
# to intended categories; this is required only for the normalization updating).
add_test_and_categorize <- function(data, test = NULL) {
  assert_that(is_tibble(data))
  assert_that(
    "posterior" %in% names(data),
    msg = "data must contain a column posterior that is a nested ideal adaptor.")
  if (is.null(test)) {
    assert_that(
      all(c("x", "Item.Intended_category") %in% names(data)),
      msg = "If test is NULL, data must contains column x with nested test tokens and Item.Intended_category with mappings of x to the intended/correct category.")
    assert_that(is_list(data$x))
  }
  
  data %>%
    { if (!is.null(test)) add_test_tokens(., test) else select(., -Item.Intended_category) } %>%
    add_categorization() %>%
    # Keep only posterior of intended category
    { if (!is.null(test)) {
      left_join(
        .,
        test %>%
          select(x, Item.Intended_category), 
        by = "x") 
    } else {
      left_join(
        .,
        first(data$Item.Intended_category),
        by = "x")
    }} %>%
    filter(category == Item.Intended_category)
}

get_accuracy <- function(categorizations) {
  categorizations %>%
    pull(response) %>%
      mean()
}

update_representations_and_categorize_test <- function(
  prior,
  exposure,
  test,
  cues = c("VOT", "f0_Mel")
) {
  update_NIW_ideal_adaptor_incrementally(
    prior = prior,
    exposure = exposure,
    exposure.category = "Item.Category",
    exposure.cues = cues,
    noise_treatment = "marginalize",
    lapse_treatment = "marginalize",
    method = "label-certain",
    keep.update_history = FALSE,
    keep.exposure_data = FALSE) %>%
    nest(posterior = everything()) %>%
    add_test_and_categorize(test)
}

# This function is intended for the optimization run right below it
# (we also store the history of the optimizers search)
history.optimization_representations <- tibble(.rows = 0)
get_accuracy_from_updated_representations <- function(
    pars,
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), 
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1"))
) {
  kappa <- exp(pars[1])
  nu <- exp(pars[2])
  
  prior <- 
    model %>% 
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = kappa, nu = nu)
  
  u <-
    update_representations_and_categorize_test(
    prior = prior,
    exposure = exposure,
    test = test) %>%
    get_accuracy()
  
  history.optimization_representations <- 
    bind_rows(
      history.optimization_representations, 
      tibble(kappa = kappa, nu = nu, accuracy = u))
  
  return(u)
}

if (RESET_MODELS || !file.exists(get_path("../models/best_performing_parameters.representations.rds"))) {
  best_performing_parameters.representations <- 
  optim(
    # Parameters are kappa, nu
    par = c(mean(log(range.prior_kappa)), mean(log(range.prior_nu))), 
    fn = get_accuracy_from_updated_representations,
    # The only method that works for multiple parameters with bounds
    method = "L-BFGS-B",
    # Stop searching outside of those bounds
    lower = c(min(log(range.prior_kappa)), min(log(range.prior_nu))),
    upper = c(max(log(range.prior_kappa)), max(log(range.prior_nu))), 
    control = list(
      # trace = 1, 
      fnscale = -1,   # negative scale since we are seeking to maximize accuracy
      factr = 10^8))  # smaller factr means higher optimum but slower convergence

  best_performing_parameters.representations$par <- exp(best_performing_parameters.representations$par)
  saveRDS(best_performing_parameters.representations, get_path("../models/best_performing_parameters.representations.rds"))
} else {
  best_performing_parameters.representations <- readRDS(get_path("../models/best_performing_parameters.representations.rds"))
}

# TO DO: Xin, I think we can consolidate some parts of this and the functions right below it.
# specifically, I think we can use the function update_representations_and_categorize_test 
# and group_map it onto the grouping structure that the code chunk right below this uses 
# (e.g, with group_map(...) %>% reduce(bind_rows) %>% ...). And we can do the same for all
# other updating procedures.
d.AA.representations <- 
  d.AA.exposure %>%
  add_prior_and_get_posterior_beliefs_based_on_exposure(prior = m.ia.VOT_f0.AA) %>%
  add_test_tokens(d.AA.test) %>%
  # TO DO: Xin, why are you adding the cat functions? It seems you never use them? (unlike in PR section)
  add_categorization_functions() %>% 
  group_by(Condition, Subject, prior_kappa, prior_nu) %>%
  add_categorization() %>%
  ungroup() %>%
  mutate_at(
    vars(starts_with("prior_")),
    ~ factor(as.character(.x), levels = as.character(rev(sort(unique(.x)))))) %>%
  distinct() %>%
  left_join(
    d.AA.test %>%
      select(x, Item.Intended_category), by = "x") %>%
  distinct() # TO DO: Xin, why is there a distinct() here?

 d.AA.exposure %>%
  add_prior_and_get_posterior_beliefs_based_on_exposure(prior = m.ia.VOT_f0.AA %>% filter(prior_kappa == 64, prior_nu == 256)) %>%
  add_test_tokens(d.AA.test) %>%
  # TO DO: Xin, why are you adding the cat functions? It seems you never use them? (unlike in PR section)
  add_categorization_functions() %>% 
  group_by(Condition, Subject, prior_kappa, prior_nu) %>%
  add_categorization() %>%
  ungroup() %>%
  mutate_at(
    vars(starts_with("prior_")),
    ~ factor(as.character(.x), levels = as.character(rev(sort(unique(.x)))))) %>%
  distinct() %>%
  left_join(
    d.AA.test %>%
      select(x, Item.Intended_category), by = "x") %>%
  distinct() %>% 
   get_accuracy()
```

(ref:AA-result-changes-in-representations) Predictions of a learning model that derives accent adaptation as changes in category representations. Predicted categorization accuracies for the L2-accented test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, as a function of the strength of the prior beliefs in category means ($\kappa_{0,c}$) and covariances ($\nu_{0,c}$). The average accuracy across /d/ and /t/ is shown above the bars for each exposure condition. Error bars show 95% bootstrapped confidence intervals. The highlighted panel is the one closest to the best-performing parameterization ($\kappa_{0,c} = `r best_performing_parameters.representations$par[1]`$; $\nu_{0,c}=`r best_performing_parameters.representations$par[2]`$; overall accuracy $=`r best_performing_parameters.representations$value`$). 
<!-- TO DO: Xin, let's increase the distance between the text label and top of graph (i.e., increase y-range a bit)
     also some of the plots have panel labels that need to be smaller to fit (might apply to PR section, too) -->

```{r plot-AA-results-representations-functions}
basic_AA_result_plot <- function(data) {
  data %>%
    # Show the response only for intended category
    filter(category == Item.Intended_category) %>% 
    ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
    list(
      stat_summary(
        fun = mean,
        geom = "bar", position = pos, width = 0.6),
      stat_summary(
        aes(color = Item.Intended_category),
        fun.data = mean_cl_boot,
        geom = "uperrorbar",
        position = pos, width = 0.2),
      coord_cartesian(ylim =  c(0,1)),
      scale_color_manual("Category", values = colors.category, aesthetics = c("color", "fill")),
      scale_alpha_discrete(range = c(0.2, 1), guide = "none"),
      scale_x_discrete(labels= c("L1-accented", "L2-accented")),
      xlab("Exposure condition"),
      ylab("Predicted categorization accuracy"),
      myGplot.defaults(base_size = 14, set_theme = F),
      theme(legend.position = "top", panel.grid.major.x = element_blank(), axis.text.x = element_text(angle = 22.5, hjust = 1)))
}

make_results_plot_representations <- function(data, data_best) {
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, 
      data = . %>%
        group_by(Condition, prior_nu, prior_kappa) %>%
        summarise(mAcc = round(mean(response), digits = 2)), 
      mapping = aes(label = mAcc, x = Condition, y = 1.05), 
      size = geom_text.size) +
    facet_grid(
      prior_nu ~ prior_kappa,
      labeller = label_bquote(
        cols = {kappa[0~","~ .(categories.AA[1])] == kappa[0~","~ .(categories.AA[2])]} == .(as.character(prior_kappa)),
        rows = {nu[0~","~ .(categories.AA[1])] == nu[0~","~ .(categories.AA[2])]} == .(as.character(prior_nu)))) 
  
  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = data %>%
          filter(category == Item.Intended_category) %>%
          distinct(prior_kappa, prior_nu) %>%
          mutate(
            highlight = 
              ifelse(
                prior_kappa %in% prior_kappa[which.min(abs(as.numeric(as.character(prior_kappa)) - data_best$par[1]))] & 
                  prior_nu %in% prior_nu[which.min(abs(as.numeric(as.character(prior_nu)) - data_best$par[2]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf)) +
  scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
  guides(fill = "none")
}
```

```{r AA-result-changes-in-representations, fig.width= base.width * 5, fig.height= base.height * 5 + 1, fig.cap="(ref:AA-result-changes-in-representations)", warning=FALSE}
make_results_plot_representations(data = d.AA.representations, data_best = best_performing_parameters.representations)
```

Figure \@ref(fig:AA-result-changes-in-representations) shows the predicted categorization accuracy after exposure to L1- or L2-accented speech, as a function of the strength of the prior beliefs for the category means and variances. First consider the top left panel where both $\kappa_{0,c}$s and $\nu_{0,c}$ have very high values. This simulates a extremely slow learner who has strong prior beliefs for both category means and variances such that the small number of tokens heard during exposure phase has minimal influence on subsequent perception of the test tokens. Therefore, the performance from both L1- and L2-accented exposure conditions are highly similar (or identical, if even slower learning rates are applied), reflecting how a listener would categorize based on their long-term prior experience predominantly. 

As $\kappa_{0,c}$s and $\nu_{0,c}$ get smaller (i.e., weaker prior beliefs), the effects of exposure conditions become more noticeable. Both signature results described above can be predicted by changes in category representations. First, for L1-accented exposure, the accuracy is always predicted to be lower for /d/ than for /t/, indicating that /d/ is often misheard as /t/.^[One additional finding for the L1-accented condition is of note. Compared across the rows from top to bottom, accuracy in the L1-accented condition improves for models with weaker prior beliefs about the category variances (small $\nu_{0,c}$, bottom rows of Figure \@ref(fig:AA-result-changes-in-representations)). At first blush, this is surprising given that listeners in the L1-exposure condition receive only input that matches their prior L1 expectations and mismatches the L2 speech they receive during test. However, for this instance of the (simulated) experiment, it turns out that the L1-accented /d/-exposure stimuli in Figure \@ref(fig:study-AA-exposure-test-plot) happen to---by chance---exhibit somewhat larger sample variance along f0 ($s^2$ = `r sample_var$s_var.f0[1]`) than is expected for a typical L1 talker [$\sigma^2$ = `r population_var$prior_variance[2]` based on the data from @chodroff-wilson2018]. The L2-accented /d/-exposure stimuli show a similar pattern, albeit slightly less so (sample variance $s^2$ = `r sample_var$s_var.f0[3]`). For these particular combinations of stimuli, a learner with weak prior beliefs about the category variance will thus correctly come to expect a larger category variance during test, leading to improved recognition accuracy on the L2-accented test tokens. This result is an example of the type of unexpected insights that fully specified linking hypotheses can yield. It also illustrates that some results of perception experiments can be highly dependent on the specific stimulus statistics of the exposure and/or test tokens. A failure to take these factors into account---as remains common---can make results *appear* surprising even when they are not.] Second, L2-accented exposure is predicted to improve recognition accuracy of /d/ compared to L1-accented exposure, and this improvement occurs without equivalent decreases in the recognition accuracy of /t/. 

Looking across the distinct $\kappa_{0,c}$s and $\nu_{0,c}$ values, we make two more observations. On the one hand, faster learning of category means is generally beneficial under the current scenario. When listeners' beliefs about category covariances are held constant (i.e., looking within each row), smaller $\kappa_{0,c}$ values yield higher categorization accuracy. This is a straightforward consequence of the fact that the mean of the /d/ category is shifted in the L2 accents, as shown in Figure \@ref(fig:study-AA-exposure-test-plot). Adapting to the L2 accent thus hinges primarily on learning the new category mean for the /d/ category. On the other hand, updating beliefs about the _variance_ too rapidly is not necessarily desirable, as seen by comparing panels within each column. In the current scenario, the _true_ underlying variance of /d/ is unchanged between the L1 and the L2 accents. When the _category mean_ is updated at slower rates (i.e., see the left four columns), faster updating of _variance_ appears to be beneficial because it allows some ambiguous tokens to be included into /d/. But when $\kappa_{0,c}$s are small (i.e., see the right two columns) and the _category means_ are rapidly adjusted, weaker beliefs about the category covariance thus can run the risk of expanding the categories to a degree that causes miscategorization of relatively ambiguous tokens. This is an example of the flexibility-stability trade-off that human listeners are known to exhibit, and that any theory of adaptive speech perception needs to account for [@kleinschmidt-jaeger2015, pp. 178-182].

Taken together, these results show that changes in representation can generally explain the types of results that are considered the signature of accent adaptation [e.g., @bradlow-bent2008; @clarke-garrett2004; @sidaras2009; @xie2016].

### Changes in decision-making

```{r study-AA-models-changes-in-decision-making-functions}
add_prior_and_posterior_with_changed_response_biases_based_on_exposure.AA <- function(
  data.exposure,
  prior,
  idealized = T,
  decision_rule = if (idealized) "proportional" else "sample",
  keep.update_history = FALSE,
  keep.exposure_data = FALSE
) {
  suppressWarnings(data.exposure %>%
    nest(data = -c(Condition, Subject)) %>%
    crossing(
      posterior.lapse_rate = c(.0005, .005, .05, .5, 1),
      beta_pi = c(0, .1, .8, 2, 4, 8, 16)) %>%
    crossing(
      prior %>%
        filter(prior_kappa == max(prior_kappa), prior_nu == max(prior_nu)) %>%
        nest(prior = everything())) %>%
    group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
    mutate(
      posterior = pmap(
        .l = list(data, prior, posterior.lapse_rate, beta_pi),
        .f = function(.data, .prior, .posterior.lapse_rate, .beta_pi)
          update_NIW_response_bias_incrementally(
            prior = .prior,
            exposure = .data,
            exposure.category = "Item.Category",
            exposure.cues = c("VOT", "f0_Mel"),
            beta = .beta_pi,
            decision_rule = decision_rule,
            noise_treatment = if (idealized) "marginalize" else "sample",
            lapse_treatment = if (idealized) "marginalize" else "sample",
            keep.update_history = keep.update_history,
            keep.exposure_data = keep.exposure_data) %>%
        mutate(lapse_rate = .posterior.lapse_rate))))
}
```

```{r study-AA-models-changes-in-decision-making, echo=FALSE, message=FALSE, warning=FALSE}
range.lapse_rate <- c(0, 1)
range.beta_pi <- c(10^-3, 10^2) 
min_n_simulations_for_bias_update <- 10
max_n_simulations_for_bias_update <- 150
step_n_simulations_for_bias_update <- 2
target_se_for_bias_update <- .01

update_bias_and_categorize_test <- function(
  prior,
  beta_pi,
  exposure,
  test,
  cues = c("VOT", "f0_Mel"),
  control = list(
    min.simulations = min_n_simulations_for_bias_update, 
    max.simulations = max_n_simulations_for_bias_update, 
    step.simulations = step_n_simulations_for_bias_update,
    target_accuracy_se = target_se_for_bias_update),
  add_updates = NULL,
  verbose = FALSE
) {
  u <- 
    lapply(
    X = as_list(1:control[["min.simulations"]]),
    FUN = function(x) 
      update_NIW_response_bias_incrementally(
        prior = prior,
        beta = beta_pi,
        # Reshuffle expose on each run
        exposure = exposure %>% sample_frac(1, replace = F), 
        exposure.category = "Item.Category",
        exposure.cues = cues,
        decision_rule = "proportional",
        noise_treatment = "marginalize",
        lapse_treatment = "marginalize",
        keep.update_history = FALSE,
        keep.exposure_data = FALSE) %>%
      nest(posterior = everything()) %>%
      mutate(sim = x) %>%
      add_test_and_categorize(test)) %>%
    reduce(bind_rows) 
  
  # If existing updates were provided add them to the ones just created before
  # checking whether one of the convergence criteria is reached.
  if (!is.null(add_updates)) 
    u %<>% 
    mutate(sim = sim + max(add_updates$sim)) %>%
    bind_rows(add_updates)
  
  if (verbose) 
    u %>% 
    group_by(sim) %>% 
    summarise(response.mean = mean(response)) %>%
    ungroup() %>%
    summarise(
      n.sims_so_far = n_distinct(sim),
      lapse_rate = first(prior$lapse_rate),
      beta_pi = .env$beta_pi,
      response.mean.mean = mean(response.mean),
      response.mean.se = sd(response.mean)) %>%
    print()
  
  # If max simulations not yet reached AND target_se not yet reached, run more 
  # simulations.
  if (control[["min.simulations"]] < control[["max.simulations"]]) 
    if(u %>% 
       group_by(sim) %>% 
       summarise(response.mean = mean(response)) %>%
       ungroup() %>%
       summarise(response.mean.se = sd(response.mean)) %>%
       pull(response.mean.se) %>%
       { . > control[["target_accuracy_se"]]}) {
      u <-
        update_bias_and_categorize_test(
          prior = prior,
          beta_pi = beta_pi,
          exposure = exposure,
          test = test,
          cues = cues,
          control = list(
            min.simulations = control[["step.simulations"]], 
            max.simulations = control[["max.simulations"]] - control[["min.simulations"]], 
            step.simulations = control[["step.simulations"]],
            target_accuracy_se = control[["target_accuracy_se"]]),
          add_updates = u) 
      }
  
  # Return average predicted accuracy (and its SE) for each item across all simulations  
  u %>% 
  group_by(!!! syms(setdiff(names(.), c("sim", "posterior", "response")))) %>% 
  summarise(
    response.n_sims = n_distinct(sim),
    response.se = sd(response) / sqrt(response.n_sims),
    response = mean(response)) %>%
  relocate(
    !!! syms(setdiff(names(.), c("response", "response.se", "response.n_sims"))), 
    response, response.se, response.n_sims)
}

history.optimization_bias <- tibble(.rows = 0)
get_accuracy_from_updated_bias <- function(
    pars,
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), 
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")),
    verbose = FALSE
) {
  lapse_rate <- pars[1]
  beta_pi <- exp(pars[2])
  max_kappa_nu <- 10000
  
  prior <- 
    model %>% 
    mutate(lapse_rate = .env$lapse_rate) %>%
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = max_kappa_nu, nu = max_kappa_nu)
  
  u <- 
    update_bias_and_categorize_test(
    prior = prior,
    beta_pi = beta_pi,
    exposure = exposure,
    test = test,
    verbose = verbose, 
    control = list(
      min.simulations = min(10, max(3, (1 + beta_pi * 2)^2)), 
      max.simulations = max_n_simulations_for_bias_update, 
      step.simulations = 2,
      target_accuracy_se = .01)) %>%
    get_accuracy()
  
  history.optimization_bias <- 
    bind_rows(
      history.optimization_bias, 
      tibble(lapse_rate = lapse_rate, beta_pi = beta_pi, accuracy = u))
  
  return(u)
}

if (RESET_MODELS || !file.exists(get_path("../models/best_performing_parameters.bias.rds"))) {
  best_performing_parameters.bias <- 
    optim(
      par = c(mean(range.lapse_rate), mean(log(range.beta_pi))), 
      fn = get_accuracy_from_updated_bias,
      verbose = TRUE,
      method = "L-BFGS-B",
      lower = c(min(range.lapse_rate), min(log(range.beta_pi))),
      upper = c(max(range.lapse_rate), max(log(range.beta_pi))), 
      control = list(
        fnscale = -1,   
        factr = 10^9))
  
  best_performing_parameters.bias$par <- c(best_performing_parameters.bias$par[1], exp(best_performing_parameters.bias$par[2]))
  saveRDS(best_performing_parameters.bias, get_path("../models/best_performing_parameters.bias.rds"))
} else {
  best_performing_parameters.bias <- readRDS(get_path("../models/best_performing_parameters.bias.rds"))
}
```


```{r}
# TO DO: Xin, see comments about representational model and how to consolidate code
# Once that is integrated we might not need the wieldy function in the preceding chunk?
d.AA.bias <- 
  d.AA.exposure %>%
  mutate(nrow = row_number()) %>% # TO DO: Xin, what is this for?
  add_prior_and_posterior_with_changed_response_biases_based_on_exposure.AA(prior = m.ia.VOT_f0.AA) %>%
  add_test_tokens(d.AA.test) %>%
  add_categorization_functions() %>%
  group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
  add_categorization() %>%
  distinct() %>%
  left_join(., d.AA.test %>% select(x, Item.Intended_category), by = "x") %>%
  distinct() # TO DO: Xin, why is there a distinct() here?
```

<!-- Here we need to decide whether we a) state that this is just one simulation (which might, however, become confusing if those sims conflict with the optimal results) or b) run a sufficiently high number of sims per parameterization(I suspect that 10-20 is enough). Either way, the caption needs to state what we're doing and the text needs to briefly state why (order-sensitivity of this model). 

NB: we might need to start doing this in the PR section. and introduce the reason there. -->

(ref:AA-result-changes-in-decision-making) Predictions of a model that derives accent adaptation as changes in decision-making. Predicted categorization responses for the test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, depending on the rate at which response biases change ($\beta_{\pi}$) and the rate of attentional lapses ($\lambda$). The highlighted panel is the one closest to the best-performing parameterization ($\beta_{\pi} = `r best_performing_parameters.bias$par[1]`$; $\lambda=`r best_performing_parameters.bias$par[2]`$; overall accuracy$=`r best_performing_parameters.bias$value`$).

```{r plot-AA-results-decision-making-functions}
make_results_plot_decision_making <- function(data, data_best){
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, data = . %>%
        group_by(Condition,posterior.lapse_rate,beta_pi) %>%
        summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
    facet_grid(
      posterior.lapse_rate ~ beta_pi,
      labeller = label_bquote(
        cols = beta[pi] == .(beta_pi),
        rows = lambda == ~.(posterior.lapse_rate))) 

  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = data %>%
          filter(category == Item.Intended_category) %>%
          distinct(posterior.lapse_rate, beta_pi) %>%
          ungroup() %>%
          mutate(
            highlight = 
              ifelse(
                posterior.lapse_rate %in% posterior.lapse_rate[which.min(abs(posterior.lapse_rate - data_best$par[1]))] & 
                  beta_pi %in% beta_pi[which.min(abs(beta_pi - data_best$par[2]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf)
    ) +
    scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
    guides(fill = "none")
}
```

```{r AA-result-changes-in-decision-making, fig.width= base.width * 5, fig.height= base.height * 5 + 1, fig.cap="(ref:AA-result-changes-in-decision-making)", warning=FALSE}
make_results_plot_decision_making(data = d.AA.bias, data_best = best_performing_parameters.bias)
```

Figure \@ref(fig:AA-result-changes-in-decision-making) shows the effects of changes in decision-making, depending on the rate at which response biases change $\beta_{\pi}$ and the lapse rate $\lambda$. For L1-accented exposure, we again see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens, regardless of the specific value of ($\beta_{\pi}$). The second signature result---improved accuracy after L2-accented exposure for /d/---is obtained for sufficiently large $\beta_{\pi}$. As $\beta_{\pi}$ increases, the overall recognition accuracy first increases and then plateaus. The largest improvements are found for the smallest lapse rates and fast-enough change rates (gray panel in Figure \@ref(fig:AA-result-changes-in-decision-making)).

Figure \@ref(fig:AA-result-changes-in-decision-making) also shows that changes in response biases do not necessarily result in the type of zero-sum trade-off that is sometimes attributed to them---accuracy improving by some degree for one category while decreasing by the same degree for the other category. Rather, L2-accented exposure can predict improvements in the *overall* recognition accuracy across both categories (e.g., top right panel). Indeed, zero-sum trade-offs are only expected under the highly implausible assumptions that responses are entirely independent of stimulus properties (bottom row of Figure \@ref(fig:AA-result-changes-in-decision-making)).

### Changes in normalization

```{r study-AA-models-normalization-functions}
add_prior_and_normalize_test_tokens_based_on_exposure.AA <- function(data.exposure, data.test, prior.normalization, prior.categories) {
  # Get prior mean
  mu_0 <- prior.normalization$x_mean[[1]]

  # Get normalization parameters for exposure data
  exposure.normalization <- data.exposure %>%
    group_by(Condition, Subject) %>%
    summarise(
      x_N = length(x),
      x_mean = list(colMeans(reduce(x, rbind))),
      x_cov = list(cov(reduce(x, rbind))))

  data.exposure %>%
    nest(data = -c(Condition, Subject)) %>%
    crossing(
      normalization = factor(c("no normalization", "centered based\non exposure"), levels = c("no normalization", "centered based\non exposure")),
      prior_kappa.normalization = c(1, 4, 16, 64, 256, 1024),
      prior.categories %>%
        filter(prior_kappa == max(prior_kappa) & prior_nu == max(prior_nu)) %>%
        nest(prior = everything())) %>%
    group_by(Condition, Subject, prior_kappa.normalization) %>%
    mutate(posterior = prior) %>%
    crossing(data.test %>% distinct(x)) %>%
    # Normalize test tokens
    mutate(
      # Get inferred mean
      mu_inferred = pmap(
        .l = list(Condition, Subject, prior_kappa.normalization),
        .f = function(currentCondition, currentSubject, currentKappa) {
          x_N <- exposure.normalization[exposure.normalization$Condition == currentCondition & exposure.normalization$Subject == currentSubject, "x_N"][[1]]
          x_bar <- exposure.normalization[exposure.normalization$Condition == currentCondition & exposure.normalization$Subject == currentSubject, "x_mean"][[1]][[1]]
          mu <- 1 / (currentKappa + x_N) * (currentKappa * mu_0 + x_N * x_bar)
          return(mu)
        }),
      x = ifelse(normalization == "no normalization", x, map2(x, mu_inferred, ~ .x - (.y - mu_0)))
      ) %>%
    nest(x = c(x))
}
```

Finally, we compare models that normalize test tokens based on the phonetic inputs experienced during exposure to models that continue to apply normalization based on previous long-term L1 experience. Figure \@ref(fig:AA-result-changes-in-normalization) shows the predicted categorization accuracy following changes in normalization in L1- and L2-accented exposure conditions. We again obtain both signature results of experiments on accent adaptation. For L1-accented exposure, we continue to see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens. For L2-accented exposure, recognition accuracy is improved for /d/ test tokens without the cost of equivalent decreases in recognition accuracy for /t/. That is, like the other two change models, normalization can predict *overall* improvements in recognition accuracy after L2-accented exposure. 

```{r study-AA-models-normalization, echo=FALSE, message=FALSE, warning=FALSE}
range.prior_kappa.normalization <- c(1, 10000)

update_normalization_and_categorize_test <- function(
  prior,
  mu_0 = first(prior_marginal_VOT_f0_stats$x_mean),
  kappa.normalization,
  exposure,
  test
) {
  # Get normalization parameters for exposure data
  exposure.normalization <- 
    exposure %>%
    summarise(
      x_N = length(x),
      x_mean = list(colMeans(reduce(x, rbind))),
      x_cov = list(cov(reduce(x, rbind))))
  
  # Apply normalization based on exposure to test
  mu_inferred <- 1 / 
            (kappa.normalization + exposure.normalization$x_N[[1]]) * 
            (kappa.normalization * mu_0 + exposure.normalization$x_N[[1]] * exposure.normalization$x_mean[[1]])
  test %<>%
    mutate(x = map(x, ~ .x - (mu_inferred - mu_0)))
  
  test %>%
    select(x, Item.Intended_category) %>%
    nest(x = x, Item.Intended_category = c(x, Item.Intended_category)) %>%
    mutate(posterior = list(prior)) 
}

# This function is intended for the optimization run right below it
history.optimization_normalization <- tibble(.rows = 0)
get_accuracy_from_updated_normalization <- function(
    par, 
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), 
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1"))
) {
  kappa.normalization <- exp(par[1])
  max_kappa_nu <- 10000
  
  prior <-
    model %>% 
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = max_kappa_nu, nu = max_kappa_nu)
  
  u <- 
    update_normalization_and_categorize_test(
    prior = prior, 
    kappa.normalization = kappa.normalization,
    exposure = exposure,
    test = test) %>%
    # Don't add test again since it's already in the data
    add_test_and_categorize(test = NULL) %>%
    get_accuracy()
  
  history.optimization_normalization <- 
    bind_rows(
      history.optimization_normalization, 
      tibble(kappa = kappa, accuracy = u))
  
  return(u)
}

best_performing_parameters.normalization <- 
  optim(
    par = log(mean(range.prior_kappa.normalization)), 
    fn = get_accuracy_from_updated_normalization,
    method = "L-BFGS-B",
    lower = log(min(range.prior_kappa.normalization)),
    upper = log(max(range.prior_kappa.normalization)), 
    control = list(
      fnscale = -1,
      factr = 10^8))

best_performing_parameters.normalization$par <- exp(best_performing_parameters.normalization$par[1])

# TO DO: Xin, see comments on other change models about consolidating code.
# This might also mean we don't need the code in the preceding chunk?
d.AA.normalization <-
  d.AA.exposure %>%
  add_prior_and_normalize_test_tokens_based_on_exposure.AA(
    data.test = d.AA.test,
    prior.normalization = prior_marginal_VOT_f0_stats,
    prior.categories = m.ia.VOT_f0.AA) %>%
  group_by(Condition, Subject, prior_kappa.normalization, normalization) %>%
  add_categorization_functions() %>%
  add_categorization() %>%
  distinct() %>%
  ungroup() %>%
  mutate(prior_kappa.normalization = factor(as.character(prior_kappa.normalization), levels = as.character(rev(sort(unique(prior_kappa.normalization))))))

# get ItemID for normalized cue values so that the intended_category is known
pair.ItemID.observationID <- 
  d.AA.normalization %>%
  select(observationID, x, category) %>%
  left_join(., d.AA.test %>% select(x, Item.Intended_category, ItemID), by = "x") %>%
  ungroup() %>%
  filter(!is.na(ItemID)) %>%
  distinct(observationID, ItemID)

d.AA.normalization %<>%
  left_join(pair.ItemID.observationID) %>%
  left_join(d.AA.test %>% select(Item.Intended_category, ItemID), by = "ItemID") %>%
  distinct()
```

(ref:AA-result-changes-in-normalization) Predictions of a model that derives accent adaptation from changes in normalization. Predicted categorization responses for the test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, depending on the relative weighting of previous experience ($\kappa_0$) during the inference of the cue mean during exposure. The highlighted panel is the one closest to the best-performing parameterization ($\kappa_0 = `r best_performing_parameters.normalization$par[1]`$; overall accuracy $=`r best_performing_parameters.normalization$value`$).

```{r plot-AA-results-normalization-functions}
make_results_plot_normalization <- function(data, data_best){
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, 
      data = . %>%
        # TO DO: Xin, somehow we're ending up with different accuracies from your way of calculating accuracy
        # and my (recently added) way of doing it. I'd like to understand why. One possibility is that it has 
        # to do with my questions about the distinct() statement in the code above but there are many other 
        # reasons this could happen. (this is nto specific to normalization but I'm putting this comment here
        # so that we don't forget.)
        group_by(Condition, normalization, prior_kappa.normalization) %>%
        summarise(mAcc = round(mean(response), digits = 2)), 
      mapping = aes(label = mAcc, x = Condition, y = 1), 
      size = geom_text.size) +
    facet_grid(
      . ~ prior_kappa.normalization,
      labeller = label_bquote(
        cols = ~kappa[0] == .(as.character(prior_kappa.normalization)))) 
  
  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = 
          data %>%
          filter(category == Item.Intended_category) %>%
          distinct(prior_kappa.normalization) %>%
          mutate(
            highlight = 
              ifelse(
                prior_kappa.normalization %in% prior_kappa.normalization[which.min(abs(as.numeric(as.character(prior_kappa.normalization)) - data_best$par[1]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf ,ymax = Inf)) +
    scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
    guides(fill = "none")
}
```

```{r AA-result-changes-in-normalization, fig.width= base.width * 5, fig.height= base.height + 1.75, fig.cap="(ref:AA-result-changes-in-normalization)", warning=FALSE}
make_results_plot_normalization(
  d.AA.normalization %>% filter(normalization == "centered based\non exposure"), 
  best_performing_parameters.normalization)
```

## Summary
Paralleling Case Study 1 on perceptual recalibration, we find that any of the three change mechanisms can qualitatively explain the signature results of experiments on accent adaptation. Even non-representational mechanisms---normalization and decision-making---can explain improvements in the perception of L2 accents that differ in complex ways from L1 listeners prior expectations. And, contrary to common assumptions, changes in decision-making can lead to _overall_ improvements in recognition accuracy, rather than only zero-sum trade-offs. Overall improvements in accuracy after exposure to an unfamiliar accent are thus _not_ diagnostic of listeners' adaptation to L2-specific category realizations. This supports the conclusion of Case Study 1: at the level of analysis that is commonly applied in previous work (and thus here), experiments on accent adaptation do not decisively rule out any of the three mechanisms as a driver of the facilitatory effects of exposure.

Beyond the similarity in their qualitative predictions, the three change models exhibit quantitative differences. For the hypothetical L2 accent considered in Case Study 2, the maximal benefits of L2-accented exposure are predicted to be largest for the representational change model (overall improvements from `r get_accuracy_from_updated_representations(log(best_performing_parameters.representations$par), exposure = d.AA.exposure %>% filter(Condition == "L1-accented", Subject == paste0(Condition, ".1")))` to `r best_performing_parameters.representations$value`), followed by the decision-making change model (from `r get_accuracy_from_updated_bias(c(best_performing_parameters.bias$par[1], log(best_performing_parameters.bias$par[2])), exposure = d.AA.exposure %>% filter(Condition == "L1-accented", Subject == paste0(Condition, ".1")))` to `r best_performing_parameters.bias$value`) <!-- TO DO: this will need to be fixed since it's currently selecting a really large beta_pi value --> and smallest in the normalization change model (`r get_accuracy_from_updated_normalization(log(best_performing_parameters.normalization$par), exposure = d.AA.exposure %>% filter(Condition == "L1-accented", Subject == paste0(Condition, ".1")))` to `r best_performing_parameters.normalization$value`). <!-- TO DO: this will need to be changed if we use the new optimization method --> However, these differences do not mean that the representational change model will _always_ perform better than the other two change models. Rather, the quantitative results of Case Study 2 illustrate a critical insight of the ASP framework: the specific predicted benefit of L2-accented exposure under each mechanism depends on the statistics of the exposure and test stimuli, relative to L1 listeners prior expectations. For experiments that employ naturally accented exposure and test tokens, this also means that the results are predicted to depend on how exactly the acoustic-phonetic distributions of an L2 accent deviate from those of the L1 accent.

To further illustrate this point, we simulated four additional scenarios of adaptation to an L2 accent (Panel A in Figure \@ref(fig:AA-plot-results-additional-cases)). Each of these scenarios is representative of attested L2-accents, selected to illustrate some of the dynamics of the different mechanisms and their dependence on exposure and test statistics. The first row of Figure \@ref(fig:AA-plot-results-additional-cases) shows an example of *contrast reduction*, in which the L2 accent shows a greater category overlap than the L1 accent. Here it is simulated by shifting /d/ category towards /t/ along VOT while keeping the latter unchanged [as qualitatively attested for, e.g., vowels in Spanish-accented English, @wade2007]. The second row of Figure \@ref(fig:AA-plot-results-additional-cases) shows a more extreme example, exhibiting almost complete *contrast collapse* [similar to the loss of the [s]-[`r linguisticsdown::cond_cmpl("")`] contrast that can occur in Mandarin-accented English, @zheng-samuel2020]. Here it is simulated by shifting both /d/ and /t/ towards a common mean. The third and four rows show examples of *contrast shift*, where two categories in L2-accented speech are shifted along one or more of the cue dimensions as compared to L1-accented speech. Here it is simulated by shifting both /d/ and /t/ categories towards lower VOT [qualitatively attested for, e.g., word-initial stops in French-accented English, @Sumner2011a]. The only difference between these two scenarios is that the bottom scenario includes an increase in category variance in addition to a shift in the category means.

```{r compare-AA-models-best-three-functions}
# Function to construct change model of each type and select the best performing parameter combination
construct_three_best_models <- function(
    example_label = "example_label",
    experimenter.ideal_observer,
    category_dist_ratio1 = dist.L2_category.cue1,
    category_dist_ratio2 = dist.L2_category.cue2,
    shift_mean_1 = shift.cue1, 
    shift_mean_2 = shift.cue2,
    test.n_block = n.test_block,
    n_test = n.test.token
  ) {
  exposure.data <-
    make_accent_adaptation_exposure_design(
      experimenter.ideal_observer = m.io.VOT_f0.AA, 
      category_dist_ratio1 = dist.L2_category.cue1, 
      category_dist_ratio2 = dist.L2_category.cue2, 
      shift_mean_1 = shift.cue1, 
      shift_mean_2 = shift.cue2) %>%
    add_subjects_to_exposure(n.subject = n.subject)

  test.data <- 
    make_accent_adaptation_test_design(
      exposure.data, experimenter.ideal_observer = m.io.VOT_f0.AA, 
      test.n_block = n.test_block, 
      n_test = n.test.token, 
      category_dist_ratio1 = dist.L2_category.cue1, 
      category_dist_ratio2 = dist.L2_category.cue2, 
      shift_mean_1 = shift.cue1, 
      shift_mean_2 = shift.cue2) %>%
    add_subjects_to_test(n.subject = n.subject)

  d.AA.representations <- 
    exposure.data %>%
    add_prior_and_get_posterior_beliefs_based_on_exposure(prior = m.ia.VOT_f0.AA) %>%
    add_test_tokens(test.data) %>%
    add_categorization_functions() %>%
    group_by(Condition, Subject, prior_kappa, prior_nu) %>%
    add_categorization() %>%
    ungroup() %>%
    mutate_at(
      vars(starts_with("prior_")),
      ~ factor(as.character(.x), levels = as.character(rev(sort(unique(.x)))))) %>%
    distinct() %>%
    left_join(
      test.data %>%
        select(x, Item.Intended_category), by = "x") %>%
    distinct()
  
  d.AA.bias <- 
    exposure.data %>%
    add_prior_and_posterior_with_changed_response_biases_based_on_exposure.AA(prior = m.ia.VOT_f0.AA) %>%
    add_test_tokens(test.data) %>%
    add_categorization_functions() %>%
    group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
    add_categorization() %>%
    distinct() %>%
    left_join(., test.data %>% select(x, Item.Intended_category), by = "x") %>%
    distinct()

  d.AA.normalization <-
    exposure.data %>%
    add_prior_and_normalize_test_tokens_based_on_exposure.AA(
      data.test = test.data,
      prior.normalization = prior_marginal_VOT_f0_stats,
      prior.categories = m.ia.VOT_f0.AA) %>%
    group_by(Condition, Subject, prior_kappa.normalization, normalization) %>%
    add_categorization_functions() %>%
    add_categorization() %>%
    distinct() %>%
    ungroup() %>%
    mutate(prior_kappa.normalization = factor(as.character(prior_kappa.normalization), levels = as.character(rev(sort(unique(prior_kappa.normalization))))))

  # get ItemID for normalized cue values so that the intended_category is known
  pair.ItemID.observationID <- 
    d.AA.normalization %>%
    select(observationID, x, category) %>%
    left_join(., test.data %>% select(x, Item.Intended_category, ItemID), by = "x") %>%
    ungroup() %>%
    filter(!is.na(ItemID)) %>%
    distinct(observationID, ItemID)

  d.AA.normalization %<>%
    left_join(pair.ItemID.observationID) %>%
    left_join(test.data %>% select(Item.Intended_category, ItemID), by = "ItemID") %>%
    distinct()

  # examine best-performing parameterization
  d.AA.representations.rank <- d.AA.representations %>%
    filter(category == Item.Intended_category) %>%
    mutate(prior_kappa = as.numeric(as.character(prior_kappa)),
           prior_nu = as.numeric(as.character(prior_nu))) %>%
    group_by(Condition, prior_kappa, prior_nu) %>%
    summarise(mAcc = mean(response)) %>%
    arrange(-mAcc, -prior_kappa, -prior_nu) %>%
    filter(Condition == "L2-accented")
  best_performing_case.representations <- d.AA.representations.rank[1,]
  
  d.AA.bias.rank <- d.AA.bias %>%
    filter(category == Item.Intended_category) %>%
    group_by(Condition, posterior.lapse_rate, beta_pi) %>%
    summarise(mAcc = mean(response)) %>%
    arrange(-mAcc, -posterior.lapse_rate, -beta_pi) %>%
    filter(Condition == "L2-accented")
  best_performing_case.bias <- d.AA.bias.rank[1,]


  d.AA.normalization.rank <- d.AA.normalization %>%
    filter(category == Item.Intended_category) %>%
    filter(normalization == "centered based\non exposure") %>%
    mutate(prior_kappa.normalization = as.numeric(as.character(prior_kappa.normalization))) %>%
    group_by(Condition, prior_kappa.normalization) %>%
    summarise(mAcc = mean(response)) %>%
    arrange(-mAcc, -prior_kappa.normalization) %>%
    filter(Condition == "L2-accented")
  best_performing_case.normalization <- d.AA.normalization.rank[1,]

  # plot the predicted results for all parameterization and save plots
  p.AA.representations <- make_results_plot_representations(d.AA.representations, best_performing_case.representations)
  ggsave(paste0('../figures/plotly/AA_cases/', example_label, '_representations.png'), plot = p.AA.representations, width = 2*(base.width*length(levels(factor(d.AA.representations$prior_kappa))) + .5), height = 2*(base.height*length(levels(factor(d.AA.representations$prior_nu))) + .5), dpi = 300)
  
  p.AA.bias <- make_results_plot_decision_making(d.AA.bias, best_performing_case.bias)
  ggsave(paste0('../figures/plotly/AA_cases/', example_label, '_bias.png'), plot = p.AA.bias, width = 2*(base.width*length(levels(factor(d.AA.bias$beta_pi))) + .5), height = 2*(base.height*length(levels(factor(d.AA.bias$posterior.lapse_rate))) + .5), dpi = 300)
  
  p.AA.normalization <- make_results_plot_normalization(d.AA.normalization, best_performing_case.normalization)
  ggsave(paste0('../figures/plotly/AA_cases/', example_label, '_normalization.png'), plot = p.AA.normalization, width = 2*(base.width*length(levels(factor(d.AA.normalization$prior_kappa.normalization))) + .5), height = 2*(base.height + .5), dpi = 300)

  # select the parameter combination for each model that results in the best overall accuracy
  d <- bind_rows(
    d.AA.representations %>% 
      filter(category == Item.Intended_category & prior_kappa == best_performing_case.representations$prior_kappa & prior_nu == best_performing_case.representations$prior_nu) %>% 
      mutate(ModelType = "representations"),
    d.AA.bias %>% 
      filter(category == Item.Intended_category & posterior.lapse_rate == best_performing_case.bias$posterior.lapse_rate & beta_pi == best_performing_case.bias$beta_pi) %>%
      mutate(ModelType = "decision making"),
    d.AA.normalization %>%
      filter(category == Item.Intended_category & normalization == "centered based\non exposure" & prior_kappa.normalization == best_performing_case.normalization$prior_kappa.normalization) %>%
      mutate(ModelType = "normalization")) %>%
    mutate(ModelType = factor(ModelType, levels = c("representations", "decision making", "normalization")))

  return(d)
}
```

(ref:AA-plot-results-additional-cases) Predicted adaptation for three types of L2 accents, from top to bottom: contrast reduction, contrast collapse, contrast shift without variance change, contrast shift with variance increase. Predictions were derived for one random experiment of the same number of exposure and test stimuli as in Case Study 2. **Panel A - Distributions of L1-accented and L2-accented categories:** L1-accented categories, represented in light shades, are kept constant across the three scenarios and identical to that in the main scenario above; L2-accented categories, represented by solid ellipses, are varied across scenarios. **Panel B - Change model predictions:** Predicted categorization accuracies for the L2-accented test tokens after L1-accented and L2-accented exposure, for the best-performing parameterizations of each change models. The average accuracy across all test tokens for each condition is shown above the bars. Error bars show 95% bootstrapped confidence intervals. 

```{r AA-result-comparison-best-three-models-contrast-reduction, warning=FALSE}
# Examine a new scenario: contrast reduction
min_VOT <- 5
dist.L2_category.cue1 <- 0.5
dist.L2_category.cue2 <- 0
shift.cue1 <- 0
shift.cue2 <- 0
var.ratio <- 1

d.AA.results.best.contrast_reduction  <- construct_three_best_models(example_label = "contrast_reduction", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-best-three-models-contrast-collapse, warning=FALSE}
# Examine a new scenario: contrast collapse
min_VOT <- -100 # allows negative VOT when generating exposure and test data
dist.L2_category.cue1 <- 0
dist.L2_category.cue2 <- 0
shift.cue1 <- -0.6
shift.cue2 <- 0
var.ratio <- 1

d.AA.results.best.contrast_collapse  <- construct_three_best_models(example_label = "contrast_collapse", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-best-three-models-contrast-shift, warning=FALSE}
# Examine a new scenario: contrast shift
min_VOT <- -100 # allows negative VOT when generating exposure and test data
dist.L2_category.cue1 <- 1
dist.L2_category.cue2 <- 0
shift.cue1 <- -1.2
shift.cue2 <- 0
var.ratio <- 1

d.AA.results.best.contrast_shift  <- construct_three_best_models(example_label = "contrast_shift", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-best-three-models-contrast-shift_var, warning=FALSE, eval = FALSE}
# Examine a new scenario: contrast shift + variance increase
min_VOT <- -100
dist.L2_category.cue1 <- 1
dist.L2_category.cue2 <- 0
shift.cue1 <- -1.2
shift.cue2 <- 0
var.ratio <- 3

d.AA.results.best.contrast_shift_var  <- construct_three_best_models(example_label = "contrast_shift_var", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-plot-target-exposure-additional-cases, message = FALSE, warning=FALSE}
# plot target exposure ellipses
min_VOT = 5
var.ratio = 1
# simulating a case of contrast reduction
d.AA.exposure.target.contrast.reduction <- 
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 0.5, 
    category_dist_ratio2 = 0, 
    shift_mean_1 = 0, 
    shift_mean_2 = 0, 
    exposure.tokens.L1.n = 5000, 
    exposure.tokens.L2.n = 5000)

# simulating a case of contrast collapse
min_VOT = -100
var.ratio = 1
d.AA.exposure.target.contrast.collapse <-  
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 0,
    category_dist_ratio2 = 0,
    shift_mean_1 = -0.6,
    shift_mean_2 = 0,
    exposure.tokens.L1.n = 5000, 
    exposure.tokens.L2.n = 5000) 

# simulating a case of contrast shift: like French-accented English stops, so allowing negative VOTs
min_VOT = -100
var.ratio = 1
d.AA.exposure.target.contrast.shift <- 
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 1, 
    category_dist_ratio2 = 0, 
    shift_mean_1 = -1.2, 
    shift_mean_2 = 0, 
    exposure.tokens.L1.n = 5000, 
    exposure.tokens.L2.n = 5000)

# simulating a case of contrast shift + variance increase
# min_VOT = -100
# var.ratio = 3
# d.AA.exposure.target.contrast.shift_var <- make_accent_adaptation_exposure_design(
#   experimenter.ideal_observer = m.io.VOT_f0.AA, 
#   category_dist_ratio1 = 1, 
#   category_dist_ratio2 = 0, 
#   shift_mean_1 = -1.2, 
#   shift_mean_2 = 0, 
#   exposure.tokens.L1.n = 5000, 
#   exposure.tokens.L2.n = 5000)

d.AA.exposure.target.cases <- rbind(
  d.AA.exposure.target.contrast.reduction %>%
    mutate(Case = "Contrast reduction"),
  d.AA.exposure.target.contrast.collapse %>%
    mutate(Case = "Contrast collapse"),
  d.AA.exposure.target.contrast.shift %>%
    mutate(Case = "Contrast shift")) %>%
  mutate(Case = factor(Case, levels = c("Contrast reduction", "Contrast collapse", "Contrast shift")))

# plot ellipses for actual exposure
p.AA.exposure <- d.AA.exposure.target.cases %>%
  mutate(Title = "Exposure distributions") %>%
  filter(Condition == "L2-accented") %>%
  mutate(Group = factor(paste0(Condition, Item.Category))) %>%
  ggplot(aes(x = VOT, y = f0_Mel, alpha = Condition)) +
  stat_ellipse(aes(color = Group), level = .95, geom = "polygon") +
  stat_ellipse(data = d.AA.exposure.target.cases %>%
                 mutate(Title = "Exposure distributions") %>%
                 filter(Condition == "L1-accented") %>%
                 mutate(Group = factor(paste0(Condition, Item.Category))), aes(fill = Group), level = .95, geom = "polygon") +
  scale_x_continuous(expression("VOT (ms)"), 
                        # breaks=seq(0, 120, by = 30),
                        expand = expansion(mult = .1, add = 0)
                        ) +
  scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0), breaks=seq(0, 500, by = 100)) +
  scale_alpha_discrete(range = c(0.2, 0), guide = "none") +
  coord_cartesian(ylim = c(0, 500)) +
  scale_fill_manual("Category",  values = c(colors.category), guide = "none") +
  scale_color_manual("Category", values = c(colors.category), guide = "none") +
  facet_grid(Case ~ Title, switch = "y") + theme(legend.position="top") 
```

```{r AA-plot-results-additional-cases, fig.width=base.width*4 + .5, fig.height=base.height*3 + .5, fig.cap = "(ref:AA-plot-results-additional-cases)", warning=FALSE}
d.AA.results.best.cases <- rbind(
  d.AA.results.best.contrast_reduction %>%
    mutate(Case = "Contrast reduction"),
  d.AA.results.best.contrast_collapse %>%
    mutate(Case = "Contrast collapse"),
  d.AA.results.best.contrast_shift %>%
    mutate(Case = "Contrast shift") 
  # ,
  # d.AA.results.best.contrast_shift_var %>%
  #   mutate(Case = "Shift + var increase")
) %>%
  mutate(Case = factor(Case, levels = c("Contrast reduction", "Contrast collapse", "Contrast shift"))) %>%  #, "Shift + var increase"
  mutate(parameterization = case_when(
   ModelType == "representations" ~ paste0("kappa = ", prior_kappa, ", nu = ", prior_nu),
   ModelType == "decision making" ~ paste0("lambda = ", posterior.lapse_rate, ", beta = ", beta_pi),
   ModelType == "normalization" ~ paste0("kappa = ", prior_kappa.normalization)))

p.results.best.cases <- d.AA.results.best.cases %>%
  mutate(
    ModelType = factor(paste0("Changes in ", ModelType), levels = c("Changes in representations", "Changes in decision making", "Changes in normalization"))) %>%
  mutate(Label = interaction(ModelType, parameterization, sep = "\n")) %>%
#  distinct(Case, Label) %>%
  ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
  stat_summary(fun = mean,
                 geom="bar", position = pos,
                 width = 0.6) +
  stat_summary(aes(color = Item.Intended_category),
                 fun.data = mean_cl_boot,
                 geom = "uperrorbar",
                 position = pos, width = 0.2) +
  coord_cartesian(ylim =  c(0,1)) +
  scale_color_manual("Category", values = colors.category) +
  scale_fill_manual("Category", values = colors.category) +
  scale_alpha_discrete(range = c(0.2, 1)) +
  xlab("Exposure condition") +
  ylab("Predicted \ncategorization accuracy") +
  geom_text(inherit.aes = FALSE, data = . %>%
                 group_by(Case, Condition, ModelType) %>%
                 summarise(mAcc = round(mean(response), digits = 2)),
            aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
  facet_grid(Case ~ ModelType) +
  theme(legend.position = "top", strip.text.y = element_blank(), panel.grid.major.x = element_blank())

prow <- 
  plot_grid(
    p.AA.exposure, 
    p.results.best.cases + theme(legend.position="none"), 
    labels = c('A)', 'B)'), 
    ncol = 2, 
    rel_widths = c(1.5,3.3))

# extract a legend that is laid out horizontally
legend_prow <- get_legend(p.results.best.cases)
plot_grid(legend_prow, prow, ncol = 1, rel_heights = c(.2, 1)) 
```

The four different accent scenarios differ in terms of the difficulty they are predicted to pose for L1 listeners prior to exposure (see L1-accented exposure condition in Panel B of Figure \@ref(fig:AA-plot-results-additional-cases)). Contrast reduction (first row), for example, resembles the scenario considered in Case Study 2: without informative exposure, L1 listeners unfamiliar with the accent are predicted to exhibit substantially more difficulty with the recognition of /d/ than the recognition of /t/. The opposite is observed for our example of contrast collapse (second row) and contrast shift (bottom two rows). This illustrates how differences in the realization of L2 vs. L1 accents can give rise to starkly different perceptual consequences for L1 listeners. 

The four scenarios also differ in the extent to which exposure to the L2 accent can improve recognition accuracy---here illustrated for the best-performing parameterizations of each change model. We describe three aspects of this. First, confirming the point we raised above, changes of representations do not always outperform the other two mechanisms. For the case of contrast reduction (top row), changes in decision making and normalization perform as well as, or better than, changes in category representations (for the parameter values considered here). This shows that the computational complexity afforded for the representational change model does not always result in higher recognition accuracy than the other two mechanisms.

Second, the predicted benefits of L2-accented exposure vary depending on the L2 accent. This is true even if one compares the best model for each accent scenario---i.e., our best estimate of how well a listener could possibly do under any of the three change mechanisms if confronted with that particular L2 accent. For example, _none_ of the change models predicts significant benefits from L2-accented exposure for contrast collapse (second row). This highlights an important, yet often under-appreciated, insight: sometimes null results are _predicted_ for exposure-test experiment, because of the specific stimulus statistics [see @tan2021; @zheng-samuel2020]. Put differently, an absence of improvements does not _necessarily_ constitute evidence against accent adaptation or any of the three mechanisms.^[This also means that individual differences in accent adaptation around such a null effect are likely _not_ indicative of the individuals ability to adapt to L2-accented speech but instead are likely to reflect random noise [under-mining the central argument made in, e.g., @zheng-samuel2020].] 

Third, the same change model can support more or less overall accuracy improvements, depending on the specific differences between the L2 and L1 accent. Consider the case of contrast shift (third row). While changes in representations and normalization both predict large improvements in overall recognition accuracy in the L2-accented exposure, changes in decision making do not (i.e., the difference in the predicted accuracy across L1- and L2-accented exposure conditions is negligible). This outcome aligns with the zero-sum trade-offs often attributed to this change mechanism (i.e., The recognition of /d/ improves at the cost of the recognition of /t/). The heterogeneity of the results across the three scenarios makes it clear that the trade-off is not an inherent property of the decision-making mechanism. It is instead a possible outcome arising from a combination of the specific change model and properties of the L2 accent (or L1-L2 accent differences). 

While the scenarios considered here cover only a small subset of the cross-accent differences that exists in the world, they serve as an initial illustration of a general challenge to adaptive speech perception: different types of cross-talker differences (including L2 accents) constitute different adaptive challenges, and benefit from different _rates_ and _types_ of adaptation. Since the properties of unfamiliar accents are not _a priori_ known to listeners, this means that the optimal level of flexibility is achieved if the neural and cognitive systems underlying adaptive speech perception may adjust based on the properties of the input [see discussion of the trade-off between flexibility and stability in @kleinschmidt-jaeger2015, pp. 180-182]. In Figure \@ref(fig:AA-plot-results-additional-cases) (Panel B), this is reflected in the fact that the best-performing parameterizations vary across the different accent scenarios. Beyond the accent scenarios examined here, naturally produced L2 accents exhibit substantial variations: L2-accented categories often deviate from L1-accented categories not only in terms of their means (as simulated here), <!-- TO DO: might change --> but also in variances and/or covariances [the category-specific correlation between cues, e.g., @smith2019; @wade2007; @vaughn2019; @xie-jaeger2020]. These arguably more complex differences can pose difficulties to L1-listeners and researchers alike. At the same time, they also offer a unique and fertile ground to test the relative engagement of each change mechanism. Because the three change mechanisms would predict distinct perceptual outcomes depending on the properties of the exposure and test stimuli, these differences can be used to distinguish between model predictions. In the general discussion, we explore how such tests are possible.

<!--TO DO: tidy up the table and add caption-->
```{r}
  parameters.results.best.cases <- d.AA.results.best.cases %>%
    mutate(
      ModelType = factor(paste0("Changes in ", ModelType), levels = c("Changes in representations", "Changes in decision making", "Changes in normalization"))) %>%
  #  mutate(Label = interaction(ModelType, parameterization, sep = "\n")) %>%
    select(Case, ModelType, parameterization) %>%
    distinct() %>%
    pivot_wider(names_from = ModelType, values_from = parameterization)
  colnames(parameters.results.best.cases) = c("L2 accent scenario", "Changes in representations", "Changes in decision making", "Changes in normalization")
  knitr::kable(parameters.results.best.cases)
```

