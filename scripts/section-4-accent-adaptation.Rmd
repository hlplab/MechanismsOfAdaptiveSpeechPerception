# Case Study 2: accent adaptation {#sec:AA}
The second paradigm we consider focuses on *naturally* accented speech---e.g., dialectal [@smith2014], varietal [@shaw2018], or second language (L2) accents [@bradlow-bent2008; @eisner2013; @sidaras2009; @weil2001a]. Typically, exposure to an unfamiliar accent is compared to a control condition in which listeners are exposed to a familiar accent, most often the 'standard' variety of listeners' L1. Following exposure, listeners in either group are tested on the unfamiliar accent. In an influential study, @bradlow-bent2008 had listeners transcribe a total of 160 sentences of either Mandarin-accented English or L1-accented English, distributed over two sessions on two separate days. In a subsequent test phase, both groups transcribed Mandarin-accented sentences. Participants who were first exposed to Mandarin-accented English were significantly more accurate during test (over 90% accuracy compared to about 80%). This finding has since been replicated and extended [for review, see @baeseberk2020]. We now know that substantially shorter exposure can lead to similarly large improvements in accuracy [e.g., 80 sentences in a single session, about 2-5 minutes of speech, @xie2021jep], that these can persist over hours and days [@witteman2015; @xie2018lcn], and that accent adaptation can sometimes generalize across talkers of the same or similar accents [e.g., @baeseberk2013; @tzeng2016; @xie2021jep]. <!-- Facilitatory effects of exposure have also been demonstrated in tasks that tap into online processing of naturally accented speech, including cross-modal matching [@clarke-garrett2004; @xie2018jasa], phonological priming [@eisner2013; @xie2017], and visual world eye-tracking [@dahan2008; @trude2012talker].-->

Critically, naturally accented speech typically differs from listeners' expectations in ways that can be considerably more complex than the types of manipulation studied in perceptual recalibration experiments. It is thus not clear whether the same mechanisms that underlie perceptual recalibration also underlie adaptation to natural accents. This question continues to attract attention [see recent reviews, @baeseberk2018; @bent-baeseberk2021; @zheng-samuel2020], because its answer determines whether perceptual recalibration and related paradigms---which afford increased control over stimuli characteristics but come at the risk of decreased ecological validity---can shed light on how listeners overcome the challenge of cross-talker variability in everyday speech.

As in the case of perceptual recalibration, accent adaptation is often attributed to changes in category representations [e.g., @bent-baeseberk2021; @sidaras2009; @eisner2013; @sumner2009; @tzeng2016; @xie2016jep]. This might be in part due to the (false) assumption that changes in response biases can only explain trade-offs in accuracy: as the accuracy for one category improves, it has to inevitably decrease for the other category. Under this assumption, overall improvements in recognition accuracy could not possibly be explained by changes in decision-making. Similarly, pre-linguistic normalization has rarely been considered as a possible mechanism for accent adaptation---perhaps because the complexities of L2 accents make it counter-intuitive that simple centering would be sufficient to explain adaptation to L2-accented speech. However, to the best of our knowledge, previous work has never actually put these intuitions to the test. Case Study 2 thus assesses whether the signature results of experiments on accent adaptation necessarily distinguish between the three change mechanisms. 

Compared to perceptual recalibration paradigms, experiments on accent adaptation exhibit more heterogeneity in their designs and tasks. Case Study 2 focuses on experiments in which treatment exposure employs a single talker with the unfamiliar accent, and the test phase heard by both groups of participants employs previously unheard stimuli from the same accented talker [e.g., @eisner2013; @schertz2015; @xie2017].^[The approach we take here can also be applied to cross-talker generalization---i.e., paradigms in which speech from different talkers of the same (or different) accent as the exposure talker is used during test [@baeseberk2013; @bradlow-bent2008; @sidaras2009].] For example, in an exposure-test experiment, @xie2016jep exposed two groups of L1-English listeners to Mandarin-accented speech. In exposure, participants in the treatment group heard 30 words ending in /d/ (e.g., _overload_) together with 60 word fillers and 90 nonword fillers. Participants in the control group did not hear any words that contain /d/. After exposure, participants in both conditions categorized recordings of 60 minimal /d/-/t/ pairs (e.g., _seed_ vs. _seat_) spoken by the same Mandarin-accented talker. Participants who heard /d/-final words during exposure were more likely to categorize /d/-final words correctly during test, compared to the control condition. At the same time, a non-significant numerical *de*crease in accuracy was observed for /t/-final words [see also @xie2018lcn]. Related paradigms have yielded similar results for other contrasts and other L1-L2 pairs [e.g., @zheng-samuel2020; @eisner2013].

For Case Study 2, we construct a hypothetical experiment that closely follows this type of design. For the sake of continuity, we focus on the same syllable-initial /d/-/t/ contrast used in Case Study 1. Like in Case Study 1, we analyze the data following the conventions of the field. For experiments on accent adaptation, this typically means that the facilitatory effects of L2-accented exposure are assessed through improvements in categorization accuracy during test.

As in Case Study 1, we ask whether any of the three change mechanisms can *qualitatively* explain the signature results found in @xie2016jep and other similar work. The simulation process requires us to specify the properties of the L1 and L2 accents---and therefore their differences---at the level of acoustic-phonetic distributions. These properties---although well characterized in studies on L2 production---are rarely ever reported in the literature on adaptive speech perception, and their predicted consequences for perception are even less commonly modeled (for an exception, see Tan et al., 2021). In the discussion, we consider their influence on the model behaviors and further reflect on how the systematic patterns of differences between L1- and L2-accents may affect the relative benefit of each of the three change mechanisms.

## Data
The stimulus generation procedure is described in detail in the SI (\@ref(sec:SI-AA)).

```{r study-AA-setup, message=FALSE}
# Set plotting aesthetics
geom_text.size <- theme_get()$text$size *5/14 # change from mm to point scale; keeping the size of geom_text the same as the rest of text in ggplot
pos <- position_dodge(.6)

# Random seed for this study
set.seed(123498765)

# Which categories is this experiment about?
categories.AA <- c("/d/", "/t/")
conditions.AA <- c("L1-accented", "L2-accented")
colors.category <- colors.condition <- colors.voicing
shapes.category <- shapes.condition <- c(15, 17)
linetypes.condition <- c(1,2)

# Number of subjects
n.subject <- 1

# Number of exposure tokens for each category, shifted and typical
n.exposure.token <- 30

# arbitrarily assume that the variance of the speech stimuli in the experiment is $\frac{1}{`r my_experimenter.variability_reduction`}$th of that observed in natural productions (applying to L1-accented speech only)
my_experimenter.variability_reduction <- 1

# Number of test blocks
n.test_block <- 1
# Number of test tokens per category
n.test.token <- 60


m.io.VOT_f0.AA <-
  m.VOT_f0_MVG  %>%
  filter(category %in% categories.AA) %>%
  droplevels() %>%
  make_stop_VOTf0_ideal_observer() %>%
  arrange(category)


prior_kappa.plot = 4^(1:6) # specify what parameters to plot
prior_nu.plot = 4^(1:6) 
 
m.ia.VOT_f0.AA <-
  crossing(
    prior_kappa = prior_kappa.plot,
    prior_nu = prior_nu.plot) %>%
  rowwise() %>%
  mutate(ideal_adaptor = map2(prior_kappa, prior_nu, ~ make_stop_VOTf0_ideal_adaptor(m = m.io.VOT_f0.AA, kappa = .x, nu = .y))) %>%
  unnest(ideal_adaptor) %>%
  arrange(category)
```

### Exposure phase
Figure \@ref(fig:study-AA-exposure-test-plot)A shows the stimuli for the exposure and test phases of the experiment. In the L2-accented exposure condition, listeners hear word recordings containing L2-accented initial /d/ and /t/ (30 tokens per category). In the control condition with L1-accented exposure, listeners hear the same words but from an L1-accented talker.^[The use of L1-accented exposure, rather than L2-accented exposure without /d/, as control follows a typical design in accent adaptation studies [e.g., @bradlow-bent2008], rather than @xie2016jep. Xie and colleagues instead employed L2-accented exposure without /d/, using the same L2-accented talker as during test [see also @eisner2013]. For the present purpose, both types of control conditions lead to identical predictions (as long as the L2-accented control exposure successfully avoids conveying non-negligible amount of information about the categories considered during the test phase) since the current implementation of the three change models do not consider talker-switch costs.] For L1-accented exposure, the category likelihoods were set to match those observed in @chodroff-wilson2018, after C-CuRE normalization (i.e., the distributions shown in Figure \@ref(fig:demonstrate-normalization)B). This follows the same approach we took for the typical tokens in Case Study 1. For L2-accented exposure, /d/ and /t/ categories were created following the production statistics of Korean-accented English, as reported in @schertz2015. The /d/ and /t/ categories in this L2 accent differ from the L1 accent in both category means and variances. First, the VOTs of both categories are longer in the L2 accent, making the /d/s more similar to /t/s for native-English listeners who rely on VOT as a primary cue to voicing distinction. As a result, the recognition accuracy is expected to be lower for /d/ than for /t/ for the L1-accented exposure condition. Second, the two categories are more separable along f0 in the L2 accent, that is, f0 is weighted more heavily as a cue to distinguish voicing in Korean-accented English (i.e., lower f0 signaling a /d/ than a /t/ category) compared to native English. This is a representative case of cue reweighting, a common type of situation that occurs in L2 accents. 

```{r define-AA-accent-category-distribution}
# How separable the categories are in the L2 accent?
# e.g., dist.L2_category.cue1  -- The ratio of category mean difference along the primary cue in L2 to that in L1 (the primary cue does not have to be the same cue dimension across accents); 1 means the distance is equal to that in L1 accent.

# # Case 1: presented in the main text, with clear cue-weighting (e.g., word-initial stops in Korean-accented English).
dist.L2_category.cue1 <- 1
dist.L2_category.cue2 <- 0.22
shift.cue1 <- 0.5
shift.cue2 <- 0.22
var.ratio = 1 # Multiple category variance by a constant; var.ratio = 1 means no change in category variance

min_VOT = 5 # Set the minimal values for randomly generated test token cue values to make sure it is physiologically possible
min_f0_Mel = 20
example_label = "Cue_reweighting"
```


```{r study-AA-exposure-functions}
# Get acoustic locations that correspond to targeted response proportions.
get_parameters_phonetic_contrast.AA <- function(ideal.observer,
                                                category_dist_ratio1,
                                                category_dist_ratio2,
                                                shift_mean_1 = 0,
                                                shift_mean_2 = 0) {

   # native categories
   mu_native_A <- ideal.observer %>% filter(category == "/d/") %>% pull(mu) %>% .[[1]]
   Sigma_native_A <- ideal.observer %>% filter(category == "/d/") %>% pull(Sigma) %>% .[[1]]
   mu_native_B <- ideal.observer %>% filter(category == "/t/") %>% pull(mu) %>% .[[1]]
   Sigma_native_B <- ideal.observer %>% filter(category == "/t/") %>% pull(Sigma) %>% .[[1]]

   # nonnative categories
   mu_nonnative_B <- mu_native_B
   Sigma_nonnative_B <- matrix(c(Sigma_native_B[1]*var.ratio, Sigma_native_B[2], Sigma_native_B[3], Sigma_native_B[4]), nrow=2)

   # The larger the category_dist_ratio, the greater distance along that cue dimension (1 = VOT, 2 = f0) between the two categories in the L2 accent.
   # If category_dist_ratio1 = category_dist_ratio2 = 0, then the two categories overlap entirely in the L2 accent.
   # If category_dist_ratio1 = category_dist_ratio2 = 1, then /d/-/t/ will have the same distance as that in L1-accent along the primary cue dimension.

     if(example_label != "Contrast_collapse"){
       mu_nonnative_A <- c(mu_nonnative_B[1] * (1 + category_dist_ratio1 * (mu_native_A[1]- mu_native_B[1])/mu_native_B[1]),
                         mu_nonnative_B[2] * (1 + category_dist_ratio2 * (mu_native_A[1]- mu_native_B[1])/mu_native_B[1]))
  
     Omega_native_A <- cov2cor(Sigma_native_A)
     Omega_nonnative_A <- matrix(c(Omega_native_A[1], Omega_native_A[2], Omega_native_A[3], Omega_native_A[4]), nrow=2) # change omega if orientation of category dispersion changes
     Sigma_nonnative_A <- cor2cov(Omega_nonnative_A, sqrt(diag(Sigma_native_A))*var.ratio)
     }else{
       mu_nonnative_A <- mu_nonnative_B
       Sigma_nonnative_A <- Sigma_nonnative_B}
     
   # Allow a shift of nonnative category means
    shift1 <- (mu_nonnative_A[1]+ mu_nonnative_B[1])/2*shift_mean_1
    shift2 <- (mu_nonnative_A[2]+ mu_nonnative_B[2])/2*shift_mean_2
    mu_nonnative_A <- c(mu_nonnative_A[1]+ shift1,
                        mu_nonnative_A[2]+ shift2)
    mu_nonnative_B <- c(mu_nonnative_B[1]+ shift1,
                        mu_nonnative_B[2]+ shift2)
    
    tau_native_A <-  sqrt(diag(Sigma_native_A))
    Omega_native_A <- cov2cor(Sigma_native_A)
    tau_native_B <-  sqrt(diag(Sigma_native_B))
    Omega_native_B <- cov2cor(Sigma_native_B)
    tau_nonnative_A <-  sqrt(diag(Sigma_nonnative_A))
    Omega_nonnative_A <- cov2cor(Sigma_nonnative_A)
    tau_nonnative_B <-  sqrt(diag(Sigma_nonnative_B))
    Omega_nonnative_B <- cov2cor(Sigma_nonnative_B)

    # plot and check whether the two categories are simulated as intended
    d.model <- tibble(
      speech = c(rep("native", 2), rep("nonnative", 2)),
      category = rep(c("A", "B"), 2),
      mu = list(mu_native_A, mu_native_B, mu_nonnative_A, mu_nonnative_B),
      tau = list(tau_native_A, tau_native_B, tau_nonnative_A, tau_nonnative_B),
      Omega = list(Omega_native_A, Omega_native_B, Omega_nonnative_A, Omega_nonnative_B),
      Sigma = list(Sigma_native_A, Sigma_native_B, Sigma_nonnative_A, Sigma_nonnative_B)) %>%
      group_by(speech, category) %>%
      mutate(Model = list(list(
        mu = Reduce("+", mu) / length(mu),
        Sigma = Reduce("+", Sigma) / length(Sigma))))

    return(d.model)
}

# make the design of an exposure phase
make_accent_adaptation_exposure_design = function(
  # Ideal observer describing the true perceptual system for two categories.
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,

  # How separate are the two categories in L2 accent?
  category_dist_ratio1 = dist.L2_category.cue1,
  category_dist_ratio2 = dist.L2_category.cue2,
  shift_mean_1 = 0,
  shift_mean_2 = 0,
  exposure.tokens.L1.n = n.exposure.token,
  exposure.tokens.L2.n = n.exposure.token,
  quiet = F
) {
  if (!quiet) message(
    paste0("Making exposure design with ", exposure.tokens.L1.n, " L1-accented tokens and ", exposure.tokens.L2.n, " L2-accented tokens."))

  d.model.both <- 
    get_parameters_phonetic_contrast.AA(
      experimenter.ideal_observer,
      category_dist_ratio1,
      category_dist_ratio2,
      shift_mean_1,
      shift_mean_2)
  
  d.model.native <- d.model.both %>%
    filter(speech == "native") %>%
    droplevels()
  d.model.nonnative <- d.model.both %>%
    filter(speech == "nonnative") %>%
    droplevels()

  # Create basic tibble
  tibble(
    Phase = "exposure",
    ItemID = as.character(1:(exposure.tokens.L1.n + exposure.tokens.L2.n)),
    Item.Type = factor(c(rep("L1-accented", exposure.tokens.L1.n), rep("L2-accented", exposure.tokens.L2.n)))) %>%
    # Add all unique design combinations
    crossing(
      Item.Category = factor(experimenter.ideal_observer$category)) %>%
    # Get cue value of item: if L2-accented then use L2 category parameters. If not sample from L1-accented category distribution
    mutate(
      Condition = Item.Type,
      x = map2(
        Item.Type,
        Item.Category,
        ~ case_when(
          .x == "L2-accented" & .y == experimenter.ideal_observer$category[[1]] ~ rmvnorm(1, d.model.nonnative$mu[[1]], d.model.nonnative$Sigma[[1]] / experimenter.variability_reduction),
          .x == "L2-accented" & .y == experimenter.ideal_observer$category[[2]] ~ rmvnorm(1, d.model.nonnative$mu[[2]], d.model.nonnative$Sigma[[2]] / experimenter.variability_reduction),
          .x == "L1-accented" & .y == experimenter.ideal_observer$category[[1]] ~ rmvnorm(1, d.model.native$mu[[1]], d.model.native$Sigma[[1]] / experimenter.variability_reduction),
          .x == "L1-accented" & .y == experimenter.ideal_observer$category[[2]] ~ rmvnorm(1, d.model.native$mu[[2]], d.model.native$Sigma[[2]] / experimenter.variability_reduction),
          T ~ NA_real_))) %>%
    mutate(
      VOT = unlist(map(x, ~ max(min_VOT, .x[1]))), 
      f0_Mel = unlist(map(x, ~ max(min_f0_Mel, .x[2]))),  
      x = map2(VOT, f0_Mel, ~ c("VOT" = .x, "f0_Mel" = .y)))
}

#<!--TO DO remove this function if already existed in main file; leaving it here now so that this section can be knitted independently--!>
add_subjects_to_exposure <- function(
  d,
  n.subject,
  quiet = F
) {
  assert_that(!("Subject" %in% names(d)),
              msg = "This data frame already seems to contain subjects")
  if (!quiet) message(paste("Adding", n.subject, "subjects per exposure condition."))

  d %>%
    group_by(Condition) %>%
    crossing(Subject = factor(1:n.subject)) %>%
    mutate(Subject = paste(Condition, Subject, sep = ".")) %>%
    select(Condition, Subject, Phase, ItemID, Item.Category, Item.Type, x, everything()) %>%
    ungroup()
}
```

```{r study-AA-exposure-make-data, message=FALSE}
if(SET_SEED){
  set.seed(42007)
}

d.AA.exposure <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)

# calculate sample variance along each cue dimension for each exposure condition
sample_var <- d.AA.exposure %>%
  group_by(Condition, Item.Category) %>%
  summarise(s_var.f0 = var(f0_Mel),
    s_var.vot = var(VOT))

# get prior variance along each cue dimension for each exposure condition
population_var <- data.frame(
  c("/d/", "/d/", "/t/", "/t/"),
  c("VOT", "f0", "VOT", "f0"),
  c(m.VOT_f0_MVG$Sigma[2][[1]][1], m.VOT_f0_MVG$Sigma[2][[1]][4],
    m.VOT_f0_MVG$Sigma[6][[1]][1], m.VOT_f0_MVG$Sigma[6][[1]][4]))
colnames(population_var) = c("category", "cue", "prior_variance")
```

### Test phase
During test, listeners from both exposure conditions hear the same recordings from the same L2-accented talker used in the L2-accented exposure condition. 60 test tokens are randomly sampled from the distribution of each L2-accented category, half from the /d/ category and half from the /t/ category (Figure \@ref(fig:study-AA-exposure-test-plot)B).

```{r study-AA-test-functions}

# make the design of an test phase
make_accent_adaptation_test_design = function(
 # d,
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,
  category_dist_ratio1 = dist.L2_category.cue1,
  category_dist_ratio2 = dist.L2_category.cue2,
  shift_mean_1 = 0,
  shift_mean_2 = 0,
  test.n_block = n.test_block, 
  n_test = n.test.token     
) {
  d.model.both <- 
    get_parameters_phonetic_contrast.AA(
      experimenter.ideal_observer,
      category_dist_ratio1,
      category_dist_ratio2 ,
      shift_mean_1,
      shift_mean_2)
  
  ## --------------------------------------------------------------------------------
  # create a *natural* non-native test set
  ## --------------------------------------------------------------------------------
  x <- 
    rbind(
      rmvnorm(
        n_test,
        d.model.both %>% filter(speech == "nonnative", category == "A") %>% pull(mu) %>% .[[1]],
        d.model.both %>% filter(speech == "nonnative", category == "A") %>% pull(Sigma) %>% .[[1]] / experimenter.variability_reduction),
      rmvnorm(
        n_test,
        d.model.both %>% filter(speech == "nonnative", category == "B") %>% pull(mu) %>% .[[1]],
        d.model.both %>% filter(speech == "nonnative", category == "B") %>% pull(Sigma) %>% .[[1]] / experimenter.variability_reduction))
  
  d.test <-
    tibble(
      Item.Intended_category = sort(rep(c("A", "B"), n_test)),
      VOT = unlist(map(x[,1], ~ max(min_VOT, .x))), # cue 1
      f0_Mel = unlist(map(x[,2], ~ max(min_f0_Mel, .x))), # cue 2
      x = map2(VOT, f0_Mel, ~ c("VOT" = .x, "f0_Mel" =  .y))) %>%
    mutate(
      Item.Intended_category = ifelse(Item.Intended_category == "A", "/d/", "/t/"),
      Phase = "test",
      ItemID = row_number(),
      Item.Type = "test",
      Item.Category = NA,
    ) %>%
    crossing(  
       Condition = factor(conditions.AA),
      Block = 1:test.n_block) %>%
    droplevels()
}

add_subjects_to_test <- function(
  d,
  n.subject,
  quiet = F
) {
  assert_that(!("Subject" %in% names(d)),
              msg = "This data frame already seems to contain subjects")
  if (!quiet) message(paste("Adding", n.subject, "subjects per exposure condition."))

  d %>%
    group_by(Condition) %>%
    crossing(Subject = factor(1:n.subject)) %>%
    mutate(Subject = paste(Condition, Subject, sep = ".")) %>%
    select(Condition, Subject, Phase, ItemID, Item.Category,Item.Intended_category, Item.Type, x, everything()) %>%
    ungroup()
}
```

```{r study-AA-test-make-data}
# if(SET_SEED){
#   set.seed(42007)
# } #DEBUG
d.AA.test <- make_accent_adaptation_test_design(
 # d.AA.exposure, 
  experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)
```

(ref:study-AA-exposure-test-plot) **Panel A - Exposure:** Distribution of the stimuli used during the exposure phase of the accent adaptation experiment (`r n.exposure.token` tokens each of `r conditions.AA[1]` and `r conditions.AA[2]` `r paste(categories.AA, collapse = " and ")`, respectively). **Panel B - Test:**  Distribution of the stimuli used during the test phase of the accent adaptation experiment. The test tokens come from L2-accented speech (`r n.test.token` tokens per category) and are identical for the two exposure conditions. Ellipses show the 95% probability mass for the two categories in L2-accented exposure speech.

```{r plot-AA-exposure-test-functions}
# Function to plot exposure and test data
make_exposure_test_plot <- function(exposure.data, test.data){
  p.AA.exposure <- 
    exposure.data %>%
    mutate(Condition = paste0("Exposure: ", Condition)) %>%
    ggplot(aes(x = VOT, y = f0_Mel, color = Item.Category)) +
    geom_point(alpha = 0.5) +
    geom_rug(data = . %>%
                 group_by(Condition, Item.Category) %>%
                 summarise(VOT = mean(VOT), f0_Mel = mean(f0_Mel)), show.legend = FALSE) +
    scale_x_continuous(expression("VOT (ms)"), 
                        expand = expansion(mult = .1, add = 0)) +
    scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0), breaks=seq(0, 500, by = 100)) +
    coord_cartesian(ylim = c(0, 500)) +
    scale_color_manual("Category", breaks = categories.AA, values = colors.category) +
    scale_fill_manual("Category", breaks = categories.AA, values = colors.category) +
    facet_grid(~ Condition)
  
  limits <- get_plot_limits(p.AA.exposure)
  breaks <- get_plot_breaks(p.AA.exposure)

  p.AA.test <- 
    test.data %>%
    filter(Condition == "L2-accented") %>%
    mutate(SpeechLabel = "Test: L2-accented") %>%
    ggplot(aes(x = VOT, y = f0_Mel, label = as.numeric(factor(ItemID)))) +
    geom_point(aes(color = Item.Intended_category), alpha = 0.5) +
    stat_ellipse(
      data = 
        exposure.data %>%
        filter(Condition == "L2-accented"), 
      mapping = aes(fill = Item.Category), 
      level = .95, alpha = 0.3, geom = "polygon") +
    scale_x_continuous(
      expression("VOT (ms)"),
      expand = expansion(mult = .1, add = 0),
      breaks = c(breaks$xbreaks),
      limits = c(limits$xmin, limits$xmax)) +
    scale_y_continuous(
      expression("f0 (Mel)"),
      breaks = c(breaks$ybreaks),
      expand = expansion(mult = .1, add = 0),
      limits = c(limits$ymin, limits$ymax)) +
    scale_color_manual("Category", values = colors.category) +
    scale_fill_manual("Category", values = colors.category, guide = "none") +
    coord_cartesian(xlim = c(limits$xmin, limits$xmax), ylim = c(limits$ymin, limits$ymax), expand = FALSE) +
    facet_grid(~ SpeechLabel)

  prow = plot_grid(
    p.AA.exposure + theme(legend.position="none"),
    p.AA.test + theme(legend.position="none"),
    labels = c('A)', 'B)'), nrow = 1, rel_widths = c(2,1.1))
  
  legend_prow <- 
    get_legend(
      p.AA.exposure +
        guides(color = guide_legend(title.position = "left", nrow = 1)))

  p.output <-  plot_grid(legend_prow, prow, ncol = 1, rel_heights = c(.1, 1))
  return(p.output)
}

appender <- function(string, prefix = "Changes in\n") paste0(prefix, string)
```

```{r study-AA-exposure-test-plot, fig.width=base.width*3 + .5, fig.height=base.height + .5, fig.cap="(ref:study-AA-exposure-test-plot)", warning=FALSE}
make_exposure_test_plot(exposure.data = d.AA.exposure, test.data = d.AA.test)
```

```{r add-subjects-AA-exposure-test-data}
d.AA.exposure %<>% add_subjects_to_exposure(n.subject = n.subject)
d.AA.test %<>% add_subjects_to_test(n.subject = n.subject)
```

## Results
Paralleling Case Study 1, we ask which of the three change models can account for the signature results of accent adaptation experiments. Specifically, we assess for each change model whether it can explain the two types of signature results observed in, for example, Xie et al. [-@xie2016jep]: (1) for L1-accented exposure, /d/ test tokens are recognized with lower accuracy than /t/ test tokens, since the categories are shifted towards /t/ along the primary cue VOT, creating a deviation from L1 listeners' expectations; and (2) L2-accented exposure should lead to a significant increase in recognition accuracy for /d/ test tokens without an equivalent decrease in the accuracy for /t/ test tokens, resulting in increased overall accuracy. 

As was done for Case Study 1, we first model changes in representations, and then compare them against changes in decision-making and normalization. We consider a larger range of parameterizations than in Case Study 1, because the models with the highest accuracy for L2-accented exposure (henceforth, the best-performing parameterization) in some cases fell outside of the range of parameter values considered in Case Study 1. These best-performing parameterizations were determined by bounded quasi-Newton optimization of the recognition accuracy during test after L2-accented exposure [@byrd1995, implemented in function \texttt{optim()} in R], and are indicated in all result plots we present below. Table \@ref(tab:AA-table-models-parameter-setting) summarizes the parameter ranges considered by the optimization algorithm.

```{r AA-table-models-parameter-setting, warning=FALSE}

collapse_rows_df <- function(df, variable){
  group_var <- enquo(variable)
  df %>%
    group_by(!! group_var) %>%
    mutate(groupRow = 1:n()) %>%
    ungroup() %>%
    mutate(!!quo_name(group_var) := ifelse(groupRow == 1, as.character(!! group_var), "")) %>%
    select(-c(groupRow))
}

# set parameter ranges considered by the optimization algorithm
## parameters for representations model
range.prior_kappa <- c(1, 10000)
range.prior_nu <- c(4, 10000) 
## parameters for bias model
range.lapse_rate <- c(0, 1)
range.beta_pi <- c(10^-3, 10^2)

## parameters for normalization model
range.prior_kappa.normalization <- c(1, 10000)

d.parameters <- data.frame(c("Changes in representations", "Changes in representations", "Changes in decision-making", "Changes in decision-making", "Changes in normalization"),
  c("$\\kappa_{c,0}$", "$\\nu_{c,0}$", "$\\lambda$", "$\\beta_\\pi$", "$\\kappa_0$"),
                           c(c(paste0(range.prior_kappa[1], " to " ,range.prior_kappa[2])),
                           c(paste0(range.prior_nu[1], " to " ,range.prior_nu[2])),
                           c(paste0(range.lapse_rate[1], " to " ,range.lapse_rate[2])),
                           c(paste0(range.beta_pi[1], " to " ,range.beta_pi[2])),
                           c(paste0(range.prior_kappa.normalization[1], ", " ,range.prior_kappa.normalization[2]))))
colnames(d.parameters) = c("Model","Parameters", "Range")

knitr::kable(d.parameters %>% collapse_rows_df(Model), caption = "Parameter range considered for each change model", booktabs = TRUE, escape = FALSE) %>%
  collapse_rows(1, latex_hline = "major", valign = "middle") %>% 
#  row_spec(5, hline_after = T) %>% 
  kable_styling()

```

<!-- For changes in representations, the optimization procedure searched values from the smallest possible value (1 for $\kappa_{c,0}$ and 4 for $\nu_{0,c}$) to 10000 (virtually no changes in representations). -->
```{r study-AA-models-common-functions}
# This and the next function collect code chunks that are repeated a few times below
# (if test is null it won't be added and it's assumed that there already is a nested
# column of test tokens called  x and another column indicating the mapping from x 
# to intended categories; this is required only for the normalization updating).
add_test_and_categorize <- function(data, test = NULL) {
  assert_that(is_tibble(data))
  assert_that(
    "posterior" %in% names(data),
    msg = "data must contain a column posterior that is a nested ideal adaptor.")
  if (is.null(test)) {
    assert_that(
      all(c("x", "Item.Intended_category") %in% names(data)),
      msg = "If test is NULL, data must contains column x with nested test tokens and Item.Intended_category with mappings of x to the intended/correct category.")
    assert_that(is_list(data$x))
  }
  
  data %>%
    { if (!is.null(test)) add_test_tokens(., test) else select(., -Item.Intended_category) } %>%
    add_categorization() %>%
    # Keep only posterior of intended category
    { if (!is.null(test)) {
      left_join(
        .,
        test %>%
          select(x, Item.Intended_category), 
        by = "x") 
    } else {
      left_join(
        .,
        first(data$Item.Intended_category),
        by = "x")
    }} %>%
    filter(category == Item.Intended_category)
}

get_accuracy <- function(categorizations) {
  categorizations %>%
    pull(response) %>%
      mean()
}

basic_AA_result_plot <- function(data) {
  data %>%
    # Show the response only for intended category
    filter(category == Item.Intended_category) %>% 
    ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
    list(
      stat_summary(
        fun = mean,
        geom = "bar", position = pos, width = 0.6),
      stat_summary(
        aes(color = Item.Intended_category),
        fun.data = mean_cl_boot,
        geom = "uperrorbar",
        position = pos, width = 0.2),
      coord_cartesian(ylim = c(0, 1.1)),
      scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1)),
      scale_color_manual("Category", values = colors.category, aesthetics = c("color", "fill")),
      scale_alpha_discrete(range = c(0.2, 1), guide = "none"),
      scale_x_discrete(labels= c("L1-accented", "L2-accented")),
      xlab("Exposure condition"),
      ylab("Predicted categorization accuracy"),
      myGplot.defaults(base_size = 14, set_theme = F),
      theme(legend.position = "top", 
            axis.text.x = element_text(angle = 22.5, hjust = 1),
      panel.grid.major.x = element_blank()))
}
```


### Changes in representations

```{r study-AA-models-changes-in-representations-functions, echo=FALSE, message=FALSE, warning=FALSE}
update_representations_and_categorize_test <- function(
  prior,
  exposure,
  test,
  cues = c("VOT", "f0_Mel")
) {
  update_NIW_ideal_adaptor_incrementally(
    prior = prior,
    exposure = exposure,
    exposure.category = "Item.Category",
    exposure.cues = cues,
    noise_treatment = "marginalize",
    lapse_treatment = "marginalize",
    method = "label-certain",
    keep.update_history = FALSE,
    keep.exposure_data = FALSE) %>%
    nest(posterior = everything()) %>%
    add_test_and_categorize(test)
}

# This function is intended for the optimization run right below it
# (we also store the history of the optimizers search)
history.optimization_representations <- tibble(.rows = 0)
get_accuracy_from_updated_representations <- function(
    pars,
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), 
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1"))
) {
  kappa <- exp(pars[1])
  nu <- exp(pars[2])
  
  prior <- 
    model %>% 
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = kappa, nu = nu)
  
  u <-
    update_representations_and_categorize_test(
    prior = prior,
    exposure = exposure,
    test = test) %>%
    get_accuracy()
  
  history.optimization_representations <<- 
    bind_rows(
      history.optimization_representations, 
      tibble(kappa = kappa, nu = nu, accuracy = u))
  
  return(u)
}
```


```{r study-AA-models-changes-in-representations, echo=FALSE, message=FALSE, warning=FALSE}
find_best_model.representations.AA <- function(){
  if (RESET_MODELS || !file.exists(paste0("../models/best_performing_parameters.representations_", example_label,".rds"))) {
    best_performing_parameters.representations <-
    optim(
      # Parameters are kappa, nu
      par = c(mean(log(range.prior_kappa)), mean(log(range.prior_nu))),
      fn = get_accuracy_from_updated_representations,
      # The only method that works for multiple parameters with bounds
      method = "L-BFGS-B",
      # Stop searching outside of those bounds
      lower = c(min(log(range.prior_kappa)), min(log(range.prior_nu))),
      upper = c(max(log(range.prior_kappa)), max(log(range.prior_nu))),
      control = list(
        # trace = 1,
        fnscale = -1,   # negative scale since we are seeking to maximize accuracy
        factr = 10^8))  # smaller factr means higher optimum but slower convergence
  
    best_performing_parameters.representations$par <- exp(best_performing_parameters.representations$par)
    
    saveRDS(best_performing_parameters.representations, get_path(paste0("../models/best_performing_parameters.representations_", example_label,".rds")))
    
    saveRDS(history.optimization_representations, get_path(paste0("../models/d.AA.history.optimization.representations_", example_label,".rds")))
     
  } else {
    best_performing_parameters.representations <- readRDS(get_path(paste0("../models/best_performing_parameters.representations_", example_label,".rds")))
    history.optimization_representations <- readRDS(get_path(paste0("../models/d.AA.history.optimization.representations_", example_label,".rds")))
  }
  return(best_performing_parameters.representations)
}

plot_sample_model.representations.AA <- function(
    prior_kappa = prior_kappa,
    prior_nu = prior_nu
){
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/d.AA.results.representations_", example_label,".rds")))){
    m.ia.VOT_f0.AA.plot_sample <-
      crossing(
        prior_kappa = prior_kappa,
        prior_nu = prior_nu) %>%
      rowwise() %>%
      mutate(ideal_adaptor = map2(prior_kappa, prior_nu, ~ make_stop_VOTf0_ideal_adaptor(m = m.io.VOT_f0.AA, kappa = .x, nu = .y))) %>%
      unnest(ideal_adaptor) %>%
      arrange(category)
    
    representations.pred <- d.AA.exposure %>%
      nest(data = -c(Condition, Subject)) %>%
          crossing(
            m.ia.VOT_f0.AA.plot_sample %>%
              nest(prior = -c(prior_kappa, prior_nu))) %>%
      group_by(Condition, Subject, prior_kappa, prior_nu) %>%
      group_modify(~ update_representations_and_categorize_test(prior = .x$prior[[1]], exposure = .x$data[[1]], test = d.AA.test %>% filter(Condition == "L2-accented")), .keep = TRUE) %>%
      mutate_at(vars(starts_with("prior_")), ~factor(.x)) %>%
      mutate_at(vars(starts_with("prior_")), fct_rev) %>%
      ungroup()
      
    saveRDS(representations.pred, get_path(paste0("../models/d.AA.results.representations_", example_label,".rds"))) #
    } else {
    representations.pred <- readRDS(get_path(paste0("../models/d.AA.results.representations_", example_label,".rds")))
    }
  return(representations.pred)
}

 
d.AA.representations <- plot_sample_model.representations.AA(
  prior_kappa = prior_kappa.plot,
  prior_nu = prior_nu.plot
)
best_performing_parameters.representations <- find_best_model.representations.AA()
```

(ref:AA-result-changes-in-representations) Predictions of a learning model that derives accent adaptation as changes in category representations. Predicted categorization accuracies for the L2-accented test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, as a function of the strength of the prior beliefs in category means ($\kappa_{c,0}$) and covariances ($\nu_{c,0}$). The average accuracy across /d/ and /t/ is shown above the bars for each exposure condition. Error bars show 95% bootstrapped confidence intervals. The highlighted panel is the one closest to the best-performing parameterization ($\kappa_{0,c} = `r best_performing_parameters.representations$par[1]`$; $\nu_{c,0}=`r best_performing_parameters.representations$par[2]`$; overall accuracy $=`r best_performing_parameters.representations$value`$). 

```{r plot-AA-results-representations-functions}

make_results_plot_representations <- function(data, data_best) {
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, 
      data = . %>%
        group_by(Condition, prior_nu, prior_kappa) %>%
        summarise(mAcc = round(mean(response), digits = 2)), 
      mapping = aes(label = mAcc, x = Condition, y = 1.05), 
      size = geom_text.size) +
    facet_grid(
      prior_nu ~ prior_kappa,
      labeller = label_bquote(
        cols = {kappa[.(categories.AA[1])~","~0] == kappa[.(categories.AA[2])~","~0]} == .(as.character(prior_kappa)),
        rows = {nu[.(categories.AA[1])~","~0] == nu[.(categories.AA[2])~","~0]} == .(as.character(prior_nu)))) 
  
  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = data %>%
          filter(category == Item.Intended_category) %>%
          distinct(prior_kappa, prior_nu) %>%
          mutate(
            highlight = 
              ifelse(
                prior_kappa %in% prior_kappa[which.min(abs(as.numeric(as.character(prior_kappa)) - data_best$par[1]))] & #highlight the parameters (within the plotted range) that are closest to the best performing parameterization
                  prior_nu %in% prior_nu[which.min(abs(as.numeric(as.character(prior_nu)) - data_best$par[2]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf)) +
  scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
  guides(fill = "none")
}
```

```{r AA-result-changes-in-representations, fig.width= base.width * 6, fig.height= base.height * 6 + 1, fig.cap="(ref:AA-result-changes-in-representations)", warning=FALSE}
make_results_plot_representations(data = d.AA.representations, data_best = best_performing_parameters.representations)
```

Figure \@ref(fig:AA-result-changes-in-representations) shows the predicted categorization accuracy after exposure to L1- or L2-accented speech, as a function of the strength of the prior beliefs for the category means and variances. First consider the top left panel, where both $\kappa_{c,0}$s and $\nu_{c,0}$ have very high values. This simulates a extremely slow learner who has strong prior beliefs for both category means and variances such that the small number of tokens heard during exposure phase has minimal influence on subsequent perception of the test tokens. Therefore, the performance from both L1- and L2-accented exposure conditions are identical, reflecting how a listener would categorize based on their long-term prior experience predominantly. 

As $\kappa_{c,0}$s and $\nu_{c,0}$ get smaller (i.e., weaker prior beliefs), the effects of exposure conditions become more noticeable. Both of the signature results described above can be predicted by changes in category representations. First, for L1-accented exposure, the accuracy is always predicted to be lower for /d/ than for /t/, indicating that /d/ is often misheard as /t/. Second, L2-accented exposure is predicted to improve recognition accuracy of /d/ compared to L1-accented exposure, and this improvement occurs without equivalent decreases in the recognition accuracy of /t/. 
Looking across the distinct $\kappa_{c,0}$s and $\nu_{c,0}$ values, we make two more observations. First, faster learning of category means is generally beneficial under the current scenario. When listeners' beliefs about category covariances are held constant (i.e., looking within each row), smaller $\kappa_{c,0}$ values yield higher categorization accuracy. This straightforwardly follows from the fact that the mean of both /d/ and /t/ are shifted in the L2 accents, as shown in Figure \@ref(fig:study-AA-exposure-test-plot). Adapting to the L2 accent thus hinges primarily on learning the new category mean for the /d/ category. Second, rapid updating of beliefs about the _variance_ is also beneficial, as seen by comparing panels within each column. This benefit is present throughout, albeit more detectable at slower rates of category mean updating (i.e., see the left three columns). In the current scenario, the _true_ underlying variance is slightly increased along VOT and reduced along f0 in the L2 accent, compared to that in the L1 accent. Therefore, fast learning of category covariance is helpful, although its effect is not as pronounced as the learning of category means.

Taken together, these results show that changes in representation can generally explain the types of results that are considered the signature of accent adaptation [e.g., @bradlow-bent2008; @clarke-garrett2004; @sidaras2009; @xie2016].

### Changes in decision-making

```{r study-AA-models-changes-in-decision-making-functions, echo=FALSE, message=FALSE, warning=FALSE}
update_bias_and_categorize_test <- function(
  prior,
  lapse_rate,
  beta_pi,
  exposure,
  test,
  cues = c("VOT", "f0_Mel"),
  control = list(
    min.simulations = 5, 
    max.simulations = 10, 
    step.simulations = 1, 
    target_accuracy_se = .005),
  add_updates = NULL,
  verbose = FALSE
) {
  u <- 
    lapply(
    X = as_list(1:control[["min.simulations"]]),
    FUN = function(x) 
      update_NIW_response_bias_incrementally(
        prior = prior,
        beta = beta_pi,
        # Reshuffle expose on each run
        exposure = exposure %>% sample_frac(1, replace = F), 
        exposure.category = "Item.Category",
        exposure.cues = cues,
        decision_rule = "proportional",
        noise_treatment = "marginalize",
        lapse_treatment = "marginalize",
        keep.update_history = FALSE,
        keep.exposure_data = FALSE) %>%
      mutate(lapse_rate = .env$lapse_rate) %>%
      nest(posterior = everything()) %>%
      mutate(sim = x) %>%
      add_test_and_categorize(test)) %>%
    reduce(bind_rows) 
  
  # If existing updates were provided add them to the ones just created before
  # checking whether one of the convergence criteria is reached.
  if (!is.null(add_updates))
    u %<>%
    mutate(sim = sim + n_distinct(add_updates$sim)) %>%
    bind_rows(add_updates)

  u.test <- 
    u %>%
    group_by(sim) %>%
    summarise(response.mean = mean(response)) %>%
    ungroup() %>%
    summarise(
      n.sims_so_far = n_distinct(sim),
      min.additional_simulations = .env$control[["min.simulations"]],
      max.additional_simulations = .env$control[["max.simulations"]] - min.additional_simulations,
      step.simulations = .env$control[["step.simulations"]],
      target_accuracy_se = .env$control[["target_accuracy_se"]],
      lapse_rate = first(.env$prior$lapse_rate), 
      response.mean.mean = mean(response.mean),
      response.mean.se = sd(response.mean) / sqrt(n_distinct(sim)))

  if (verbose)
    u.test %>%
    print()
  
  
  # If max simulations not yet reached AND target_se not yet reached, run more
  # simulations.
 if (control[["min.simulations"]] < control[["max.simulations"]] & (u.test %>%
    pull(response.mean.se) %>%
    { . > control[["target_accuracy_se"]]})) {

   u <-
     update_bias_and_categorize_test(
       prior = prior,
       lapse_rate = lapse_rate,
       beta_pi = beta_pi,
       exposure = exposure,
       test = test,
       cues = cues,
       verbose = verbose,
       control = list(
         min.simulations = control[["step.simulations"]], # after running the minimal number of simulations (min.simulations), increase min.simulations by a few steps (step.simulations) until the max.simulations is reached or the standard error of the response mean (response.mean.se) reaches the predetermined criteria
         max.simulations = control[["max.simulations"]] - control[["min.simulations"]],
         step.simulations = control[["step.simulations"]],
         target_accuracy_se = control[["target_accuracy_se"]]),
       add_updates = u)

 } 
  return(u)
}


get_average_bias_results_across_simulations <- function(data){
  output <- data %>%
    group_by(!!! syms(setdiff(names(.), c("sim","posterior","response")))) %>%
    summarise(
      response.n_sims = n_distinct(sim),
      response.se = sd(response) / sqrt(response.n_sims),
      response = mean(response)) %>%
    relocate(
     !!! syms(setdiff(names(.), c("response", "response.se", "response.n_sims"))),
     response, response.se, response.n_sims) %>%
    ungroup()
  
  return(output)
}

history.optimization_bias <- tibble(.rows = 0)
get_accuracy_from_updated_bias <- function(
    pars,
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), # find the best-performing parameterization for the L2-accented condition
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")),
    verbose = FALSE
) {
  lapse_rate <- pars[1]
  beta_pi <- exp(pars[2])
  max_kappa_nu <- 10000

  prior <-
    model %>%
    mutate(lapse_rate = .env$lapse_rate) %>%
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = max_kappa_nu, nu = max_kappa_nu)
  
  u <-
    update_bias_and_categorize_test(
    prior = prior,
    lapse_rate = lapse_rate, 
    beta_pi = beta_pi,
    exposure = exposure,
    test = test,
    verbose = verbose,
    control = list(
      min.simulations = min(10, max(3, (1 + beta_pi * 2)^2)),
      max.simulations = 12,
      step.simulations = 2,  
      target_accuracy_se = .002))
  
  u <- get_average_bias_results_across_simulations(u) %>%
    get_accuracy()

history.optimization_bias <<-
  bind_rows(
    history.optimization_bias,
    tibble(lapse_rate = lapse_rate, beta_pi = beta_pi, accuracy = u))

  return(u)
}
```

```{r study-AA-models-changes-in-decision-making, echo=FALSE, message=FALSE, warning=FALSE}

find_best_model.bias.AA <- function(){
  if(SET_SEED){
      set.seed(42007)
      }
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))) {
    
    best_performing_parameters.bias <-
      optim(
        par = c(mean(range.lapse_rate), mean(log(range.beta_pi))),
        fn = get_accuracy_from_updated_bias,
        method = "L-BFGS-B",
        lower = c(min(range.lapse_rate), min(log(range.beta_pi))),
        upper = c(max(range.lapse_rate), max(log(range.beta_pi))),
        control = list(
          fnscale = -1,
          factr = 10^8))

  best_performing_parameters.bias$par <- c(best_performing_parameters.bias$par[1], exp(best_performing_parameters.bias$par[2]))
  
  saveRDS(best_performing_parameters.bias, get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))
  saveRDS(history.optimization_bias, get_path(paste0("../models/d.AA.history.optimization.bias_", example_label,".rds")))
  } else {
  best_performing_parameters.bias <- readRDS(get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))
  history.optimization_bias <- readRDS(get_path(paste0("../models/d.AA.history.optimization.bias_", example_label,".rds")))
  }
  return(best_performing_parameters.bias)
}

plot_sample_model.bias.AA <- function(
    lapse_rate = lapse_rate,
    beta_pi = beta_pi
    ){
  if(SET_SEED){
    set.seed(42007)
  }
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/d.AA.results.bias_", example_label,".rds")))){
  
      bias.pred <- d.AA.exposure %>%
        nest(data = -c(Condition, Subject)) %>%
        crossing(
          posterior.lapse_rate = lapse_rate, 
          beta_pi = beta_pi) %>% 
        crossing(
          m.ia.VOT_f0.AA %>%
            filter(prior_kappa == max(prior_kappa), prior_nu == max(prior_nu)) %>%
            nest(prior = everything())) %>%
          group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
          group_modify(~ update_bias_and_categorize_test(prior = .x$prior[[1]],
                                                     lapse_rate = .x$posterior.lapse_rate,
                                                     beta_pi = .x$beta_pi,
                                                     exposure = .x$data[[1]],
                                                     test = d.AA.test %>% filter(Condition == "L2-accented")), .keep = TRUE, verbose = T) %>%
          mutate_at(vars(starts_with("prior_")), ~factor(.x)) %>%
          mutate_at(vars(starts_with("prior_")), fct_rev) %>%
          ungroup()
  
  saveRDS(bias.pred, get_path(paste0("../models/d.AA.results.bias_", example_label,".rds"))) # save results for bias because it takes a long time to run
  } else {
  bias.pred <- readRDS(get_path(paste0("../models/d.AA.results.bias_", example_label,".rds")))
  }
  return(bias.pred) 
}

##------------------------------------------
  # decision-making model
##------------------------------------------

posterior.lapse_rate.plot = c(0, 0.005, 0.05, 0.5, 1) 
beta_pi.plot = c(0, 0.1, 0.5, 1, 4) # specify what parameters to plot
  
d.AA.bias <- plot_sample_model.bias.AA(
  lapse_rate = posterior.lapse_rate.plot,
  beta_pi = beta_pi.plot
)

best_performing_parameters.bias <- find_best_model.bias.AA()
```

<!-- NB: we might need to start doing this (i.e., running multiple simulations) in the PR section. and introduce the reason there. -->

(ref:AA-result-changes-in-decision-making) Predictions of a model that derives accent adaptation as changes in decision-making. Predicted categorization responses for the test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, depending on the rate at which response biases change ($\beta_{\pi}$) and the rate of attentional lapses ($\lambda$). For each parameterization, multiple simulations are run until the standard error of the predicted mean accuracy of the `r conditions.AA[2]` condition reaches a pre-set threshold or pre-set maximal number of simulations are reached. We set the maximal number of simulations to 12 although most of the time a convergence is reached within this limit. Predictions averaged across all simulations are shown. The highlighted panel is the one closest to the best-performing parameterization ($\beta_{\pi} = `r best_performing_parameters.bias$par[1]`$; $\lambda=`r best_performing_parameters.bias$par[2]`$; overall accuracy$=`r best_performing_parameters.bias$value`$).

```{r plot-AA-results-decision-making-functions}

make_results_plot_decision_making <- function(data, data_best){
  
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, data = . %>%
        group_by(Condition, posterior.lapse_rate, beta_pi) %>%
        summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1.05), size = geom_text.size) +
    facet_grid(
      posterior.lapse_rate ~ beta_pi,
      labeller = label_bquote(
        cols = beta[pi] == .(beta_pi),
        rows = lambda == ~.(posterior.lapse_rate))) 

  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = data %>%
          filter(category == Item.Intended_category) %>%
          distinct(posterior.lapse_rate, beta_pi) %>%
          ungroup() %>%
          mutate(
            highlight = 
              ifelse(
                posterior.lapse_rate %in% posterior.lapse_rate[which.min(abs(posterior.lapse_rate - data_best$par[1]))] & 
                  beta_pi %in% beta_pi[which.min(abs(beta_pi - data_best$par[2]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf)
    ) +
    scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
    guides(fill = "none")
}

```

```{r AA-result-changes-in-decision-making, fig.width= base.width * 5, fig.height= base.height * 5 + 1, fig.cap="(ref:AA-result-changes-in-decision-making)", warning=FALSE}
make_results_plot_decision_making(data = get_average_bias_results_across_simulations(d.AA.bias), data_best = best_performing_parameters.bias)
```

Because response biases are updated only when a prediction error is observed (that is, parameter updating does not necessarily occur after every single token), changes in decision making are sensitive to the order of exposure items. To minimize any effects of order-sensitivity, we run a sufficiently large number of simulations for each parameterization of $\beta_{\pi}$ and $\lambda$. Figure \@ref(fig:AA-result-changes-in-decision-making) shows the effects of changes in decision-making as a function of the rate at which response biases change $\beta_{\pi}$ and the lapse rate $\lambda$, averaged across all simulations for each parameterization. For L1-accented exposure, we again see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens, regardless of the specific value of ($\beta_{\pi}$). The second signature result---improved accuracy after L2-accented exposure for /d/---is obtained for sufficiently large $\beta_{\pi}$. As $\beta_{\pi}$ increases, the overall recognition accuracy first increases and then plateaus. The largest improvements are found for the smallest lapse rates and fast-enough change rates (gray panel in Figure \@ref(fig:AA-result-changes-in-decision-making)). Importantly, the largest improvements are similar in magnitude to that from the representational change model, which is computationally more complex.

Figure \@ref(fig:AA-result-changes-in-decision-making) also shows that changes in response biases do not necessarily result in the type of zero-sum trade-off that is sometimes attributed to them---accuracy increasing by some degree for one category while decreasing by the same degree for the other category. Rather, L2-accented exposure can predict improvements in the *overall* recognition accuracy across both categories (e.g., top right panel). Indeed, zero-sum trade-offs are only expected under the highly implausible assumptions that responses are entirely independent of stimulus properties (bottom row of Figure \@ref(fig:AA-result-changes-in-decision-making)).

### Changes in normalization

Finally, we compare models that normalize test tokens based on the phonetic inputs experienced during exposure to models that continue to apply normalization based on previous long-term L1 experience. Figure \@ref(fig:AA-result-changes-in-normalization) shows the predicted categorization accuracy following changes in normalization in L1- and L2-accented exposure conditions. We again obtain both of the signature results of experiments on accent adaptation. For L1-accented exposure, we continue to see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens. For L2-accented exposure, recognition accuracy is improved for /d/ test tokens without the cost of equivalent decreases in recognition accuracy for /t/. That is, like the other two change models, normalization can predict *overall* improvements in recognition accuracy after L2-accented exposure. And again, with a sufficiently fast updating rate, this computationally simpler model yields the amount of improvements similar to that predicted by the representational change model. 

```{r study-AA-models-normalization-functions, echo=FALSE, message=FALSE, warning=FALSE}

update_normalization_and_categorize_test <- function(
  prior,
  mu_0 = first(prior_marginal_VOT_f0_stats$x_mean),
  kappa.normalization,
  exposure,
  test
) {
  # Get normalization parameters for exposure data
  exposure.normalization <- 
    exposure %>%
    summarise(
      x_N = length(x),
      x_mean = list(colMeans(reduce(x, rbind))),
      x_cov = list(cov(reduce(x, rbind))))
  
  # Apply normalization based on exposure to test
  mu_inferred <- 1 / 
            (kappa.normalization + exposure.normalization$x_N[[1]]) * 
            (kappa.normalization * mu_0 + exposure.normalization$x_N[[1]] * exposure.normalization$x_mean[[1]])

  test %<>%
    mutate(
      x_unnormalized = x,
      x = map(x, ~ .x - (mu_inferred - mu_0)))
  
  test %>%
    select(x_unnormalized, x, Item.Intended_category) %>%
    nest(x = x, Item.Intended_category = c(x, Item.Intended_category, x_unnormalized)) %>%
    mutate(posterior = list(prior)) %>%
    # Don't add test again since it's already in the data
    add_test_and_categorize(test = NULL) 
}

# This function is intended for the optimization run right below it
history.optimization_normalization <- tibble(.rows = 0)
get_accuracy_from_updated_normalization <- function(
    par, 
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), 
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1"))
) {
  kappa.normalization <- exp(par[1])
  max_kappa_nu <- 10000
  
  prior <-
    model %>% 
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = max_kappa_nu, nu = max_kappa_nu)

  
  u <- 
    update_normalization_and_categorize_test(
    prior = prior, 
    kappa.normalization = kappa.normalization,
    exposure = exposure,
    test = test) %>%
    get_accuracy()
  
  history.optimization_normalization <<- 
    bind_rows(
      history.optimization_normalization, 
      tibble(kappa.normalization = kappa.normalization, accuracy = u)) 
  
  return(u)
}
```

```{r study-AA-models-normalization, echo=FALSE, message=FALSE, warning=FALSE}

find_best_model.normalization.AA <- function(){
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))) {
    best_performing_parameters.normalization <-
      optim(
        par = log(mean(range.prior_kappa.normalization)),
        fn = get_accuracy_from_updated_normalization,
        method = "L-BFGS-B",
        lower = log(min(range.prior_kappa.normalization)),
        upper = log(max(range.prior_kappa.normalization)),
        control = list(
          fnscale = -1,
          factr = 10^8))
    best_performing_parameters.normalization$par <- exp(best_performing_parameters.normalization$par[1])
    
    saveRDS(best_performing_parameters.normalization, get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))
    saveRDS(history.optimization_normalization, get_path(paste0("../models/d.AA.history.optimization.normalization_", example_label,".rds")))
    } else {
    best_performing_parameters.normalization <- readRDS(get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))
    history.optimization_normalization <- readRDS(get_path(paste0("../models/d.AA.history.optimization.normalization_", example_label,".rds")))
    }
  return(best_performing_parameters.normalization)
}

plot_sample_model.normalization.AA <- function(
    prior_kappa.normalization = prior_kappa.normalization
){
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/d.AA.results.normalization_", example_label,".rds")))){
     normalization.pred <- d.AA.exposure %>%
       nest(data = -c(Condition, Subject)) %>%
       crossing(
          prior_kappa.normalization = prior_kappa.normalization,
          m.ia.VOT_f0.AA %>%
            filter(prior_kappa == max(prior_kappa) & prior_nu == max(prior_nu)) %>%
            nest(prior = everything())) %>%
       group_by(Condition, Subject, prior_kappa.normalization) %>%
       group_modify(~ update_normalization_and_categorize_test(prior = .x$prior[[1]],
                                                     kappa.normalization = .x$prior_kappa.normalization,
                                                     exposure = .x$data[[1]],
                                                     test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1"))), .keep = TRUE) %>%
      mutate_at(vars(starts_with("prior_")), ~factor(.x)) %>%
      mutate_at(vars(starts_with("prior_")), fct_rev) %>%
      ungroup()
     
     saveRDS(normalization.pred, get_path(paste0("../models/d.AA.results.normalization_", example_label,".rds")))
   } else {
     normalization.pred <- readRDS(get_path(paste0("../models/d.AA.results.normalization_", example_label,".rds")))
   }
  return(normalization.pred)
}

  ##------------------------------------------
  # normalization model
  ##------------------------------------------

prior_kappa.normalization.plot = c(1, 4, 16, 64, 256, 1024) # specify what parameters to plot

best_performing_parameters.normalization <- find_best_model.normalization.AA()
d.AA.normalization <- plot_sample_model.normalization.AA(
  prior_kappa.normalization= prior_kappa.normalization.plot
)
```

(ref:AA-result-changes-in-normalization) Predictions of a model that derives accent adaptation from changes in normalization. Predicted categorization responses for the test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, depending on the relative weighting of previous experience ($\kappa_0$) during the inference of the cue mean during exposure. The highlighted panel is the one closest to the best-performing parameterization ($\kappa_0 = `r best_performing_parameters.normalization$par[1]`$; overall accuracy $=`r best_performing_parameters.normalization$value`$).

```{r plot-AA-results-normalization-functions}
make_results_plot_normalization <- function(data, data_best){
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, 
      data = . %>%
        group_by(Condition, prior_kappa.normalization) %>%
        summarise(mAcc = round(mean(response), digits = 2)), 
      mapping = aes(label = mAcc, x = Condition, y = 1.05), 
      size = geom_text.size) +
    facet_grid(
      . ~ prior_kappa.normalization,
      labeller = label_bquote(
        cols = ~kappa[0] == .(as.character(prior_kappa.normalization)))) 
  
  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = 
          data %>%
          filter(category == Item.Intended_category) %>%
          distinct(prior_kappa.normalization) %>%
          mutate(
            highlight = 
              ifelse(
                prior_kappa.normalization %in% prior_kappa.normalization[which.min(abs(as.numeric(as.character(prior_kappa.normalization)) - data_best$par[1]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf ,ymax = Inf)) +
    scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
    guides(fill = "none")
}
```

```{r AA-result-changes-in-normalization, fig.width= base.width * 5, fig.height= base.height + 1.75, fig.cap="(ref:AA-result-changes-in-normalization)", warning=FALSE}
make_results_plot_normalization(d.AA.normalization, best_performing_parameters.normalization)
```

## Summary
Paralleling Case Study 1 on perceptual recalibration, we find that any of the three change mechanisms can qualitatively explain the signature results of experiments on accent adaptation. Even non-representational mechanisms---normalization and decision-making---can explain improvements in the perception of L2 accents that differ in complex ways from L1 listeners prior expectations. And, contrary to common assumptions, changes in decision-making can lead to _overall_ improvements in recognition accuracy, rather than only zero-sum trade-offs. Overall improvements in accuracy after exposure to an unfamiliar accent are thus _not_ diagnostic of listeners' adaptation to L2-specific category realizations. This supports the conclusion of Case Study 1: at the level of analysis that is commonly applied in previous work (and thus here), experiments on accent adaptation do not decisively rule out any of the three mechanisms as a driver of the facilitatory effects of exposure.

For the current hypothetical L2 accent considered in Case Study 2, the three models are not only similar in their qualitative predictions, but they also yield highly comparable quantitative results in terms of the maximal benefits of L2-accented exposure (compare the highlighted panels across Figure \@ref(fig:AA-result-changes-in-representations)-\@ref(fig:AA-result-changes-in-normalization)). It is not always the case. Rather, the process by which these quantitative results of Case Study 2 are obtained illustrates a critical insight of the ASP framework: the specific predicted benefit of L2-accented exposure under each mechanism depends on the statistics of the exposure and test stimuli, relative to L1 listeners prior expectations. For experiments that employ naturally accented exposure and test tokens, this also means that the results are predicted to depend on how exactly the acoustic-phonetic distributions of an L2 accent deviate from those of the L1 accent.

To further illustrate this point, we simulated three additional scenarios of adaptation to an L2 accent (Panel A in Figure \@ref(fig:AA-plot-results-additional-cases)). Each of these scenarios is representative of attested L2-accents, selected to illustrate some of the dynamics of the different mechanisms and their dependence on exposure and test statistics. The top row of Figure \@ref(fig:AA-plot-results-additional-cases) shows an example of *contrast reduction*, in which the L2 accent shows a greater category overlap than the L1 accent. Here it is simulated by shifting /d/ category towards /t/ along VOT while keeping the latter unchanged [as qualitatively attested for, e.g., vowels in Spanish-accented English, @wade2007]. The middle row shows an example of *contrast shift*, where two categories in L2-accented speech are shifted along one or more of the cue dimensions as compared to L1-accented speech. Here it is simulated by shifting both /d/ and /t/ categories towards lower VOT [qualitatively attested for, e.g., word-initial stops in French-accented English, @Sumner2011a]. The bottom row of Figure \@ref(fig:AA-plot-results-additional-cases) shows a more extreme example, exhibiting almost complete *contrast collapse* [similar to the loss of the [s]-[`r linguisticsdown::cond_cmpl("")`] contrast that can occur in Mandarin-accented English, @zheng-samuel2020]. Here it is simulated by making /d/ identical to /t/. 

```{r compare-AA-models-best-three-functions}
# Function to construct change model of each type and select the best performing parameter combination
construct_three_models <- function(
    example_label = "Cue_reweighting",
    experimenter.ideal_observer,
    category_dist_ratio1 = dist.L2_category.cue1,
    category_dist_ratio2 = dist.L2_category.cue2,
    shift_mean_1 = shift.cue1, 
    shift_mean_2 = shift.cue2,
    test.n_block = n.test_block,
    n_test = n.test.token
  ) {
  if(SET_SEED) set.seed(42007)
  
  d.AA.exposure <<-
    make_accent_adaptation_exposure_design(
      experimenter.ideal_observer = m.io.VOT_f0.AA, 
      category_dist_ratio1 = dist.L2_category.cue1, 
      category_dist_ratio2 = dist.L2_category.cue2, 
      shift_mean_1 = shift.cue1, 
      shift_mean_2 = shift.cue2) %>%
    add_subjects_to_exposure(n.subject = n.subject)

  d.AA.test <<- 
    make_accent_adaptation_test_design(
      experimenter.ideal_observer = m.io.VOT_f0.AA, 
      test.n_block = n.test_block, 
      n_test = n.test.token, 
      category_dist_ratio1 = dist.L2_category.cue1, 
      category_dist_ratio2 = dist.L2_category.cue2, 
      shift_mean_1 = shift.cue1, 
      shift_mean_2 = shift.cue2) %>%
    add_subjects_to_test(n.subject = n.subject)
  
  p.exposure_test <- make_exposure_test_plot(d.AA.exposure, d.AA.test)
  ggsave(paste0('../figures/plotly/exposure_test_distribution_', example_label, '.png'), plot = p.exposure_test, width = base.width*3 + .5, height = base.height + .5, dpi = 300)


  history.optimization_representations <<- tibble(.rows = 0)
  best_performing_parameters.representations <- find_best_model.representations.AA()
  representations.pred <- plot_sample_model.representations.AA(
    prior_kappa = round(best_performing_parameters.representations$par[1], digits =0), # here one can select to plot just the best-performing parameterization or to visualize a range of parameters by specifying the values assigned to prior_kappa and prior_nu
    prior_nu = round(best_performing_parameters.representations$par[2], digits = 0)
  )

  history.optimization_bias <<- tibble(.rows = 0)
  best_performing_parameters.bias <- find_best_model.bias.AA()
  bias.pred <- plot_sample_model.bias.AA(
    lapse_rate = round(best_performing_parameters.bias$par[1], digits = 2), # here one can select to plot just the best-performing parameterization or to visualize a range of parameters by specifying the values assigned to lapse_rate and beta_pi
    beta_pi = round(best_performing_parameters.bias$par[2], digits = 2)
  )
  
  bias.pred %<>% get_average_bias_results_across_simulations() # average predictions across simulations for the bias model
  
  history.optimization_normalization <<- tibble(.rows = 0)
  best_performing_parameters.normalization <- find_best_model.normalization.AA()
  normalization.pred <- plot_sample_model.normalization.AA(
    prior_kappa.normalization = round(best_performing_parameters.normalization$par[1], digits = 0)
  )

  # plot the predicted results for all parameterization and save plots
  p.AA.representations <- 
    make_results_plot_representations(
      data = representations.pred, 
      data_best = best_performing_parameters.representations)
  ggsave(
    paste0('../figures/plotly/representations_', example_label, '.png'), 
    plot = p.AA.representations, 
    width = 2*(base.width*length(levels(factor(representations.pred$prior_kappa))) + .5), 
    height = 2*(base.height*length(levels(factor(representations.pred$prior_nu))) + .5), 
    dpi = 300)

  p.AA.bias <- 
    make_results_plot_decision_making(
      data = bias.pred, 
      data_best = best_performing_parameters.bias)
  ggsave(
    paste0('../figures/plotly/bias_', example_label, '.png'), 
    plot = p.AA.bias, 
    width = 2*(base.width*length(levels(factor(bias.pred$beta_pi))) + .5), 
    height = 2*(base.height*length(levels(factor(bias.pred$posterior.lapse_rate))) + .5), 
    dpi = 300)

  p.AA.normalization <- 
    make_results_plot_normalization(
      normalization.pred, 
      best_performing_parameters.normalization)
  ggsave(
    paste0('../figures/plotly/normalization_', example_label, '.png'), 
    plot = p.AA.normalization, 
    width = 2*(base.width*length(levels(factor(normalization.pred$prior_kappa.normalization))) + .5), 
    height = 2*(base.height + .5), 
    dpi = 300)

   d <- bind_rows(
    representations.pred %>%
      mutate(ModelType = "representations"),
    bias.pred %>%
      mutate(ModelType = "decision-making"),
    normalization.pred %>%
            mutate(ModelType = "normalization")) %>%
    mutate(ModelType = factor(ModelType, levels = c("representations", "decision-making", "normalization")))

  return(d)
}
```

(ref:AA-plot-results-additional-cases) Predicted adaptation for three types of L2 accents, from top to bottom: contrast reduction, contrast shift and contrast collapse. Predictions were derived for one random experiment of the same number of exposure and test stimuli as in Case Study 2. Multiple simulations were performed following the same procedure described in the main scenario above. **Panel A - Distributions of L1-accented and L2-accented categories:** L1-accented categories, represented in light shades, are kept constant across the three scenarios and identical to that in the main scenario above; L2-accented categories, represented by solid ellipses, are varied across scenarios. **Panel B - Change model predictions:** Predicted categorization accuracies for the L2-accented test tokens after L1-accented and L2-accented exposure, for the best-performing parameterizations of each change model (cf. highlighted panels in Figure \@ref(fig:AA-result-changes-in-representations)-\@ref(fig:AA-result-changes-in-normalization)). The average accuracy across all test tokens for each condition is shown above the bars. Error bars show 95% bootstrapped confidence intervals.

```{r AA-result-comparison-best-three-models-contrast-shift, warning=FALSE}
# Examine a new scenario: contrast shift
min_VOT <- -100 # allows negative VOT when generating exposure and test data
dist.L2_category.cue1 <- 1
dist.L2_category.cue2 <- 0
shift.cue1 <- -1.2
shift.cue2 <- 0
var.ratio <- 1
example_label = "Contrast_shift"
d.AA.results.contrast_shift.best  <- construct_three_models(example_label = "Contrast_shift", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-best-three-models-contrast-reduction, warning=FALSE}
# Examine a new scenario: contrast reduction
min_VOT <- 5
dist.L2_category.cue1 <- 0.5
dist.L2_category.cue2 <- 0
shift.cue1 <- 0
shift.cue2 <- 0
var.ratio <- 1
example_label = "Contrast_reduction"
d.AA.results.contrast_reduction.best  <- construct_three_models(example_label = "Contrast_reduction", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-best-three-models-contrast-collapse, warning=FALSE}
# Examine a new scenario: contrast collapse
min_VOT <- -100 # allows negative VOT when generating exposure and test data
dist.L2_category.cue1 <- 0
dist.L2_category.cue2 <- 0
shift.cue1 <- 0
shift.cue2 <- 0
var.ratio <- 1
example_label = "Contrast_collapse" #TO DO:COME BACK HERE AND FIX IT FOR get_parameters_phonetic_contrast.AA

d.AA.results.contrast_collapse.best  <- construct_three_models(example_label = "Contrast_collapse", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-best-three-models-cue-reweighting-DEBUG, warning=FALSE, eval= FALSE}
#DEBUG remove if done
# Examine a new scenario: contrast collapse
dist.L2_category.cue1 <- 1
dist.L2_category.cue2 <- 0.22
shift.cue1 <- 0.5
shift.cue2 <- 0.22
var.ratio = 1 # Multiple category variance by a constant; var.ratio = 1 means no change in category variance

min_VOT = 5 # Set the minimal values for randomly generated test token cue values to make sure it is physiologically possible
min_f0_Mel = 20
example_label = "Cue_reweighting"

d.AA.results.cue_reweighting.best  <- construct_three_models(example_label = "Cue_reweighting", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-additional-cases-history-DEBUG, warning=FALSE, eval = FALSE}
#DEBUG remove if done
cases = c("Cue_reweighting", "Contrast_shift", "Contrast_reduction", "Contrast_collapse")
history.optimization.representations.all = data.frame()
history.optimization.bias.all = data.frame()
history.optimization.normalization.all = data.frame()

for (i in 1:length(cases)){
  example_label = cases[i]
  
  #load history of optimization for representations model
  history.optimization.representations.current <- 
    readRDS(get_path(paste0("../models/d.AA.history.optimization.representations_", example_label,".rds"))) %>%
    mutate(Case = example_label) 
  history.optimization.representations.all <- 
    bind_rows(history.optimization.representations.all, history.optimization.representations.current)
  rm(history.optimization.representations.current)
  
  #load history of optimization for bias model
  history.optimization.bias.current <- 
    readRDS(get_path(paste0("../models/d.AA.history.optimization.bias_", example_label,".rds"))) %>%
    mutate(Case = example_label) 
  history.optimization.bias.all <- 
    bind_rows(history.optimization.bias.all, history.optimization.bias.current)
  rm(history.optimization.bias.current)
  
  #load history of optimization for normalization model
  history.optimization.normalization.current <- 
    readRDS(get_path(paste0("../models/d.AA.history.optimization.normalization_", example_label,".rds")))
  
  history.optimization.normalization.all <- 
    bind_rows(history.optimization.normalization.all, history.optimization.normalization.current)
  rm(history.optimization.normalization.current)
}
```

```{r AA-result-comparison-additional-cases, warning=FALSE}
cases = c("Contrast_reduction", "Contrast_shift", "Contrast_collapse", "Cue_reweighting")
best_performing_parameters.representations.par = data.frame()
best_performing_parameters.bias.par = data.frame()
best_performing_parameters.normalization.par = data.frame()

for (i in 1:length(cases)){
  example_label = cases[i]
  #load best-performing parameters for representation model
  best_performing_parameters.representations <- readRDS(get_path(paste0("../models/best_performing_parameters.representations_", example_label,".rds")))
  best_performing_parameters.representations.par.current = data.frame(prior_kappa = best_performing_parameters.representations$par[1],
           prior_nu = best_performing_parameters.representations$par[2])
  best_performing_parameters.representations.par %<>% bind_rows(., best_performing_parameters.representations.par.current %>% mutate(Case = example_label)) %>%
    mutate(ModelType = "representations")
  
  #load best-performing parameters for bias model
  best_performing_parameters.bias <- readRDS(get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))
  best_performing_parameters.bias.par.current = data.frame(lapse_rate = best_performing_parameters.bias$par[1],
           beta_pi = best_performing_parameters.bias$par[2])
  best_performing_parameters.bias.par %<>% bind_rows(., best_performing_parameters.bias.par.current %>% mutate(Case = example_label)) %>%
    mutate(ModelType = "decision-making")
  
  #load best-performing parameters for normalization model
  best_performing_parameters.normalization <- readRDS(get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))
  best_performing_parameters.normalization.par.current = data.frame(prior_kappa = best_performing_parameters.normalization$par[1])
  best_performing_parameters.normalization.par %<>% bind_rows(., best_performing_parameters.normalization.par.current %>% mutate(Case = example_label)) %>%
    mutate(ModelType = "normalization")
}

```

```{r AA-plot-target-exposure-additional-cases, message = FALSE, warning=FALSE}
# simulating a case of contrast shift: like French-accented English stops, so allowing negative VOTs
min_VOT = -100
var.ratio = 1
example_label = "Contrast_shift"
d.AA.exposure.target.contrast.shift <- 
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 1, 
    category_dist_ratio2 = 0, 
    shift_mean_1 = -1.2, 
    shift_mean_2 = 0, 
    exposure.tokens.L1.n = 5000, 
    exposure.tokens.L2.n = 5000)

# plot target exposure ellipses
min_VOT = 5
var.ratio = 1
# simulating a case of contrast reduction
example_label = "Contrast_reduction" # TO DO:COME BACK HERE TO FIX THE ASSIGNMENT OF EXAMPLE_LABEL
d.AA.exposure.target.contrast.reduction <- 
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 0.5, 
    category_dist_ratio2 = 0, 
    shift_mean_1 = 0, 
    shift_mean_2 = 0, 
    exposure.tokens.L1.n = 5000, 
    exposure.tokens.L2.n = 5000)

# simulating a case of contrast collapse
min_VOT = -100
var.ratio = 1
example_label = "Contrast_collapse"
d.AA.exposure.target.contrast.collapse <-  
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 0,
    category_dist_ratio2 = 0,
    shift_mean_1 = 0,
    shift_mean_2 = 0,
    exposure.tokens.L1.n = 5000, 
    exposure.tokens.L2.n = 5000) 

d.AA.exposure.target.cases <- rbind(
  d.AA.exposure.target.contrast.reduction %>%
    mutate(Case = "Contrast reduction"),
  d.AA.exposure.target.contrast.collapse %>%
    mutate(Case = "Contrast collapse"),
  d.AA.exposure.target.contrast.shift %>%
    mutate(Case = "Contrast shift")) %>%
  mutate(Case = factor(Case, levels = c("Contrast reduction", "Contrast shift", "Contrast collapse")))

# plot ellipses for actual exposure
p.AA.exposure <- d.AA.exposure.target.cases %>%
  mutate(Title = "Exposure distributions") %>%
  filter(Condition == "L2-accented") %>%
  mutate(Group = factor(paste0(Condition, Item.Category))) %>%
  ggplot(aes(x = VOT, y = f0_Mel, alpha = Condition)) +
  stat_ellipse(aes(color = Group), level = .95, geom = "polygon") +
  stat_ellipse(data = d.AA.exposure.target.cases %>%
                 mutate(Title = "Exposure distributions") %>%
                 filter(Condition == "L1-accented") %>%
                 mutate(Group = factor(paste0(Condition, Item.Category))), aes(fill = Group), level = .95, geom = "polygon") +
  scale_x_continuous(expression("VOT (ms)"), 
                        # breaks=seq(0, 120, by = 30),
                        expand = expansion(mult = .1, add = 0)
                        ) +
  scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0), breaks=seq(0, 500, by = 100)) +
  scale_alpha_discrete(range = c(0.2, 0), guide = "none") +
  coord_cartesian(ylim = c(0, 500)) +
  scale_fill_manual("Category",  values = c(colors.category), guide = "none") +
  scale_color_manual("Category", values = c(colors.category), guide = "none") +
  facet_grid(Case ~ Title, switch = "y") + theme(legend.position="top") 
```

```{r AA-plot-results-additional-cases, fig.width=base.width*4 + .5, fig.height=base.height*3 + .5, fig.cap = "(ref:AA-plot-results-additional-cases)", warning=FALSE}
d.AA.results.best.cases <- bind_rows(
  d.AA.results.contrast_reduction.best %>%
    mutate(Case = "Contrast reduction"),
  d.AA.results.contrast_collapse.best %>%
    mutate(Case = "Contrast collapse"),
  d.AA.results.contrast_shift.best %>%
    mutate(Case = "Contrast shift") 
) %>%
  mutate(Case = factor(Case, levels = c("Contrast reduction", "Contrast shift", "Contrast collapse"))) %>%  
  mutate(parameterization = case_when(
   ModelType == "representations" ~ paste0("kappa = ", prior_kappa, ", nu = ", prior_nu),
   ModelType == "decision-making" ~ paste0("lambda = ", posterior.lapse_rate, ", beta = ", beta_pi),
   ModelType == "normalization" ~ paste0("kappa = ", prior_kappa.normalization)))

p.results.best.cases <- d.AA.results.best.cases %>%
  mutate(
    ModelType = factor(paste0("Changes in ", ModelType), levels = c("Changes in representations", "Changes in decision-making", "Changes in normalization"))) %>%
  mutate(Label = interaction(ModelType, parameterization, sep = "\n")) %>%
  ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
  stat_summary(fun = mean,
                 geom="bar", position = pos,
                 width = 0.6) +
  stat_summary(aes(color = Item.Intended_category),
                 fun.data = mean_cl_boot,
                 geom = "uperrorbar",
                 position = pos, width = 0.2) +
  coord_cartesian(ylim =  c(0,1)) +
  scale_color_manual("Category", values = colors.category) +
  scale_fill_manual("Category", values = colors.category) +
  scale_alpha_discrete(range = c(0.2, 1)) +
  xlab("Exposure condition") +
  ylab("Predicted \ncategorization accuracy") +
  geom_text(inherit.aes = FALSE, data = . %>%
                 group_by(Case, Condition, ModelType) %>%
                 summarise(mAcc = round(mean(response), digits = 2)),
            aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
  facet_grid(Case ~ ModelType) +
  theme(legend.position = "top", 
        strip.text.y = element_blank(),
        panel.grid.major.x = element_blank())

prow <- 
  plot_grid(
    p.AA.exposure, 
    p.results.best.cases + theme(legend.position="none"), 
    labels = c('A)', 'B)'), 
    ncol = 2, 
    rel_widths = c(1.2,3.3))

# extract a legend that is laid out horizontally
legend_prow <- get_legend(p.results.best.cases)
plot_grid(legend_prow, prow, ncol = 1, rel_heights = c(.2, 1)) 
```

Figure \@ref(fig:AA-plot-results-additional-cases) also shows the model predictions using the best-performing parameterizations (listed in Table \@ref(tab:AA-table-best-performing-parameterization)) that yield the maximal benefit for the L2-accented exposure condition for each change model, following the same procedures described in the main scenario. The three accent scenarios differ in terms of levels of difficulties they are predicted to pose for L1 listeners *prior to* exposure (see L1-accented exposure condition in Panel B of Figure \@ref(fig:AA-plot-results-additional-cases)). <!--TO DO: consider add a footnote here to indicate that the L1-accented condition therefore appears to be different from the other two conditions at the best-performing parameterization, because the lapse rate is not zero. -->Contrast reduction (top row) and contrast collapse (bottom row), for example, resemble the scenario considered in Case Study 2: without informative exposure, L1 listeners unfamiliar with the accent are expected to struggle with the recognition of /d/ than the recognition of /t/. The opposite is observed for our example of contrast shift (middle row). This illustrates how differences in the realization of L2 vs. L1 accents can give rise to starkly different perceptual consequences for L1 listeners. 

The three scenarios also differ in the maximal benefits that L1 listeners can gain from L2 accent exposure. Three aspects are worth noting. First, corroborating the point we raised above, changes of representations do not always outperform the other two mechanisms. For the case of contrast reduction (top row), changes in decision-making and normalization perform as well as, or better than, changes in category representations. This underscores our main conclusion from Case Studies 1 and 2 that the computational complexity afforded for the representational change model does not always result in higher recognition accuracy than the other two mechanisms.

Second, the same change model can support more or less overall accuracy improvements (as gleaned from comparing the panels within each column), depending on the specific differences between the L2 and L1 accents. Consider the case of contrast shift (middle row). While changes in representations and normalization both predict large improvements in overall recognition accuracy in the L2-accented exposure, changes in decision-making do not (i.e., the difference in the predicted accuracy across L1- and L2-accented exposure conditions is negligible). This outcome aligns with the zero-sum trade-offs often attributed to this change mechanism (i.e., The recognition of /d/ improves at the cost of the recognition of /t/). The heterogeneity of the results across the three scenarios makes it clear that the trade-off is not an inherent property of the decision-making mechanism. It is instead a possible outcome arising from a combination of the specific change model and properties of the L2 accent (or L1-L2 accent differences). 

Third, _none_ of the change models predicts significant benefits from L2-accented exposure for contrast collapse (bottom row). This highlights an important, yet often under-appreciated, insight: sometimes null results are _predicted_ for exposure-test experiment, because of the specific stimulus statistics [see @tan2021; @zheng-samuel2020]. Put differently, an absence of improvements does not _necessarily_ constitute evidence against accent adaptation or any of the three mechanisms.^[This also means that individual differences in accent adaptation around such a null effect are likely _not_ indicative of the individuals ability to adapt to L2-accented speech but instead are likely to reflect random noise [under-mining the central argument made in, e.g., @zheng-samuel2020].] 

While the scenarios considered here cover only a small subset of the cross-accent differences that exist in the world, they clearly illustrate a general challenge: different types of cross-talker differences (including L2 accents) pose heterogeneous challenges to listeners. It follows that, depending on the exact nature and sources of the difficulties, listeners would benefit from different _rates_ and _types_ of adaptation. Since the properties of unfamiliar accents are not _a priori_ known to listeners, this means that the optimal level of flexibility is achieved if the neural and cognitive systems underlying adaptive speech perception may adjust based on the properties of the input <!-- [ck] I don't think "adapt based on" is clear enough. But I am actually not sure what this sentence is trying to say...Can either of you rephrase this?--> [see discussion of the trade-off between flexibility and stability in @kleinschmidt-jaeger2015, pp. 180-182]. In Figure \@ref(fig:AA-plot-results-additional-cases) (Panel B), this is reflected in the fact that the best-performing parameterizations vary across the different accent scenarios (see Table \@ref(tab:AA-table-best-performing-parameterization)). 

In summary, benefits of L2-accented exposure expected under each change mechanism hinge on the specifics of L2-accented categories. Beyond the accent scenarios examined here (cue-reweighting, contrast reduction, contrast shift, and contrast collapse), naturally produced L2 accents exhibit substantial variations in complex ways, which can pose difficulties to L1-listeners and researchers alike. At the same time, it is also the case that they offer a unique and fertile ground to investigate the relative engagement of each change mechanism. Because the three change mechanisms would predict distinct perceptual outcomes depending on the properties of the exposure and test stimuli, these differences can be used to distinguish between model predictions. In the general discussion below, we explore how such tests are possible.

<!--TO DO: tidy up the table and add caption-->
```{r AA-table-best-performing-parameterization}
parameters.results.best.cases <- bind_rows(best_performing_parameters.representations.par,                      best_performing_parameters.bias.par, best_performing_parameters.normalization.par) %>%
  mutate(Case = gsub("_", " ", Case)) %>%
  mutate(parameterization = case_when(
   ModelType == "representations" ~ paste0("$\\kappa_{c,0}$ = ", round(prior_kappa, digits = 0), ", $\\nu_{c,0}$ = ", round(prior_nu, digits = 0)),
   ModelType == "decision-making" ~ paste0("$\\lambda$ = ", round(lapse_rate, digits = 2),   ", $\\beta_\\pi$ = ", round(beta_pi, digits = 2)),
   ModelType == "normalization" ~ paste0("$\\kappa_0$ = ", round(prior_kappa, digits = 0)))) %>%
   mutate(
      ModelType = factor(paste0("Changes in ", ModelType), levels = c("Changes in representations", "Changes in decision-making", "Changes in normalization"))) %>%
    select(Case, ModelType, parameterization) %>%
    distinct() %>%
    pivot_wider(names_from = ModelType, values_from = parameterization)


colnames(parameters.results.best.cases) = c("L2 accent scenario", "Changes in representations", "Changes in decision-making", "Changes in normalization")

knitr::kable(parameters.results.best.cases, caption = "Best-performing parameters across different accent scenarios", align = 'l', booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = "scale_down")
```

