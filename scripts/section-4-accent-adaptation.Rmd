# Case Study 2: accent adaptation {#sec:AA}
The second paradigm we consider focuses on *naturally* accented speech---e.g., dialectal [@smith2014], varietal [@shaw2018], or second language (L2) accents [@bradlow-bent2008; @eisner2013; @sidaras2009; @weil2001a]. Typically, exposure to the unfamiliar accent is compared to a control condition in which listeners are exposed to a familiar accent, most often the 'standard' variety of listeners' L1. Following exposure, listeners in either group are tested on the unfamiliar accent. In an influential study, @bradlow-bent2008 had listeners transcribe a total of 160 sentences of either Mandarin-accented English or L1-accented English, distributed over two sessions on two separate days. In a subsequent test phase, both groups transcribed Mandarin-accented sentences. Participants who were first exposed to Mandarin-accented English were significantly more accurate during test (over 90% accuracy compared to about 80%). This finding has since been replicated and extended [for review, see @baeseberk2020]. We now know that substantially shorter exposure can lead to similarly large improvements in accuracy [e.g., 80 sentences in a single session, about 2-5 minutes of speech, @xie2021jep], that these can persist over hours and days [@witteman2015; @xie2018lcn], and that accent adaptation can sometimes generalize across talkers of the same or similar accents [e.g., @baeseberk2013; @tzeng2016; @xie2021jep]. <!-- Facilitatory effects of exposure have also been demonstrated in tasks that tap into online processing of naturally accented speech, including cross-modal matching [@clarke-garrett2004; @xie2018jasa], phonological priming [@eisner2013; @xie2017], and visual world eye-tracking [@dahan2008; @trude2012talker].-->

Critically, naturally accented speech typically differs from listeners' expectations in ways that can be considerably more complex than the types of manipulation studied in perceptual recalibration experiments. It is thus not clear whether the same mechanisms that underlie perceptual recalibration also underlie adaptation to natural accents. This question continues to attract attention [see recent reviews, @baeseberk2018; @bent-baeseberk2021; @zheng-samuel2020], because its answer determines whether perceptual recalibration and related paradigms---which afford increased control over stimuli characteristics but come at the risk of decreased ecological validity---can shed light on how listeners overcome the challenge of cross-talker variability in everyday speech.

As in the case of perceptual recalibration, accent adaptation is often attributed to changes in category representations [e.g., @bent-baeseberk2021; @sidaras2009; @eisner2013; @sumner2009; @tzeng2016; @xie2016jep]. This might in part due to the (false) assumption that changes in response biases can only explain trade-offs in accuracy: as the accuracy for one category improves, it has to inevitably decrease for the other category. Under this assumption, overall improvements in recognition accuracy could not possibly be explained by changes in decision-making. Similarly, pre-linguistic normalization has rarely been considered as a possible mechanism for accent adaptation---perhaps because the complexities of L2-accents make it counter-intuitive that simple centering would be sufficient to explain adaptation to L2-accented speech. However, to the best of our knowledge, previous work has never actually put these intuitions to the test. Case Study 2 thus assesses whether the signature results of experiments on accent adaptation necessarily distinguish between the three change mechanisms. 

Compared to perceptual recalibration paradigms, experiments on accent adaptation exhibit more heterogeneity in their designs and tasks. Case Study 2 focuses on experiments in which treatment exposure employs a single talker with the unfamiliar accent, and the test phase heard by both groups of participants employs previously unheard stimuli from the same accented talker [e.g., @eisner2013; @schertz2015; @xie2017].^[The approach we take here can also be applied to cross-talker generalization---i.e., paradigms in which speech from different talkers of the same (or different) accent as the exposure talker is used during test [@baeseberk2013; @bradlow-bent2008; @sidaras2009].] For example, in an exposure-test experiment, @xie2016jep exposed two groups of L1-English listeners to Mandarin-accented speech. In exposure, participants in the treatment group heard 30 words ending in /d/ (e.g., _overload_) together with 60 word fillers and 90 nonword fillers. Participants in the control group did not hear any words that contain /d/. After exposure, participants in both conditions categorized recordings of 60 minimal /d/-/t/ pairs (e.g., _seed_ vs. _seat_) spoken by the same Mandarin-accented talker. Participants who heard /d/-final words during exposure were more likely to categorize /d/-final words correctly during test, compared to the control condition. At the same time, a non-significant numerical *de*crease in accuracy was observed for /t/-final words [see also @xie2018lcn]. Related paradigms have yielded similar results for other contrasts and other L1-L2 pairs [e.g., @zheng-samuel2020; @eisner2013].

For Case Study 2, we construct a hypothetical experiment that closely follows this type of design. For the sake of continuity, we focus on the same syllable-initial /d/-/t/ contrast used in Case Study 1. Like in Case Study 1, we analyze the data in Case Study 2 following the conventions of the field. For experiments on accent adaptation, this typically means that the facilitatory effects of L2-accented exposure are assessed through improvements in categorization accuracy during test.

The simulation process requires us to be specific about the properties of the L2 accent and how it deviates from the L1 accent, expressed via a set of exposure and test stimuli, at the level of acoustic-phonetic distributions. These properties, however, are rarely ever reported in the literature, and their predicted consequences for perception are even less commonly modeled (for an exception, see Tan et al., 2021). We therefor focus on whether any of the three change mechanisms can *qualitatively* explain the signature results found in @xie2016jep and other similar work. In the discussion, we further reflect on how the three change mechanisms can be distinguished.

## Data
The stimulus generation procedure is described in detail in the SI (\@ref(sec:SI-AA)).

```{r study-AA-setup, message=FALSE}
# Set plotting aesthetics
geom_text.size <- theme_get()$text$size *5/14 # change from mm to point scale; keeping the size of geom_text the same as the rest of text in ggplot

# Random seed for this study
set.seed(123498765)

# Which categories is this experiment about?
categories.AA <- c("/d/", "/t/")
conditions.AA <- c("L1-accented", "L2-accented")
colors.category <- colors.condition <- colors.voicing
shapes.category <- shapes.condition <- c(15, 17)
linetypes.condition <- c(1,2)

# Number of subjects
n.subject <- 1

# Number of exposure tokens for each category, shifted and typical
n.exposure.token <- 30

# How separable the categories are in the L2 accent?
# e.g., dist.L2_category.cue1  -- The ratio of category mean difference along the primary cue in L2 to that in L1 (the primary cue does not have to be the same cue dimension across accents); 1 means the distance is equal to that in L1 accent.

# Case 1: presented in the main text, with clear cue-weighting (e.g., word-initial stops in Korean-accented English).
dist.L2_category.cue1 <- 0.5
dist.L2_category.cue2 <- 0.7
shift.cue1 <- 0
shift.cue2 <- 0

# Case 2: no cue-weighting, but greater category overlap due to reduced distance (e.g., s-th in Mandarin-accented English)
# dist.L2_category.cue1 <- 0.5
# dist.L2_category.cue2 <- 0
# shift.cue1 <- 0
# shift.cue2 <- 0

# Case 3: no cue-weighting, no change in category overlap, but a general shift along the primary cue dimension (e.g., word-initial stops in Spanish-accented English)
# dist.L2_category.cue1 <- 1
# dist.L2_category.cue2 <- 0
# shift.cue1 <- 0.2
# shift.cue2 <- 0

# arbitrarily assume that the variance of the speech stimuli in the experiment is $\frac{1}{`r my_experimenter.variability_reduction`}$th of that observed in natural productions (applying to L1-accented speech only)
my_experimenter.variability_reduction <- 1

# Number of test blocks
n.test_block <- 1
# Number of test tokens per category
n.test.token <- 60

# Set the minimal values for randomly generated test token cue values to make sure it is physiologically possible
min_VOT = 5
min_f0_Mel = 20

m.io.VOT_f0.AA <-
  m.VOT_f0_MVG  %>%
  filter(category %in% categories.AA) %>%
  droplevels() %>%
  make_stop_VOTf0_ideal_observer() %>%
  arrange(category)

m.ia.VOT_f0.AA <-
  crossing(
    prior_kappa = 4^(1:5),
    prior_nu = 4^(1:5)) %>%
  rowwise() %>%
  mutate(
    ideal_adaptor = map2(prior_kappa, prior_nu, ~ make_stop_VOTf0_ideal_adaptor(m = m.io.VOT_f0.AA, kappa = .x, nu = .y))) %>%
  unnest(ideal_adaptor) %>%
  arrange(category)
```

### Exposure phase
Figure \@ref(fig:study-AA-exposure-test-plot)A shows the stimuli for the exposure and test phases of the experiment. In the L2-accented exposure condition, listeners hear 30 word recordings containing L2-accented initial /d/ and 30 word recordings containing L1-accented initial /t/. In the control condition with L1-accented exposure, listeners hear the same words but from an L1-accented talker.^[The use of L1-accented exposure, rather than L2-accented exposure without /d/, as control follows a typical design in accent adaptation studies [e.g., @bradlow-bent2008], rather than @xie2016jep. Xie and colleagues instead employed L2-accented exposure without /d/, using the same L2-accented talker as during test [see also @eisner2013]. For the present purpose, both types of control conditions lead to identical predictions (as long as the L2-accented control exposure successfully avoids conveying non-negligible amount of information about the categories considered during the test phase) since the current implementation of the three change models do not consider talker-switch costs.] For L1-accented exposure, the category likelihoods were set to match those observed in @chodroff-wilson2018, after C-CuRE normalization (i.e., the distributions shown in Figure \@ref(fig:demonstrate-normalization)B). This follows the same approach we took for the typical tokens in Case Study 1. For L2-accented exposure, the /t/ was given the exact same distribution as the L1-accented /t/, whereas the L2-accented /d/ differed from the L1-accented /d/. We simulated the /d/ distribution such that the primary cue for L1 listeners (VOT) becomes the secondary cue in the L2-accented speech while preserving the relative ordering between categories (i.e., shorter VOT and lower f0 signaling a /d/ than a /t/ category). This simulates a common type of situation that occurs in L2 accents. For example, Korean-accented English relies more on f0 rather than VOT to signal word-initial /d/-/t/ contrasts, compared to L1-accented English [e.g., @schertz2015].

```{r study-AA-exposure-functions}
# Get acoustic locations that correspond to targeted response proportions.
get_parameters_phonetic_contrast.AA <- function(ideal.observer,
                                                category_dist_ratio1,
                                                category_dist_ratio2,
                                                shift_mean_1 = 0,
                                                shift_mean_2 = 0) {

   # native categories
   mu_native_A <- ideal.observer %>% filter(category == "/d/") %>% pull(mu) %>% .[[1]]
   Sigma_native_A <- ideal.observer %>% filter(category == "/d/") %>% pull(Sigma) %>% .[[1]]
   mu_native_B <- ideal.observer %>% filter(category == "/t/") %>% pull(mu) %>% .[[1]]
   Sigma_native_B <- ideal.observer %>% filter(category == "/t/") %>% pull(Sigma) %>% .[[1]]

   # nonnative categories
   mu_nonnative_B <- mu_native_B
   Sigma_nonnative_B <- Sigma_native_B

   # The larger the category_dist_ratio, the greater distance along that cue dimension (1 = VOT, 2 = f0) between the two categories in the L2 accent.
   # If category_dist_ratio1 = category_dist_ratio2 = 0, then the two categories overlap entirely in the L2 accent.
   # If category_dist_ratio1 = category_dist_ratio2 = 1, then /d/-/t/ will have the same distance as that in L1-accent along the primary cue dimension.
   mu_nonnative_A <- c(mu_nonnative_B[1] * (1 + category_dist_ratio1 * (mu_native_A[1]- mu_native_B[1])/mu_native_B[1]),
                       mu_nonnative_B[2] * (1 + category_dist_ratio2 * (mu_native_A[1]- mu_native_B[1])/mu_native_B[1]))

   Omega_native_A <- cov2cor(Sigma_native_A)
   Omega_nonnative_A <- matrix(c(Omega_native_A[1], Omega_native_A[2], Omega_native_A[3], Omega_native_A[4]), nrow=2) # change omega if orientation of category dispersion changes
   Sigma_nonnative_A <- cor2cov(Omega_nonnative_A, sqrt(diag(Sigma_native_A)))
    
   # Allow a shift of nonnative category means
    shift1 <- (mu_nonnative_A[1]+ mu_nonnative_B[1])/2*shift_mean_1
    shift2 <- (mu_nonnative_A[2]+ mu_nonnative_B[2])/2*shift_mean_2
    mu_nonnative_A <- c(mu_nonnative_A[1]+ shift1,
                        mu_nonnative_A[2]+ shift2)
    mu_nonnative_B <- c(mu_nonnative_B[1]+ shift1,
                        mu_nonnative_B[2]+ shift2)
    
    tau_native_A <-  sqrt(diag(Sigma_native_A))
    Omega_native_A <- cov2cor(Sigma_native_A)
    tau_native_B <-  sqrt(diag(Sigma_native_B))
    Omega_native_B <- cov2cor(Sigma_native_B)
    tau_nonnative_A <-  sqrt(diag(Sigma_nonnative_A))
    Omega_nonnative_A <- cov2cor(Sigma_nonnative_A)
    tau_nonnative_B <-  sqrt(diag(Sigma_nonnative_B))
    Omega_nonnative_B <- cov2cor(Sigma_nonnative_B)

    # plot and check whether the two categories are simulated as intended
    d.model <- tibble(
      speech = c(rep("native", 2), rep("nonnative", 2)),
      category = rep(c("A", "B"), 2),
      mu = list(mu_native_A, mu_native_B, mu_nonnative_A, mu_nonnative_B),
      tau = list(tau_native_A, tau_native_B, tau_nonnative_A, tau_nonnative_B),
      Omega = list(Omega_native_A, Omega_native_B, Omega_nonnative_A, Omega_nonnative_B),
      Sigma = list(Sigma_native_A, Sigma_native_B, Sigma_nonnative_A, Sigma_nonnative_B)) %>%
      group_by(speech, category) %>%
      mutate(Model = list(list(
        mu = Reduce("+", mu) / length(mu),
        Sigma = Reduce("+", Sigma) / length(Sigma))))

    return(d.model)
}

# make the design of an exposure phase
make_accent_adaptation_exposure_design = function(
  # Ideal observer describing the true perceptual system for two categories.
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,

  # How separate are the two categories in L2 accent?
  category_dist_ratio1 = dist.L2_category.cue1,
  category_dist_ratio2 = dist.L2_category.cue2,
  shift_mean_1 = 0,
  shift_mean_2 = 0,
  exposure.tokens.L1.n = n.exposure.token,
  exposure.tokens.L2.n = n.exposure.token,
  quiet = F
) {
  if (!quiet) message(
    paste0("Making exposure design with ", exposure.tokens.L1.n, " L1-accented tokens and ", exposure.tokens.L2.n, " L2-accented tokens."))



  d.model.both <- get_parameters_phonetic_contrast.AA(experimenter.ideal_observer,
                                                      category_dist_ratio1,
                                                      category_dist_ratio2,
                                                      shift_mean_1,
                                                      shift_mean_2)
  d.model.native <- d.model.both %>%
    filter(speech == "native") %>%
    droplevels()
  d.model.nonnative <- d.model.both %>%
    filter(speech == "nonnative") %>%
    droplevels()

  # Create basic tibble
  tibble(
    Phase = "exposure",
    ItemID = as.character(1:(exposure.tokens.L1.n + exposure.tokens.L2.n)),
    Item.Type = factor(c(rep("L1-accented", exposure.tokens.L1.n), rep("L2-accented", exposure.tokens.L2.n)))
  ) %>%
    # Add all unique design combinations
    crossing(
      Item.Category = factor(experimenter.ideal_observer$category)) %>%
    # Get cue value of item: if L2-accented then use L2 category parameters. If not sample from L1-accented category distribution
    mutate(
      Condition = Item.Type,
      x = map2(
        Item.Type,
        Item.Category,
        ~ case_when(
          .x == "L2-accented" & .y == experimenter.ideal_observer$category[[1]] ~ rmvnorm(1, d.model.nonnative$mu[[1]], d.model.nonnative$Sigma[[1]] / experimenter.variability_reduction),
          .x == "L2-accented" & .y == experimenter.ideal_observer$category[[2]] ~ rmvnorm(1, d.model.nonnative$mu[[2]], d.model.nonnative$Sigma[[2]] / experimenter.variability_reduction),
          .x == "L1-accented" & .y == experimenter.ideal_observer$category[[1]] ~ rmvnorm(1, d.model.native$mu[[1]], d.model.native$Sigma[[1]] / experimenter.variability_reduction),
          .x == "L1-accented" & .y == experimenter.ideal_observer$category[[2]] ~ rmvnorm(1, d.model.native$mu[[2]], d.model.native$Sigma[[2]] / experimenter.variability_reduction),
          T ~ NA_real_))) %>%
# mutate(VOT = map(x, ~ .x[1]) %>% unlist(), f0_Mel = map(x, ~ .x[2]) %>% unlist()) %>%
    mutate(VOT = unlist(map(x, ~max(min_VOT, .x[1]))), # cue 1
        f0_Mel = unlist(map(x, ~max(min_f0_Mel, .x[2]))), # cue 2 %>%  
        x = map2(VOT, f0_Mel, ~ c(.x, .y)))
}

#<!--remove this function if already existed in main file; leaving it here now so that this section can be knitted independently--!>
add_subjects_to_exposure <- function(
  d,
  n.subject,
  quiet = F
) {
  assert_that(!("Subject" %in% names(d)),
              msg = "This data frame already seems to contain subjects")
  if (!quiet) message(paste("Adding", n.subject, "subjects per exposure condition."))

  d %>%
    group_by(Condition) %>%
    crossing(Subject = factor(1:n.subject)) %>%
    mutate(Subject = paste(Condition, Subject, sep = ".")) %>%
    select(Condition, Subject, Phase, ItemID, Item.Category, Item.Type, x, everything()) %>%
    ungroup()
}
```

```{r study-AA-exposure-make-data, message=FALSE}
d.AA.exposure <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)

# calculate sample variance along each cue dimension for each exposure condition
sample_var <- d.AA.exposure %>%
  group_by(Condition, Item.Category) %>%
  summarise(s_var.f0 = var(f0_Mel),
    s_var.vot = var(VOT))

# get prior variance along each cue dimension for each exposure condition
population_var <- data.frame(
  c("/d/", "/d/", "/t/", "/t/"),
  c("VOT", "f0", "VOT", "f0"),
  c(m.VOT_f0_MVG$Sigma[2][[1]][1], m.VOT_f0_MVG$Sigma[2][[1]][4],
    m.VOT_f0_MVG$Sigma[6][[1]][1], m.VOT_f0_MVG$Sigma[6][[1]][4])
)
colnames(population_var) = c("category", "cue", "prior_variance")
```

### Test phase
During test, listeners from both exposure conditions hear the same recordings from the same L2-accented talker used in the L2-accented exposure condition. 60 test tokens are randomly sampled from the distribution of each L2-accented category (Figure \@ref(fig:study-AA-exposure-test-plot)B).

```{r study-AA-test-functions}
# make the design of an test phase
make_accent_adaptation_test_design = function(
  d,
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,
  category_dist_ratio1 = dist.L2_category.cue1,
  category_dist_ratio2 = dist.L2_category.cue2,
  shift_mean_1 = 0,
  shift_mean_2 = 0,
  test.n_block = n.test_block,
  n_test = n.test.token     # Number of test blocks (total)
) {

  d.model.both <- get_parameters_phonetic_contrast.AA(experimenter.ideal_observer,
                                                      category_dist_ratio1,
                                                      category_dist_ratio2 ,
                                                      shift_mean_1,
                                                      shift_mean_2)

  ## --------------------------------------------------------------------------------
  # create a *natural* non-native test set
  ## --------------------------------------------------------------------------------

  x <- rbind(
    rmvnorm(
      n_test,
      d.model.both %>% filter(speech == "nonnative", category == "A") %>% pull(mu) %>% .[[1]],
      d.model.both %>% filter(speech == "nonnative", category == "A") %>% pull(Sigma) %>% .[[1]]/ experimenter.variability_reduction),
    rmvnorm(
      n_test,
      d.model.both %>% filter(speech == "nonnative", category == "B") %>% pull(mu) %>% .[[1]],
      d.model.both %>% filter(speech == "nonnative", category == "B") %>% pull(Sigma) %>% .[[1]]/ experimenter.variability_reduction)
    )

    d.test <-
      tibble(
        Item.Intended_category = sort(rep(c("A", "B"), n_test)),
        # VOT = x[,1], # cue 1
        # f0_Mel = x[,2], # cue 2
        VOT = unlist(map(x[,1], ~max(min_VOT, .x))), # cue 1
        f0_Mel = unlist(map(x[,2], ~max(min_f0_Mel, .x))), # cue 2
        x = map2(VOT, f0_Mel, ~ c(.x, .y))) %>%
      mutate(
        Item.Intended_category = ifelse(Item.Intended_category == "A", "/d/", "/t/"),
        Phase = "test",
        ItemID = row_number(),
        Item.Type = "test",
        Item.Category = NA,
      ) %>%
      crossing(  
        Condition = factor(paste0(paste0(.$Item.Intended_category, "_"), d$Condition)),
        Block = 1:1) %>%
      filter((Item.Intended_category == "/d/" & grepl("/d/", Condition)) | (Item.Intended_category == "/t/" & grepl("/t/", Condition))) %>%
      droplevels()
}

add_subjects_to_test <- function(
  d,
  n.subject,
  quiet = F
) {
  assert_that(!("Subject" %in% names(d)),
              msg = "This data frame already seems to contain subjects")
  if (!quiet) message(paste("Adding", n.subject, "subjects per exposure condition."))

  d %>%
    group_by(Condition) %>%
    crossing(Subject = factor(1:n.subject)) %>%
    mutate(Subject = paste(Condition, Subject, sep = ".")) %>%
    select(Condition, Subject, Phase, ItemID, Item.Category,Item.Intended_category, Item.Type, x, everything()) %>%
    ungroup()
}

```

```{r study-AA-test-make-data}
d.AA.test <- make_accent_adaptation_test_design(d.AA.exposure, experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)
```

(ref:study-AA-exposure-test-plot) **Panel A - Exposure:** Distribution of the stimuli used during the exposure phase of the accent adaptation experiment (`r n.exposure.token` tokens each of `r conditions.AA[1]` and `r conditions.AA[2]` `r paste(categories.AA, collapse = " and ")`, respectively). **Panel B - Test:**  Distribution of the stimuli used during the test phase of the accent adaptation experiment. The test tokens come from L2-accented speech (`r n.test.token` tokens per category) and are identical for the two exposure conditions. Ellipses show the 95% probability mass for the two categories in L2-accented exposure speech.

```{r plot-AA-exposure-test-functions}
# Function to plot exposure and test data
Make_exposure_test_plot <- function(exposure.data, test.data){
  p.AA.exposure <- exposure.data %>%
    mutate(Condition = paste0("Exposure: ", Condition)) %>%
    ggplot(aes(x = VOT, y = f0_Mel, color = Item.Category)) +
    geom_point(alpha = 0.5) +
    geom_rug(data = . %>%
                 group_by(Condition, Item.Category) %>%
                 summarise(VOT = mean(VOT), f0_Mel = mean(f0_Mel)), show.legend = FALSE) +
    scale_x_continuous(expression("VOT (ms)"), 
                        expand = expansion(mult = .1, add = 0)
                        ) +
    scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0), breaks=seq(0, 500, by = 100)) +
     coord_cartesian(
    ylim = c(0, 500)) +
    scale_color_manual("Category", breaks = categories.AA, values = colors.category) +
    scale_fill_manual("Category", breaks = categories.AA, values = colors.category) +
    facet_grid(~ Condition)

  limits <- get_plot_limits(p.AA.exposure)
  breaks <- get_plot_breaks(p.AA.exposure)

  p.AA.test <- test.data %>%
    mutate(Speech = ifelse(grepl("L1", Condition), "L1-accented", "L2-accented")) %>%
    filter(Speech == "L2-accented") %>%
    mutate(Speech = paste0("Test: ", Speech)) %>%
    ggplot(aes(x = VOT, y = f0_Mel, label = as.numeric(factor(ItemID)))) +
    geom_point(aes(color = Item.Intended_category), alpha = 0.5) +
    stat_ellipse(data = exposure.data %>% filter(Condition == "L2-accented"), aes(fill = Item.Category), level = .95, alpha = 0.3, geom = "polygon") +
    scale_x_continuous(expression("VOT (ms)"),
                       expand = expansion(mult = .1, add = 0),
                       breaks = c(breaks$xbreaks),
                       limits = c(limits$xmin, limits$xmax)) +
    scale_y_continuous(expression("f0 (Mel)"),
                       breaks = c(breaks$ybreaks),
                       expand = expansion(mult = .1, add = 0),
                       limits = c(limits$ymin, limits$ymax)
                       ) +
    scale_color_manual("Category", values = colors.category) +
    scale_fill_manual("Category", values = colors.category, guide = "none") +
    coord_cartesian(xlim = c(limits$xmin, limits$xmax), ylim = c(limits$ymin, limits$ymax), expand = FALSE) +
    facet_grid(~ Speech)

  prow = plot_grid(p.AA.exposure + theme(legend.position="none"),
                   p.AA.test + theme(legend.position="none"),
                   labels = c('A)', 'B)'), nrow = 1, rel_widths = c(2,1.1))

  # extract a legend that is laid out horizontally
  legend_prow <- get_legend(
    p.AA.exposure +
      guides(color = guide_legend(title.position = "left", nrow = 1))
  )

  p.output <-  plot_grid(legend_prow, prow, ncol = 1, rel_heights = c(.1, 1))
  return(p.output)
}

appender <- function(string, prefix = "Changes in\n") paste0(prefix, string)

```

```{r study-AA-exposure-test-plot, fig.width=base.width*3 + .5, fig.height=base.height + .5, fig.cap="(ref:study-AA-exposure-test-plot)"}
Make_exposure_test_plot(exposure.data = d.AA.exposure, test.data = d.AA.test)
```

```{r add-subjects-AA-exposure-test-data}
d.AA.exposure %<>%
  add_subjects_to_exposure(n.subject = n.subject)
d.AA.test %<>%
  add_subjects_to_test(n.subject = n.subject)
```


## Results
Paralleling Case Study 1, we ask which of the three change models can account for the signature results of accent adaptation experiments. Specifically, we assess for each change model whether it can explain the two types of signature results observed in, for example, Xie et al. [-@xie2016jep]: (1) for L1-accented exposure, /d/ test tokens are recognized with lower accuracy than /t/ test tokens, since the difference between the two accents is more pronounced for /d/ than for /t/, creating a larger deviation from L1 listeners' expectations; and (2) L2-accented exposure should lead to a significant increase in recognition accuracy for /d/ test tokens without equivalent decreases in the accuracy for /t/ test tokens, resulting in increased overall accuracy. 

Below we focus on whether each change model can *qualitatively* account for these signature results, rather than the specific magnitude of accuracy improvements. We do so, because the predicted improvements do not only depend on each change model's parameters, but also the statistics of the L2-accented exposure and test tokens. For a different set of (real or simulated) stimuli, the predicted magnitude of accuracy improvements after L2-accented exposure can thus differ. This is an important consideration to keep in mind, and we return to it in the discussion. 

As in Case Study 1, we first model changes in representations, and then compare them against change models in decision-making and normalization. In  all cases, we employ the same parameterizations as in Case Study 1.

### Changes in representations

```{r study-AA-models-changes-in-representations}
d.AA.representations <- d.AA.exposure %>%
  add_prior_and_get_posterior_beliefs_based_on_exposure(prior = m.ia.VOT_f0.AA) %>%
  add_test_tokens(d.AA.test) %>%
  add_categorization_functions() %>%
  group_by(Condition, Subject, prior_kappa, prior_nu) %>%
  add_categorization() %>%
  ungroup() %>%
  mutate_at(
    vars(starts_with("prior_")),
    ~ factor(as.character(.x), levels = as.character(rev(sort(unique(.x)))))) %>%
  distinct() %>%
  left_join(
    d.AA.test %>%
      select(x, Item.Intended_category), by = "x") %>%
  distinct()
```

(ref:AA-result-changes-in-representations) Predictions of a learning model that derives accent adaptation as changes in category representations. Predicted categorization accuracies are shown for the L2-accented test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, as a function of the strength of the prior beliefs in category means ($\kappa_{0,c}$) and covariances ($\nu_{0,c}$). The average accuracy across /d/ and /t/ is shown above the bars for each exposure condition. Error bars show 95% bootstrapped confidence intervals. The highlighted panel corresponds to the combination of $\kappa_{0,c}$ and $\nu_{0,c}$ resulting in the best overall accuracy.

```{r AA-result-changes-in-representations, fig.width= base.width * 5, fig.height= base.height * 5 +.5, fig.cap="(ref:AA-result-changes-in-representations)", warning=FALSE}
pos <- position_dodge(.6)
p.AA.results.representations <-
  d.AA.representations %>%
  filter(category == Item.Intended_category) %>% # show the response for intended category
  ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
  stat_summary(fun = mean,
               geom = "bar", position = pos, width = 0.6) +
  stat_summary(aes(color = Item.Intended_category),
               fun.data = mean_cl_boot,
               geom = "uperrorbar",
               position = pos, width = 0.2) +
  coord_cartesian(ylim =  c(0,1)) +
  scale_color_manual("Category", values = colors.category) +
  scale_fill_manual("Category", values = colors.category) +
  scale_alpha_discrete(range = c(0.2, 1), guide = "none") +
  scale_x_discrete(labels= c("L1-\naccented", "L2-\naccented")) +
  xlab("Exposure condition") +
  ylab("Predicted categorization accuracy") +
  geom_text(inherit.aes = FALSE, data = . %>%
               group_by(Condition,prior_nu,prior_kappa) %>%
              summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
  facet_grid(
    prior_nu ~ prior_kappa,
    labeller = label_bquote(
      cols = {kappa[0~","~ .(categories.AA[1])] == kappa[0~","~ .(categories.AA[2])]} == .(as.character(prior_kappa)),
      rows = {nu[0~","~ .(categories.AA[1])] == nu[0~","~ .(categories.AA[2])]} == .(as.character(prior_nu)))) + # as.table = T doesn't seem to work
  myGplot.defaults(base_size = 14, set_theme = F) +
  theme(legend.position = "top", panel.grid.major.x = element_blank())


p.AA.results.representations +
  ggnewscale::new_scale_fill() +
  insert_layers(
    geom_rect(data = d.AA.representations %>%
                filter(category == Item.Intended_category) %>%
                distinct(prior_kappa, prior_nu) %>%
                mutate(highlight = case_when(
                  prior_kappa == 4 & prior_nu == 1024 ~ T,
                  T ~ F)),
              aes(fill = highlight), alpha = .1, inherit.aes = F,
              xmin = -Inf,xmax = Inf, ymin = -Inf, ymax = Inf)) +
  scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
  guides(fill = "none")

best_performing_case <- d.AA.representations %>%
  filter(category == Item.Intended_category) %>%
  filter(prior_kappa == 4 & prior_nu == 1024) %>%
  group_by(Condition,prior_nu,prior_kappa) %>%
  summarise(mAcc = round(mean(response), digits = 2))

best_performing_increase_log <- qlogis(best_performing_case$mAcc[2]) - qlogis(best_performing_case$mAcc[1])
```

Figure \@ref(fig:AA-result-changes-in-representations) shows the predicted categorization accuracy after exposure to L1- or L2-accented speech, as a function of the strength of the prior beliefs for the category means and variances. Both signature results described above can be predicted by changes in category representations. First, for L1-accented exposure, the accuracy is always predicted to be lower for /d/ than for /t/, indicating that /d/ is often misheard as /t/.^[One additional finding for the L1-accented condition is of note. Compared across the rows from top to bottom, accuracy in the L1-accented condition improves for models with weaker prior beliefs about the category variances (small $\nu_{0,c}$, bottom rows of Figure \@ref(fig:AA-result-changes-in-representations)). At first blush, this is surprising given that listeners in the L1-exposure condition receive only input that matches their prior L1 expectations and mismatches the L2 speech they receive during test. However, for this instance of the (simulated) experiment, it turns out that the L1-accented /d/-exposure stimuli in Figure \@ref(fig:study-AA-exposure-test-plot) happen to---by chance---exhibit somewhat larger sample variance along f0 ($s^2$ = `r sample_var$s_var.f0[1]`) than is expected for a typical L1 talker [$\sigma^2$ = `r population_var$prior_variance[2]` based on the data from @chodroff-wilson2018]. The L2-accented /d/-exposure stimuli show a similar pattern, albeit slightly less so (sample variance $s^2$ = `r sample_var$s_var.f0[3]`). For these particular combinations of stimuli, a learner with weak prior beliefs about the category variance will thus correctly come to expect a larger category variance during test, leading to improved recognition accuracy on the L2-accented test tokens. This result is an example of the type of unexpected insights that fully specified linking hypotheses can yield. It also illustrates that some results of perception experiments can be highly dependent on the specific stimulus statistics of the exposure and/or test tokens. A failure to take these factors into account---as remains common---can make results *appear* surprising even when they are not.] Second, L2-accented exposure is predicted to improve recognition accuracy of /d/ compared to L1-accented exposure, and this improvement occurs without equivalent decreases in the recognition accuracy of /t/. 
<!-- The best-performing model predicts a substantial increase from `r best_performing_case$mAcc[1]` to `r best_performing_case$mAcc[2]` after L2-accented exposure, compared to L1-accented exposure (+`r best_performing_increase_log` log-odds).  -->

Looking across the distinct $\kappa_{0,c}$s and $\nu_{0,c}$ values, we make two more observations. On the one hand, faster learning of category means is generally beneficial under the current scenario. When listeners' beliefs about category covariances are held constant (i.e., looking within each row), smaller $\kappa_{0,c}$ values yield higher categorization accuracy. This is a straightforward consequence of the fact that the mean of the /d/ category is shifted in the L2 accents, as shown in Figure \@ref(fig:study-AA-exposure-test-plot). Adapting to the L2 accent thus hinges primarily on learning the new category mean for the /d/ category. On the other hand, updating beliefs about the _variance_ too rapidly is not necessarily desirable, as seen by comparing panels within each column. In the current scenario, the _true_ underlying variance of /d/ is unchanged between the L1 and the L2 accents. Weaker beliefs about the category covariance thus can run the risk of expanding the categories to a degree that causes miscategorization of relatively ambiguous tokens. This is an example of the flexibility-stability trade-off that human listeners are known to exhibit, and that any theory of adaptive speech perception needs to account for [@kleinschmidt-jaeger2015, pp. 178-182].

Taken together, these results show that changes in representation can generally explain the types of results that are considered the signature of accent adaptation [e.g., @bradlow-bent2008; @clarke-garrett2004; @sidaras2009; @xie2016].

### Changes in decision-making

```{r study-AA-models-changes-in-decision-making-functions}
add_prior_and_posterior_with_changed_response_biases_based_on_exposure.AA <- function(
  data.exposure,
  prior,
  idealized = T,
  decision_rule = if (idealized) "proportional" else "sample",
  keep.update_history = FALSE,
  keep.exposure_data = FALSE
) {
  suppressWarnings(data.exposure %>%
    nest(data = -c(Condition, Subject)) %>%
    crossing(
      posterior.lapse_rate = c(.0005, .005, .05, .5, 1),
      beta_pi = c(0, .05, .1, .2, .8)) %>%
    crossing(
      prior %>%
        filter(prior_kappa == max(prior_kappa), prior_nu == max(prior_nu)) %>%
        nest(prior = everything())) %>%
    group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
    mutate(
      posterior = pmap(
        .l = list(data, prior, posterior.lapse_rate, beta_pi),
        .f = function(.data, .prior, .posterior.lapse_rate, .beta_pi)
          update_NIW_response_bias_incrementally(
            prior = .prior,
            exposure = .data,
            exposure.category = "Item.Category",
            exposure.cues = c("VOT", "f0_Mel"),
            beta = .beta_pi,
            decision_rule = decision_rule,
            noise_treatment = if (idealized) "marginalize" else "sample",
            lapse_treatment = if (idealized) "marginalize" else "sample",
            keep.update_history = keep.update_history,
            keep.exposure_data = keep.exposure_data) %>%
        mutate(lapse_rate = .posterior.lapse_rate))))
}
```

(ref:AA-result-changes-in-decision-making) Predictions of a model that derives accent adaptation as changes in decision-making. Predicted categorization responses are shown for the test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, depending on the rate at which response biases change ($\beta_{\pi}$) and the rate of attentional lapses ($\lambda_{posterior}$). The highlighted panel corresponds to the combination of $\beta_{\pi}$ and $\lambda_{posterior}$ resulting in the best overall accuracy.

```{r AA-result-changes-in-decision-making, fig.width= base.width * 5, fig.height= base.height * 5 +.5, fig.cap="(ref:AA-result-changes-in-decision-making)", warning=FALSE}
d.AA.bias <- d.AA.exposure %>%
  add_prior_and_posterior_with_changed_response_biases_based_on_exposure.AA(prior = m.ia.VOT_f0.AA) %>%
  add_test_tokens(d.AA.test) %>%
  add_categorization_functions() %>%
  group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
  add_categorization() %>%
  distinct() %>%
  left_join(., d.AA.test %>% select(x, Item.Intended_category), by = "x") %>%
  distinct()

p.AA.results.bias <- d.AA.bias %>%
  filter(category == Item.Intended_category) %>% # show the response for intended category
  ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
  stat_summary(fun = mean,
               geom="bar", position = pos,
               width = 0.6) +
  stat_summary(aes(color = Item.Intended_category),fun.data = mean_cl_boot,
               geom = "uperrorbar",
               position = pos, width = 0.2) +
  coord_cartesian(ylim =  c(0,1)) +
  scale_color_manual("Category", values = colors.category) +
  scale_fill_manual("Category", values = colors.category) +
  scale_alpha_discrete(range = c(0.2, 1), guide = "none") +
  scale_x_discrete(labels= c("L1-\naccented", "L2-\naccented")) +
  xlab("Exposure condition") +
  ylab("Predicted categorization accuracy") +
  geom_text(inherit.aes = FALSE, data = . %>%
               group_by(Condition,posterior.lapse_rate,beta_pi) %>%
               summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
  facet_grid(
    posterior.lapse_rate ~ beta_pi,
    labeller = label_bquote(
      cols = beta[pi] == .(beta_pi),
      rows = lambda[posterior] == ~.(posterior.lapse_rate))) +
  myGplot.defaults(base_size = 14, set_theme = F) +
  theme(legend.position = "top", panel.grid.major.x = element_blank())

p.AA.results.bias +
  ggnewscale::new_scale_fill() +
  insert_layers(
      geom_rect(data = d.AA.bias %>%
    filter(category == Item.Intended_category) %>%
                  distinct(posterior.lapse_rate, beta_pi) %>%
                  mutate(highlight = case_when(
                    posterior.lapse_rate == min(d.AA.bias$posterior.lapse_rate) & beta_pi == max(d.AA.bias$beta_pi) ~ T,
                    T ~ F)),
                aes(fill = highlight), alpha = .1, inherit.aes = F,
                xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf)
      ) +
     scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
  guides(fill = "none")

best_performing_case.bias <- d.AA.bias %>%
  filter(category == Item.Intended_category) %>%
  filter(posterior.lapse_rate == min(d.AA.bias$posterior.lapse_rate) & beta_pi == max(d.AA.bias$beta_pi)) %>%
  summarise(mAcc = round(mean(response), digits = 2))
```

Figure \@ref(fig:AA-result-changes-in-decision-making) shows the effects of changes in decision-making, depending on the rate at which response biases change $\beta_{\pi}$ and the lapse rate $\lambda$. For L1-accented exposure, we again see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens, regardless of the specific value of ($\beta_{\pi}$). The second signature result---improved accuracy after L2-accented exposure for /d/---is obtained for sufficiently large $\beta_{\pi}$. The largest improvements are found for the smallest lapse rates and fastest change rates (gray panel in Figure \@ref(fig:AA-result-changes-in-decision-making)).

Figure \@ref(fig:AA-result-changes-in-decision-making) also shows that changes in response biases do not necessarily result in the type of zero-sum trade-off that is sometimes attributed to them---accuracy improving by some degree for one category while decreasing by the same degree for the other category. Rather, L2-accented exposure can predict improvements in the *overall* recognition accuracy across both categories (e.g., top right panel). Indeed, zero-sum trade-offs are only expected under the highly implausible assumptions that responses are entirely independent of stimulus properties (bottom row of Figure \@ref(fig:AA-result-changes-in-decision-making)).

### Changes in normalization

```{r study-AA-models-normalization-functions}
add_prior_and_normalize_test_tokens_based_on_exposure.AA <- function(data.exposure, data.test, prior.normalization, prior.categories) {
  # Get prior mean
  mu_0 <- prior.normalization$x_mean[[1]]

  # Get normalization parameters for exposure data
  exposure.normalization <- data.exposure %>%
    group_by(Condition, Subject) %>%
    summarise(
      x_N = length(x),
      x_mean = list(colMeans(reduce(x, rbind))),
      x_cov = list(cov(reduce(x, rbind))))

  data.exposure %>%
    nest(data = -c(Condition, Subject)) %>%
    crossing(
      normalization = factor(c("no normalization", "centered based\non exposure"), levels = c("no normalization", "centered based\non exposure")),
      prior_kappa.normalization = 4^(1:5),
      prior.categories %>%
        filter(prior_kappa == max(prior_kappa) & prior_nu == max(prior_nu)) %>%
        nest(prior = everything())) %>%
    group_by(Condition, Subject, prior_kappa.normalization) %>%
    mutate(posterior = prior) %>%
    crossing(data.test %>% distinct(x)) %>%
    # Normalize test tokens
    mutate(
      # Get inferred mean
      mu_inferred = pmap(
        .l = list(Condition, Subject, prior_kappa.normalization),
        .f = function(currentCondition, currentSubject, currentKappa) {
          x_N <- exposure.normalization[exposure.normalization$Condition == currentCondition & exposure.normalization$Subject == currentSubject, "x_N"][[1]]
          x_bar <- exposure.normalization[exposure.normalization$Condition == currentCondition & exposure.normalization$Subject == currentSubject, "x_mean"][[1]][[1]]

          mu <- 1 / (currentKappa + x_N) * (currentKappa * mu_0 + x_N * x_bar)
          return(mu)
        }),
      x = ifelse(normalization == "no normalization", x, map2(x, mu_inferred, ~ .x - (.y - mu_0)))
      ) %>%
    nest(x = c(x))
}
```

Finally, we compare models that normalize test tokens based on the phonetic inputs experienced during exposure to models that continue to apply normalization based on previous long-term L1 experience. Figure \@ref(fig:AA-result-changes-in-normalization) shows the predicted categorization accuracy following changes in normalization in L1- and L2-accented exposure conditions. We again obtain both signature results of experiments on accent adaptation. For L1-accented exposure, we continue to see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens. For L2-accented exposure, recognition accuracy is improved for /d/ test tokens without the cost of equivalent decreases in recognition accuracy for /t/. That is, like the other two change models, normalization can predict *overall* improvements in recognition accuracy after L2-accented exposure.

```{r study-AA-models-normalization}
d.AA.normalization <-
  d.AA.exposure %>%
  add_prior_and_normalize_test_tokens_based_on_exposure.AA(
    data.test = d.AA.test,
    prior.normalization = prior_marginal_VOT_f0_stats,
    prior.categories = m.ia.VOT_f0.AA) %>%
  group_by(Condition, Subject, prior_kappa.normalization, normalization) %>%
  add_categorization_functions() %>%
  add_categorization() %>%
  distinct() %>%
  ungroup() %>%
  mutate(
    prior_kappa.normalization = factor(as.character(prior_kappa.normalization), levels = as.character(rev(sort(unique(prior_kappa.normalization))))))

# get ItemID for normalized cue values so that the intended_category is known
pair.ItemID.observationID <- d.AA.normalization %>%
  select(observationID, x, category) %>%
  left_join(., d.AA.test %>% select(x, Item.Intended_category, ItemID), by = "x") %>%
  ungroup() %>%
  filter(!is.na(ItemID)) %>%
  distinct(observationID, ItemID)

d.AA.normalization %<>%
  left_join(pair.ItemID.observationID) %>%
  left_join(d.AA.test %>% select(Item.Intended_category, ItemID), by = "ItemID") %>%
  distinct()

best_performing_case.normalization <- d.AA.normalization %>%
  group_by(Condition) %>%
  filter(category == Item.Intended_category) %>%
  filter(normalization == "centered based\non exposure") %>%
  filter(prior_kappa.normalization == min(as.numeric(as.character(d.AA.normalization$prior_kappa.normalization)))) %>%
  summarise(mAcc = round(mean(response), digits = 2))
```

(ref:AA-result-changes-in-normalization) Predictions of a model that derives accent adaptation from changes in normalization, depending on the relative weighting of previous experience ($\kappa_0$) during the inference of the cue mean during exposure. The highlighted panel corresponds to the $\kappa_0$ resulting in the best overall accuracy.

```{r AA-result-changes-in-normalization, fig.width= base.width * 5, fig.height= base.height + 1.5, fig.cap="(ref:AA-result-changes-in-normalization)", warning=FALSE}
p.AA.results.normalization <- d.AA.normalization %>%
  filter(category == Item.Intended_category) %>% # show the response for intended category
  filter(normalization == "centered based\non exposure") %>%
  ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
  stat_summary(fun = mean,
               geom="bar", position = pos,
               width = 0.6) +
    stat_summary(aes(color = Item.Intended_category),
               fun.data = mean_cl_boot,
               geom = "uperrorbar",
               position = pos, width = 0.2) +
  coord_cartesian(ylim =  c(0,1)) +
  scale_color_manual("Category", values = colors.category) +
  scale_fill_manual("Category", values = colors.category) +
  scale_alpha_discrete(range = c(0.2, 1), guide = "none") +
  xlab("Exposure condition") +
  ylab("Predicted \ncategorization accuracy") +
  scale_x_discrete(labels= c("L1-\naccented", "L2-\naccented")) +
  geom_text(inherit.aes = FALSE, data = . %>%
               group_by(Condition, normalization, prior_kappa.normalization) %>%
               summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
  facet_grid(
    . ~ prior_kappa.normalization,
    labeller = label_bquote(
      cols = ~kappa[0] == .(as.character(prior_kappa.normalization)))) +
  myGplot.defaults(base_size = 14, set_theme = F) +
  theme(legend.position = "top", panel.grid.major.x = element_blank())

p.AA.results.normalization +
  ggnewscale::new_scale_fill() +
  insert_layers(
      geom_rect(data = d.AA.normalization %>%
                  filter(category == Item.Intended_category) %>%
                  filter(normalization == "centered based\non exposure") %>%
                  distinct(prior_kappa.normalization) %>%
                  mutate(highlight = case_when(
                    prior_kappa.normalization == min(as.numeric(as.character(d.AA.normalization$prior_kappa.normalization))) ~ T,
                    T ~ F)),
                aes(fill = highlight), alpha = .1, inherit.aes = F,
                xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf)
      ) +
     scale_fill_manual(breaks = c(T, F), values = c("darkgray", "white")) +
  guides(fill = "none")
```

## Summary
<!-- The range of the overall improvements predicted after L2-accented exposure includes effect sizes that have been observed in similar previous experiments on accent adaptation [e.g., ~3% in @xie2016jep; ~3.5% in @xie2018lcn, cf. panels towards the top left in Figure \@ref(fig:AA-result-changes-in-representations)]. This finding should, however, be interpreted with caution.  -->  

Paralleling Case Study 1 on perceptual recalibration, we find that any of the three change mechanisms can *qualitatively* explain the signature results of experiments on accent adaptation. Contrary to common assumptions, even non-representational mechanisms---normalization and decision-making---can lead to _overall_ improvements in recognition accuracy, rather than only zero-sum trade-offs. Overall improvements in accuracy after exposure to an unfamiliar accent are thus _not_ diagnostic of representational changes. This supports the conclusion of Case Study 1: at the level of analysis that is commonly applied in previous work (and thus here), experiments on accent adaptation do not decisively rule out any of the three mechanisms as a driver of the facilitatory effects of exposure.

Beyond their similarity in qualitative fit to existing behavioral patterns, the three change models exhibit some quantitative differences. The maximal benefits of L2-accented exposure are predicted to be largest for the representational change model (overall improvements from `r best_performing_case$mAcc[1]` to `r best_performing_case$mAcc[2]`), followed by the decision-making change model (from `r best_performing_case.bias$mAcc[1]` to `r best_performing_case.bias$mAcc[2]`) and smallest in the normalization change model (`r best_performing_case.normalization$mAcc[1]` to `r best_performing_case.normalization$mAcc[2]`). <!--This difference has to be interpreted with caution for at least two reasons. First, the present simulations consider only a range of parameter settings. It is thus possible that, for example, higher values of $\beta_{\pi}$ yield larger benefits of L2-accented exposure from changes in decision-making.--><!-- We note, however, that the magnitude of overall accuracy improvements after talker-specific L2-accented exposure that has been observed in experiments [e.g., about 6% in @xie2016jep; ~10% in @bradlow-bent2008] falls well within the range that any of the three change models can explain (for the present stimuli and parameter settings).-->However, these differences do not mean that the representational change model is always more advantageous than the other two. Rather, it exemplifies a critical prediction of the ASP framework: the specific quantitative benefit of L2-accented exposure under each mechanisms depends on distributional structures of the exposure and test stimuli of the experiment and thus---if naturally produced speech is employed---how exactly acoustic-phonetic distributions of an L2 accent deviate from those of an L1 accent. 

To make this point more concrete, Figure \@ref(fig:AA-result-comparison-best-three-models-case2) presents three additional (simulated) scenarios that are common for L2 accents, along with the best-performing variants of each change model in each scenario. The top panel shows an example of *contrast shift*, where two categories in L2-accented speech are shifted along one or more of the cue dimensions as compared to L1-accented speech [e.g., attested for word-initial stops in French-accented English, @Sumner2011a]. The middle panel shows an example of *contrast reduction*, where an L2-accent shows a greater category overlap than an L1-accent [e.g., attested for vowels in Spanish-accented English, @wade2007]. The bottom panel shows an example of *contrast collapse*, where two contrasting categories in an L1-accent overlap substantilly or completely in an L2-accent [e.g., characteristic of a loss of distinction between [s] and [`r linguisticsdown::cond_cmpl("")`] in Mandarin-accented English, @zheng-samuel2020]. The L1-accented exposure condition is the same as in the scenario above (Figure \@ref(fig:study-AA-exposure-test-plot) and held constant across the three scenarios. <!-- CK: I added the following. Redundant?--> As above, test tokens are randomly sampled from the distribution of each of the L2-accent category. Consequently, test tokens varied across the three scenarios examined here. For consistency, we focus on the model parameter settings identical to the main scenario. 

Contrasted side-by-side, the three scenarios differ significantly both in terms of the predicted types and levels of difficulty in the L2-accent category recognition for L1-listeners (L1-accented exposure condition) and the amount of improvements expected post exposure (L2-accented exposure condition). For instance, the recognition of /t/ is more difficult than the recognition of /d/ in the case of a contrast shift (top panel), whereas it was the opposite in the other two scenarios as well as in the main scenario (Figure \@ref(fig:AA-result-changes-in-representations), \@ref(fig:AA-result-changes-in-decision-making), \@ref(fig:AA-result-changes-in-normalization)). This illustrates that arguably subtle differences in the underlying distributional strucutres of L1- vs. L2-accent categories (e.g., contrast shift vs. reduction) could give rise to starkly different perceptual consequences for L1-accented listeners. Similarly, the relative benefit of L2-accented exposure---compared to L1-accented exposure---as predicted by each change model, differs across these scenarios. This latter point offers three insights of critical importance.

<!-- CK: For each point, I tried to summarize the core conceptual point in the first sentence-->
First, the same exact change model can support more or less benefit according to properties of an L2-accent (or L1-L2 accent difference). Consider the case of a contrast shift (top panel). While changes in representations and normalization both predict large improvements in overall recognition accuracy in the L2-accented exposure, changes in decision making does not. (The predicted accuracy is .61 for both the L1-accented and L2-accented exposure conditions). This outcome aligns with the zero-sum trade-offs often attributed to this change mechanism (i.e., The recognition of /t/ improves at the cost of the recognition of /d/). The heterogeneity of the results across the three scenarios make it clear that it is not an inherent property of the decision-making mechanism. It is instead a possible outcome arising from a combination of a specific change model and properties of the L2 accent (or L1-L2 accent difference). 

Second, changes of representations do not always outperform the other two mechanisms. In the case of contrast reduction (middle panel), the best overall performance is predicted from changes in decision making, followed by normalization, and representations. While it is possible that the representational model can perform better when a wider variety of parameters are considered, it is important to keep it in mind that the computational complexity afforded for the representational change model does not necessarily predict a greater degree of recognition improvements than the other two mechanisms.

Lastly, for some contrast types, none of the change models predicts significant benefits from L2-accent exposure. The scenario of contrast collapse (bottom panel) provides a case in point. Recognition accuracy improvements in the L2-exposure condition, as compared to those in the L1-accent exposure condition are negligible, if any. This result highlights a critical, yet often under-appreciated, insight: given specific category statistics, one should expected a null result in an exposure-test experiment [@tan2021]. Put differently, an *absence of improvements* should not be taken as evidence to reject any of the three mechanisms. Rejection of a mechanism is only valid if it is in fact *predicted* to facilitate comprehension during test. As can be seen in the comparison of the three scenarios here, whether this prediction holds for a given experiment depends on the exposure and test stimuli---an assumption that is sometimes not considered in the interpretation of experiments [cf. @zheng-samuel2020]. 

In summary, benefits of L2-accent exposure expected under each change mechanism depend on specifics of L2-accented categories.<!--The best-fitting parameters for each change model are very similar across these three scenarios simulated and the main scenario presented above. This is a direct consequence of us having-->Beyond the three scenarios examined here (contrast shift, reduction, and collapse), naturally produced L2-accents exhibit substantial variability due to factors both relatively predictable (e.g., mismatches between L1- vs. L2 sound systems) and random (e.g., individual variations). Even with more predictable factors, there are a wide range of possibilities in terms of how a given L2-talker's L1 and L2 weight the same cues in signaling a specific phonetic contrast, or even whether a cue is used at all in the realization of a phonetic contrast [@ingvalson2011; @liu-holt2015; @schertz2013; @yamada-tohkura1992]. These differences cause L2-accented categories to deviate from L1-accented categories not only in terms of their category means, but also in category variances, and/or category covariances [the category-specific correlation between cues, e.g., @smith2019; @wade2007; @vaughn2019; @xie-jaeger2020]. These differences can pose difficulties to L1-listeners and researchers alike. However, at the same time, they also offer a furtile ground to test the relative engagement of each change mechanism. Because the mechanisms would predict different perceptual outcomes depending on the properties of the exposure and test stimuli, we can use different accent categories as testing grounds for testing their distinct predictions.  Next, we discuss the implications of our findings and provide recommendations for future research.

```{r compare-AA-models-best-three-functions}
# Function to construct change model of each type and select the best performing parameter combination
construct_three_best_models <- function(
    experimenter.ideal_observer,
    category_dist_ratio1 = dist.L2_category.cue1,
    category_dist_ratio2 = dist.L2_category.cue2,
    shift_mean_1 = shift.cue1, 
    shift_mean_2 = shift.cue2,
    test.n_block = n.test_block,
    n_test = n.test.token) {
  exposure.data <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, category_dist_ratio1 = dist.L2_category.cue1, category_dist_ratio2 = dist.L2_category.cue2, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)
  exposure.data %<>%
    add_subjects_to_exposure(n.subject = n.subject)

  test.data <- make_accent_adaptation_test_design(exposure.data, experimenter.ideal_observer = m.io.VOT_f0.AA, test.n_block = n.test_block, n_test = n.test.token, category_dist_ratio1 = dist.L2_category.cue1, category_dist_ratio2 = dist.L2_category.cue2, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)

  test.data %<>%
    add_subjects_to_test(n.subject = n.subject)

  d.AA.representations <- exposure.data %>%
    add_prior_and_get_posterior_beliefs_based_on_exposure(prior = m.ia.VOT_f0.AA) %>%
    add_test_tokens(test.data) %>%
    add_categorization_functions() %>%
    group_by(Condition, Subject, prior_kappa, prior_nu) %>%
    add_categorization() %>%
    ungroup() %>%
    mutate_at(
      vars(starts_with("prior_")),
      ~ factor(as.character(.x), levels = as.character(rev(sort(unique(.x)))))) %>%
    distinct() %>%
    left_join(
      test.data %>%
        select(x, Item.Intended_category), by = "x") %>%
    distinct()
  

  d.AA.bias <- exposure.data %>%
    add_prior_and_posterior_with_changed_response_biases_based_on_exposure.AA(prior = m.ia.VOT_f0.AA) %>%
    add_test_tokens(test.data) %>%
    add_categorization_functions() %>%
    group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
    add_categorization() %>%
    distinct() %>%
    left_join(., test.data %>% select(x, Item.Intended_category), by = "x") %>%
    distinct()

  d.AA.normalization <-
    exposure.data %>%
    add_prior_and_normalize_test_tokens_based_on_exposure.AA(
      data.test = test.data,
      prior.normalization = prior_marginal_VOT_f0_stats,
      prior.categories = m.ia.VOT_f0.AA) %>%
    group_by(Condition, Subject, prior_kappa.normalization, normalization) %>%
    add_categorization_functions() %>%
    add_categorization() %>%
    distinct() %>%
    ungroup() %>%
    mutate(
      prior_kappa.normalization = factor(as.character(prior_kappa.normalization), levels = as.character(rev(sort(unique(prior_kappa.normalization))))))

  # get ItemID for normalized cue values so that the intended_category is known
  pair.ItemID.observationID <- d.AA.normalization %>%
    select(observationID, x, category) %>%
    left_join(., test.data %>% select(x, Item.Intended_category, ItemID), by = "x") %>%
    ungroup() %>%
    filter(!is.na(ItemID)) %>%
    distinct(observationID, ItemID)

  d.AA.normalization %<>%
    left_join(pair.ItemID.observationID) %>%
    left_join(test.data %>% select(Item.Intended_category, ItemID), by = "ItemID") %>%
    distinct()

 # select the parameter combination for each model that results in the best overall accuracy
  d <- bind_rows(
    d.AA.representations %>% filter(category == Item.Intended_category & prior_kappa == 4 & prior_nu == 1024) %>% mutate(ModelType = "representations"),
    d.AA.bias %>% filter(category == Item.Intended_category & posterior.lapse_rate == min(d.AA.bias$posterior.lapse_rate) & beta_pi == max(d.AA.bias$beta_pi)) %>% mutate(ModelType = "decision making"),
    d.AA.normalization %>% filter(category == Item.Intended_category & normalization == "centered based\non exposure" & prior_kappa.normalization == min(as.numeric(as.character(d.AA.normalization$prior_kappa.normalization)))) %>% mutate(ModelType = "normalization")) %>%
    mutate(ModelType = factor(ModelType, levels = c("representations", "decision making", "normalization")))


  return(d)
}
```

<!--(ref:AA-result-comparison-best-three-models-case2) A simulated accent adaptation experiment representing another common type of L1-L2 accent difference: contrast collapse or reduction. **Panel A - Exposure:** Distribution of the stimuli used during the exposure phase shows how the L2-accented speech almost entirely collapses the /d/-/t/ contrast along VOT without increasing informativity of f0 (cf. Figure \@ref(fig:study-AA-exposure-test-plot)A). **Panel B - Test:**  Distribution of the stimuli used during the test phase (cf. Figure \@ref(fig:study-AA-exposure-test-plot)B). **Panel C - Change model predictions:** Predicted categorization accuracies for the L2-accented test tokens after L1-accented and L2-accented exposure, for the *best-performing* parameter settings of each of the three change models (cf. gray panels in Figures \@ref(fig:AA-result-changes-in-representations)-\@ref(fig:AA-result-changes-in-normalization)), from left to right: representations ($\kappa_{0,/d/}$ = $\kappa_{0,/t/}$ = 4 and $\nu_{0,/d/}$ = $\nu_{0,/t/}$ = 1024), decision making ($\beta_{\pi}$ = 0.8, $\lambda_{posterior}$ = 5e-04), and normalization ($\kappa_{0}$ = 4). The best-performing model was determined by considering the same range of parameter settings as in Figures \@ref(fig:AA-result-changes-in-representations)-\@ref(fig:AA-result-changes-in-normalization). The average accuracy across all test tokens for each condition is shown above the bars. Error bars show 95% bootstrapped confidence intervals.-->

```{r AA-result-comparison-best-three-models-case2, fig.width=base.width*3 + .5, fig.height=base.height*2 + .5, fig.cap = "(ref:AA-result-comparison-best-three-models-case2)", warning=FALSE, eval = FALSE}
# Examine a new scenario
dist.L2_category.cue1 <- 0.5
dist.L2_category.cue2 <- 0
shift.cue1 <- 0
shift.cue2 <- 0

d.AA.exposure.case2 <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)
d.AA.test.case2 <- make_accent_adaptation_test_design(d.AA.exposure.case2, experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)


d.AA.results.best.case2  <- construct_three_best_models(experimenter.ideal_observer = m.io.VOT_f0.AA)
p.top <- Make_exposure_test_plot(exposure.data = d.AA.exposure.case2, test.data = d.AA.test.case2)


p.bottom <- d.AA.results.best.case2 %>%
  filter(category == Item.Intended_category) %>%
ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
    stat_summary(fun = mean,
                 geom="bar", position = pos,
                 width = 0.6) +
      stat_summary(aes(color = Item.Intended_category),
                 fun.data = mean_cl_boot,
                 geom = "uperrorbar",
                 position = pos, width = 0.2) +
    coord_cartesian(ylim =  c(0,1)) +
    scale_color_manual("Category", values = colors.category) +
    scale_fill_manual("Category", values = colors.category) +
    scale_alpha_discrete(range = c(0.2, 1), guide = "none") +
    xlab("Exposure condition") +
    ylab("Predicted \ncategorization accuracy") +
    scale_x_discrete(labels= c("L1-\naccented", "L2-\naccented")) +
    geom_text(inherit.aes = FALSE, data = . %>%
                 group_by(Condition, ModelType) %>%
                 summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
    facet_grid(
      . ~ ModelType, labeller = as_labeller(appender)) +
    theme(legend.position = "none", panel.grid.major.x = element_blank())

plot_grid(p.top, p.bottom, labels = c('', 'C)'), ncol = 1, rel_heights = c(1, 1.2), rel_widths = c(2,1))
```


<!--(ref:AA-result-comparison-best-three-models-case3) A simulated accent adaptation experiment representing another common type of L1-L2 accent difference: shifted categories with a good amount of between-category distinction. **Panel A - Exposure:** Distribution of the stimuli used during the exposure phase shows how the L2-accented speech almost entirely collapses the /d/-/t/ contrast along VOT without increasing informativity of f0 (cf. Figure \@ref(fig:study-AA-exposure-test-plot)A). **Panel B - Test:** Distribution of the stimuli used during the test phase (cf. Figure \@ref(fig:study-AA-exposure-test-plot)B). **Panel C - Change model predictions:** Predicted categorization accuracies for the L2-accented test tokens after L1-accented and L2-accented exposure, for the *best-performing* parameter settings of each of the three change models (cf. gray panels in Figures \@ref(fig:AA-result-changes-in-representations)-\@ref(fig:AA-result-changes-in-normalization)), from left to right: representations ($\kappa_{0,/d/}$ = $\kappa_{0,/t/}$ = 4 and $\nu_{0,/d/}$ = $\nu_{0,/t/}$ = 1024), decision making ($\beta_{\pi}$ = 0.8, $\lambda_{posterior}$ = 5e-04), and normalization ($\kappa_{0}$ = 4). The best-performing model was determined by considering the same range of parameter settings as in Figures \@ref(fig:AA-result-changes-in-representations)-\@ref(fig:AA-result-changes-in-normalization). The average accuracy across all test tokens for each condition is shown above the bars. Error bars show 95% bootstrapped confidence intervals.-->

```{r AA-result-comparison-best-three-models-case3, fig.width=base.width*3 + .5, fig.height=base.height*2 + .5, fig.cap = "(ref:AA-result-comparison-best-three-models-case3)", warning=FALSE, eval = FALSE}
# Examine a new scenario
min_VOT = -100 # allows negative VOT when generating exposure and test data
dist.L2_category.cue1 <- 1
dist.L2_category.cue2 <- 0
shift.cue1 <- -1.2
shift.cue2 <- 0


d.AA.exposure.case3 <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)
d.AA.test.case3 <- make_accent_adaptation_test_design(d.AA.exposure.case3, experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)

d.AA.results.best.case3  <- construct_three_best_models(experimenter.ideal_observer = m.io.VOT_f0.AA)


p.top <- Make_exposure_test_plot(exposure.data = d.AA.exposure.case3, test.data = d.AA.test.case3)
p.bottom <- d.AA.results.best.case3 %>%
ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
    stat_summary(fun = mean,
                 geom="bar", position = pos,
                 width = 0.6) +
      stat_summary(aes(color = Item.Intended_category),
                 fun.data = mean_cl_boot,
                 geom = "uperrorbar",
                 position = pos, width = 0.2) +
    coord_cartesian(ylim =  c(0,1)) +
    scale_color_manual("Category", values = colors.category) +
    scale_fill_manual("Category", values = colors.category) +
    scale_alpha_discrete(range = c(0.2, 1), guide = "none") +
    xlab("Exposure condition") +
    ylab("Predicted \ncategorization accuracy") +
    scale_x_discrete(labels= c("L1-\naccented", "L2-\naccented")) +
    geom_text(inherit.aes = FALSE, data = . %>%
                 group_by(Condition, ModelType) %>%
                 summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
    facet_grid(
      . ~ ModelType, labeller = as_labeller(appender)) +
    theme(legend.position = "none", panel.grid.major.x = element_blank())

plot_grid(p.top, p.bottom, labels = c('', 'C)'), ncol = 1, rel_heights = c(1, 1.2), rel_widths = c(2,1))
```


```{r AA-result-comparison-best-three-models-case4, fig.width=base.width*3 + .5, fig.height=base.height*2 + .5, fig.cap = "(ref:AA-result-comparison-best-three-models-case4)", warning=FALSE, eval = FALSE}
# Examine a new scenario
min_VOT = -100 # allows negative VOT when generating exposure and test data
# Examine a new scenario
dist.L2_category.cue1 <- 0
dist.L2_category.cue2 <- 0
shift.cue1 <- -0.6
shift.cue2 <- 0


d.AA.exposure.case4 <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)
d.AA.test.case4 <- make_accent_adaptation_test_design(d.AA.exposure.case4, experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)

d.AA.results.best.case4  <- construct_three_best_models(experimenter.ideal_observer = m.io.VOT_f0.AA)


p.top <- Make_exposure_test_plot(exposure.data = d.AA.exposure.case4, test.data = d.AA.test.case4)
p.bottom <- d.AA.results.best.case4 %>%
ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
    stat_summary(fun = mean,
                 geom="bar", position = pos,
                 width = 0.6) +
      stat_summary(aes(color = Item.Intended_category),
                 fun.data = mean_cl_boot,
                 geom = "uperrorbar",
                 position = pos, width = 0.2) +
    coord_cartesian(ylim =  c(0,1)) +
    scale_color_manual("Category", values = colors.category) +
    scale_fill_manual("Category", values = colors.category) +
    scale_alpha_discrete(range = c(0.2, 1), guide = "none") +
    xlab("Exposure condition") +
    ylab("Predicted \ncategorization accuracy") +
    scale_x_discrete(labels= c("L1-\naccented", "L2-\naccented")) +
    geom_text(inherit.aes = FALSE, data = . %>%
                 group_by(Condition, ModelType) %>%
                 summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1), size = geom_text.size) +
    facet_grid(
      . ~ ModelType, labeller = as_labeller(appender)) +
    theme(legend.position = "none", panel.grid.major.x = element_blank())

plot_grid(p.top, p.bottom, labels = c('', 'C)'), ncol = 1, rel_heights = c(1, 1.2), rel_widths = c(2,1))
```

```{r AA-plot-target-exposure-additional-cases, eval= FALSE}
# plot target exposure ellipses
d.AA.exposure.target.case2 <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, category_dist_ratio1 = 0.5, category_dist_ratio2 = 0, shift_mean_1 = 0, shift_mean_2 = 0, exposure.tokens.L1.n = 5000, exposure.tokens.L2.n = 5000)
d.AA.exposure.target.case4 <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, category_dist_ratio1 = 0, category_dist_ratio2 = 0, shift_mean_1 = -0.6, shift_mean_2 = 0, exposure.tokens.L1.n = 5000, exposure.tokens.L2.n = 5000)
min_VOT = -100 
d.AA.exposure.target.case3 <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, category_dist_ratio1 = 1, category_dist_ratio2 = 0, shift_mean_1 = -1.2, shift_mean_2 = 0, exposure.tokens.L1.n = 5000, exposure.tokens.L2.n = 5000)

d.AA.exposure.target.cases <- rbind(d.AA.exposure.target.case2 %>%
                               mutate(Case = "case2"),
                             d.AA.exposure.target.case3 %>%
                               mutate(Case = "case3"),
                             d.AA.exposure.target.case4 %>%
                               mutate(Case = "case4")
                             ) %>%
  mutate(Case = factor(Case, levels = c("case3", "case2", "case4"), 
                            labels = c("Contrast shift", "Contrast reduction", "Contrast collapse"))) 


# plot ellipses for actual exposure
p.AA.exposure <- d.AA.exposure.target.cases %>%
    # mutate(Condition = factor(Condition, levels = c("L1-accented", "L2-accented"), 
    #                           labels = c("Exposure condition 1", "Exposure condition 2"))) %>%
    ggplot(aes(x = VOT, y = f0_Mel)) +
  #  geom_point(alpha = 0.5) +
   stat_ellipse(aes(fill = Item.Category), level = .95, alpha = 0.3, geom = "polygon") +
    geom_rug(data = . %>%
                 group_by(Case, Condition, Item.Category) %>%
                 summarise(VOT = mean(VOT), f0_Mel = mean(f0_Mel)), aes(color = Item.Category), show.legend = FALSE) +
    scale_x_continuous(expression("VOT (ms)"), 
                        # breaks=seq(0, 120, by = 30),
                        expand = expansion(mult = .1, add = 0)
                        ) +
    scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0), breaks=seq(0, 500, by = 100)) +
    coord_cartesian(ylim = c(0, 500)) +
    scale_color_manual("Category", breaks = categories.AA, values = colors.category) +
    scale_fill_manual("Category", breaks = categories.AA, values = colors.category) +
    facet_grid(Case ~ Condition) + theme(legend.position="top") 

```

