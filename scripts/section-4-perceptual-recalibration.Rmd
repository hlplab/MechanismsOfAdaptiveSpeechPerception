# Case Study 1: perceptual recalibration {#sec:PR}

```{r study-PR-setup, message=FALSE}
# Which categories is this experiment about?
categories.PR <- c("/d/", "/t/")
conditions.PR <- paste0(categories.PR, "-shifted")
colors.category <- colors.condition <- colors.voicing 
shapes.category <- shapes.condition <- c(15, 17)
linetypes.condition <- c(1,2)

# Number of subjects
n.subject <- 1

# Number of exposure tokens for each category, shifted and typical
n.exposure.token <- 20 

# How strong is the average lexical bias? This can range from 0 (no lexical context effect on perception) to 
# 1 (the label perfectly determines perception) and will determine the placement of the ambiguous tokens.
my_experimenter.lexical_bias_strength <- .7
my_experimenter.variability_reduction <- 8

# Number of test blocks
n.test_block <- 1
# Test locations (proportion category 1)
test_locations <- c(.15, .35, .45, .55, .65, .85) %>% rev()

m.io.VOT_f0.PR <-
  m.VOT_f0_MVG  %>%
  filter(category %in% categories.PR) %>%
  droplevels() %>%
  make_stop_VOTf0_ideal_observer() %>%
  arrange(category)

m.ia.VOT_f0.PR <- 
  crossing(
    prior_kappa = 4^(1:5),
    prior_nu = 4^(1:5)) %>%
  rowwise() %>%
  mutate(
    ideal_adaptor = map2(prior_kappa, prior_nu, ~ make_stop_VOTf0_ideal_adaptor(m = m.io.VOT_f0.PR, kappa = .x, nu = .y))) %>%
  unnest(ideal_adaptor) %>%
  arrange(category)
```

In a typical perceptual recalibration experiment, participants are randomly assigned to one of two exposure groups that subsequently perform the same test. During exposure, participants hear stimuli that contain a critical phonetic contrast [e.g., syllable-intial /d/ vs. /t/, as in _croco*d*ile_ or _cafe*t*eria_, @kraljic-samuel2006], mixed with many fillers that do not contain that phonetic contrast. Between participants, critical exposure stimuli are manipulated with the goal to shift the category boundary along the phonetic contrast into different directions. This is achieved by exposing listeners to *typical* pronunciations from one category and pronunciations of the other category that are acoustically *shifted* towards the point of maximal perceptual ambiguity between the two categories. Which of the two categories is shifted is manipulated between the two exposure groups. To ensure that the shifted stimuli are still recognized as instances of the intended category, exposure stimuli are typically lexically or visually *labeled* (e.g., hearing the shifted sound /?dt/ embedded in the lexical context of "croco_ile" would label the shifted sound as a /d/). In a subsequent test phase, both groups of participants then categorize the same test stimuli along an artificially generated acoustic continuum between the two categories. The goal of this test phase is to estimate participants' categorization function for the critical phonetic contrast, and so test stimuli are *unlabeled*. This is achieved by presenting the manipulated continuum in the contexts of a lexical minimal pair (e.g., _dip_-_tip_) or a nonce-word pair (e.g., _idi_-_iti_).

For example, an influential study by @kraljic-samuel2006 exposed participants to either shifted /d/ and typical /t/ or shifted /t/ and typical /d/. Participants heard 20 typical and 20 shifted tokens (mixed with 160 fillers). Following exposure, participants in both conditions categorized non-word tokens (/`r linguisticsdown::cond_cmpl("ɪ")`d`r linguisticsdown::cond_cmpl("ɪ")`/-/`r linguisticsdown::cond_cmpl("ɪ")`t`r linguisticsdown::cond_cmpl("ɪ")`/) tokens along a six-step /d/-/t/ continuum. Figure \@ref(fig:kraljic-samuel-2007-replotted) shows the results obtained by Kraljic and Samuel (2006).^[We estimated the VOT range in Figure \@ref(fig:kraljic-samuel-2007-replotted) based on the observation that post-burst aspiration accounts for about $2/3^{rd}$s of the stop durations reported in Kraljic and Samuel (2007, p. 6). We also averaged over the two talkers employed in the study, as Kraljic and Samuel did not report results relative to talker-specific VOT ranges.] Although relatively small, a significant perceptual recalibration effect was observed: participants in the /d/-shifted exposure condition were more likely to give "d"-responses, resulting a categorization function that is shifted rightwards relative to participants in the /t/-shifted condition. This shift in the category boundary is the hallmark of perceptual recalibration experiments.

(ref:kraljic-samuel-2007-replotted) Categorization functions observed during the test phase of the perceptual recalibration experiment presented in Kraljic and Samuel (2006; replotted from Kraljic & Samuel, 2007, Figure 1). No error bars were provided in the original, and the original data are no longer available (Samuel, p.c.).

```{r kraljic-samuel-2007-replotted, fig.width = base.width + 2.5, fig.height = base.height, fig.cap="(ref:kraljic-samuel-2007-replotted)"}
replot_perceptual_recalibration_result <- function(categories, continuum.steps, responses, continuum.label = NULL, conditions = paste0("/", categories, "/-shifted")) {
  crossing(
    Condition = conditions,
    Step = as.character(continuum.steps)) %>%
    mutate(Proportion = responses) %>%
    ggplot(aes(x = Step, y = Proportion, color = Condition, linetype = Condition)) +
    geom_point() +
    geom_line(aes(group = Condition)) +
    scale_x_discrete(
      name = continuum.label, 
      breaks = continuum.steps, 
      labels = c(
        paste0("/", categories[1], "/\nVOT = ", round(((20+34)/2)*2/3), "ms"), 
        rep("", length(continuum.steps) - 2), 
        paste0("/", categories[2], "/\nVOT = ", round(((116+140)/2)*2/3), "ms"))) +
    scale_y_continuous(paste0('Proportion ', categories[1], '-responses'), limits = c(0, 1)) +
    scale_color_manual("Exposure condition", breaks = conditions, values = colors.condition) +
    scale_linetype_manual("Exposure condition", breaks = conditions, values = linetypes.condition)
}

p10 <- replot_perceptual_recalibration_result(
  categories = c("d", "t"), 
  continuum.steps = 1:6,
  responses = c(.93, .785, .31, .095, .05, .035, .895, .68, .28, .07, .025, .025))

p20 <- replot_perceptual_recalibration_result(
  categories = c("d", "t"), 
  continuum.steps = 1:6,
  responses = c(.95, .78, .41, .14, .055, .045, .885, .695, .36, .095, .05, .02))

p20 + theme(legend.position = "right")
```

<!-- In a seminal study, Norris et al. [-@norris2003] exposed Dutch listeners to 20 words that contained an [f] (e.g., "witlo*f*"---chicory) and 20 words that contained an [s] (e.g., "naaldbo*s*"---pine forest), mixed with 160 fillers that did not contain either sound. Between participants, either all instances of [f] *or* all instances of [s] were phonetically manipulated so as to make them perceptually ambiguous between [f] and [s]. The other sound remained unchanged. A third, baseline, condition left both [f] and [s] unchanged. During a subsequent test phase, participants heard instances of an acoustic continuum ranging from clear [`r linguisticsdown::cond_cmpl("ɛ")`f] to clear [`r linguisticsdown::cond_cmpl("ɛ")`s], and responded whether they heard "ef" or "es".  -->
<!-- These [`r linguisticsdown::cond_cmpl("ɛ")`f]-[`r linguisticsdown::cond_cmpl("ɛ")`s] stimuli were created using the same acoustic manipulation that was used to make the perceptually ambiguous exposure words.--> 
<!-- Norris and colleagues found that the two groups of participants exhibited strikingly different categorization responses during test: compared to participants in the baseline condition, participants who were exposed to shifted [f] and typical [s] shifted their categorization boundary (the step along the acoustic [`r linguisticsdown::cond_cmpl("ɛ")`f]–[`r linguisticsdown::cond_cmpl("ɛ")`s] continuum at which "ef" and "es" responses are equally likely) away from [`r linguisticsdown::cond_cmpl("ɛ")`f] towards [`r linguisticsdown::cond_cmpl("ɛ")`s], so that the participants now categorized more tokens along the [`r linguisticsdown::cond_cmpl("ɛ")`f]–[`r linguisticsdown::cond_cmpl("ɛ")`s] continuum as "ef"; participants who had heard typical [f] and shifted [s] did the opposite, shifting their categorization boundary away from [`r linguisticsdown::cond_cmpl("ɛ")`s] towards [`r linguisticsdown::cond_cmpl("ɛ")`f], categorizing more tokens as "es". These changes after exposure to just 20 shifted word recordings were substantial: e.g., the exact same [`r linguisticsdown::cond_cmpl("ɛ")`f]–[`r linguisticsdown::cond_cmpl("ɛ")`s] recording was categorized as "ef" more than 80% of the time by participants in the [f]-shifted condition but less than 40% of the time by participants in the [s]-shifted condition.  -->
Perceptual recalibration continues to be one of the most frequently used paradigm in research on adaptive speech perception. A recent review cites over 200 studies using variants of this paradigm [@theodore2021]. Boundary shifts like those in Figure \@ref(fig:kraljic-samuel-2007-replotted) (though often larger) have been demonstrated for an increasing number of phonetic contrasts and languages [e.g., @hanulikova-weber2012; @kraljic-samuel2005; @kraljic-samuel2006; @reinisch2013; @sumner2011; @vroomen2007]. We now know that boundary shifts can occur for both isolated and connected speech [e.g., @eisner-mcqueen2005; @reinisch-holt2013], even after fewer shifted tokens [e.g., as few as four, @liu-jaeger2018; @liu-jaeger2019] or under cognitive load [@baart-vroomen2010; @zhang-samuel2014; but see @samuel2016], and that they can persist over hours and days [@eisner-mcqueen2006; @vroomen-baart2009; @saltzman-myers2021] though they eventually decay [@samuel2021; @zheng-samuel2023]. More recent work has begun to identify the brain regions involved in perceptual recalibration, which range from primary auditory cortex and superior temporal cortices to more frontal and parietal areas [@bonte2017; @kilianhutten2011; @myers-mesite2014; @ullas2020; @luthra2020a; for review, see @guediche2014]. 

While the signature boundary shift observed in perceptual recalibration experiments is often considered the consequence of representational changes, it is also possible that this finding is compatible with explanations in terms of either of the two alternative change mechanisms. We use ASP to assess this possibility. Specifically, we predict changes in categorization following the type of exposure employed in experiments on perceptual recalibration, while switching on and off the three different change models of ASP. 

## Data
Most published experiments on perceptual recalibration do *not* measure and report the acoustic properties of their stimuli. We therefore generate data based on the same procedure that experimenters use to create and select the stimuli for perceptual recalibration experiments. The details of our stimulus generation approach, which closely follows the procedure described in @kraljic-samuel2006, are described in the SI (\@ref(sec:SI-PR)). These details do not affect the general results we present below. They are merely meant to provide a sufficiently concrete example. Readers can revisit any of the assumptions we made by downloading the [R markdown document that this article is generated from](https://osf.io/q7gjp/). 

### Exposure phase

```{r study-PR-exposure-functions"}
get_experimenter_posterior_of_category1 <- function(x, model) {
  get_categorization_from_MVG_ideal_observer(x, model, decision_rule = "proportional", noise_treatment = "marginalize", lapse_treatment = "sample") %>%
    filter(category == categories.PR[1]) %>%
    pull(response)
}

# Get acoustic locations that correspond to targeted response proportions.
get_locations_along_phonetic_contrast <- function(ideal_observer, target_probabilities = .5) {
  locations <- 
    map2(
      seq(ideal_observer$mu[[1]][1], ideal_observer$mu[[2]][1], length.out = 51),
      seq(ideal_observer$mu[[1]][2], ideal_observer$mu[[2]][2], length.out = 51),
  ~ c(.x, .y))
  
  location_selected <- list()
  for (t in target_probabilities) {
   # For each location, get distance from the desired shift for that category
    probability_of_category1_in_its_lexical_context_at_location <- abs(t - get_experimenter_posterior_of_category1(locations, ideal_observer))

    location_selected[[as.character(t)]] <- locations[[which(probability_of_category1_in_its_lexical_context_at_location ==
                                                              min(probability_of_category1_in_its_lexical_context_at_location))]]
  }
  
  if (length(target_probabilities) == 1) location_selected <- location_selected[[as.character(target_probabilities)]]
  
  return(location_selected)
}

# make the design of an exposure phase
make_perceptual_recalibration_exposure_design = function(
  # Ideal observer describing the true perceptual system for two categories.
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,
  # How far is the shifted mean shifted towards the other category?
  exposure.token_shift = .5,  
  # What's the prior for the category when it's lexically labeled?
  exposure.category_prior_of_lexical_context = my_experimenter.lexical_bias_strength, 
  exposure.tokens.typical.n = n.exposure.token,
  exposure.tokens.shifted.n = n.exposure.token,
  quiet = F
) {
  if (!quiet) message(
    paste0("Making exposure design with ", exposure.tokens.typical.n, " typical tokens and ", exposure.tokens.shifted.n, " shifted tokens. Experimenters are assumed to have a lapse rate of 0, and the category prior is assumed to reflect a lexical bias of ", exposure.category_prior_of_lexical_context, ". For now, typical tokens are taken to be always the mean of the category, and shifted tokens always fall exactly on the point of maximal ambiguity."))
 
  location.shifted_category1 <- get_locations_along_phonetic_contrast(
    experimenter.ideal_observer %>% mutate(prior = c(exposure.category_prior_of_lexical_context, 1 - exposure.category_prior_of_lexical_context)),
    exposure.token_shift)
  
  location.shifted_category2 <- get_locations_along_phonetic_contrast(
    experimenter.ideal_observer %>% mutate(prior = c(1 - exposure.category_prior_of_lexical_context, exposure.category_prior_of_lexical_context)),
    exposure.token_shift)

  # Create basic tibble
  tibble(
    Phase = "exposure",
    ItemID = as.character(1:(exposure.tokens.typical.n + exposure.tokens.shifted.n)),
    Item.Type = factor(c(rep("typical", exposure.tokens.typical.n), rep("shifted", exposure.tokens.shifted.n)))
  ) %>% 
    # Add all unique design combinations
    crossing(
      Item.Category = factor(experimenter.ideal_observer$category)) %>%
    # Get cue value of item: if shifted then set to shifted location. If not sample from typical distrbibution
    mutate(
      Condition = factor(ifelse(
        (Item.Category ==  experimenter.ideal_observer$category[[1]] & Item.Type == "typical") | (Item.Category ==  experimenter.ideal_observer$category[[2]] & Item.Type == "shifted"),
        paste0(experimenter.ideal_observer$category[[2]], "-shifted"),
        paste0(experimenter.ideal_observer$category[[1]], "-shifted"))),
      x = map2(
        Item.Type,
        Item.Category,
        ~ case_when(
          .x == "shifted" & .y == experimenter.ideal_observer$category[[1]] ~ 
            rmvnorm(1, location.shifted_category1, experimenter.ideal_observer$Sigma[[1]] / experimenter.variability_reduction),
          .x == "shifted" & .y == experimenter.ideal_observer$category[[2]] ~ 
            rmvnorm(1, location.shifted_category2, experimenter.ideal_observer$Sigma[[2]] / experimenter.variability_reduction),
          .x == "typical" & .y == experimenter.ideal_observer$category[[1]] ~ 
            rmvnorm(1, experimenter.ideal_observer$mu[[1]], experimenter.ideal_observer$Sigma[[1]] / experimenter.variability_reduction),
          .x == "typical" & .y == experimenter.ideal_observer$category[[2]] ~ 
            rmvnorm(1, experimenter.ideal_observer$mu[[2]], experimenter.ideal_observer$Sigma[[2]] / experimenter.variability_reduction),
          T ~ NA_real_))) %>%
  mutate(VOT = map(x, ~ .x[1]) %>% unlist(), f0_Mel = map(x, ~ .x[2]) %>% unlist())
}
```

```{r study-PR-exposure-make-data, message=FALSE}
d.PR.exposure <- make_perceptual_recalibration_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.PR) 
```

```{r study-PR-test-functions}
# make the design of an test phase
make_perceptual_recalibration_test_design = function(
  d,
  experimenter.ideal_observer,
  test.n_block,     # Number of test blocks (total)
  test.locations
) {
  # get test locations under assumption that prior is 50/50, not matter what the ideal observer says.
  # since this is a reasonable assumption for categorization during test
  locations <- get_locations_along_phonetic_contrast(
   ideal_observer = experimenter.ideal_observer %>% mutate(prior = c(.5, .5)),
    target_probabilities = test.locations)
  
  tibble(
    Phase = "test",
    ItemID = as.numeric(factor(names(locations), levels = names(locations))),
    Item.Intended_proportion_category1 = names(locations),
    Item.Type = "test",
    Item.Category = NA,
    x = locations) %>%
  mutate(VOT = map(x, ~ .x[1]) %>% unlist(), f0_Mel = map(x, ~ .x[2]) %>% unlist()) %>%
  crossing(  
    Condition = factor(paste0(experimenter.ideal_observer$category, "-shifted")), 
    Block = 1:test.n_block)
} 
```

```{r study-PR-test-make-data}
d.PR.test <- make_perceptual_recalibration_test_design(d.PR.exposure, experimenter.ideal_observer = m.io.VOT_f0.PR, test.n_block = n.test_block, test.locations = test_locations) 
```

Figure \@ref(fig:PR-exposure-test-plot) shows the stimuli for the exposure and test phases of the experiment. During `r conditions.PR[1]` exposure, listeners hear ```r n.exposure.token``` lexically-labeled word recordings containing `r categories.PR[1]` shifted towards `r categories.PR[2]` and ```r n.exposure.token``` lexically-labeled word recordings containing typical `r categories.PR[2]`, mixed in with word and non-word fillers for a total of 200 recordings. As is typical for perceptual recalibration experiments, these fillers are assumed not to contain any information about the VOT distributions of `r paste(categories.PR, collapse = " and ")`, and thus do not affect listeners' beliefs about those distributions. During `r conditions.PR[2]` exposure, participants hear lexically-labeled ```r n.exposure.token``` word recordings containing `r categories.PR[2]` shifted toward `r categories.PR[1]` and ```r n.exposure.token``` lexically-labeled word recordings containing typical `r categories.PR[1]`. Shifted recordings are selected to be perceptually half-way between `r paste(categories.PR, collapse = " and ")`. 

<!-- Readers familiar with perceptual recalibration experiments might find Figure \@ref(fig:PR-exposure-test-plot)A puzzling at first glance: the conventional way of visualizing the results of perceptual recalibration experiments wrongly suggests that the targeted phonetic contrast falls along a *single* acoustic continuum (see, e.g., Figure \@ref(fig:kraljic-samuel-2007-replotted)). This is, however, rather unlikely to be the case: phonetic contrasts tend to vary along multiple phonetic dimensions [e.g., @flege1992; @idemaru-holt2020; for a particular powerful demonstration, see @mcmurray-jongman2011]. For the phonetic contrast at hand, VOT is known to be the primary cue to syllable-initial stop voicing in L1 US English, but f0 and other cues also affect listeners' categorization responses [e.g., @burchill-jaeger2022; @idemaru-holt2011; @schertz2015; @toscano-mcmurray2012]. Although not strikingly evident in Figure \@ref(fig:PR-exposure-test-plot)A, this also applies to the `r paste(categories.PR, collapse = "-")` contrast: `r paste(categories.PR, collapse = " and ")` vary along both VOT and f0 among other cues [e.g., @chodroff-wilson2018; @kirby2020; @schertz2015]. -->

(ref:PR-exposure-test-plot) Distribution of the exposure and test of the perceptual recalibration experiment. **Panel A - Exposure:** `r n.exposure.token` tokens each of shifted and typical `r paste(categories.PR, collapse = " and ")`, respectively. Arrows indicate how the shifted tokens created by the experimenter differ from typical tokens of the same category (arrows originate from the mean of typical tokens and end at the mean of shifted tokens). As would be expected given the procedures researchers typically employ to generate stimuli for this type of experiment, the exposure distributions for `r paste(categories.PR, collapse = " and ")` in both conditions differ primarily along VOT but, notably, *also* differ along f0. **Panel B - Test (identical for both exposure groups):** Numbers indicate the item ID of the test tokens used in subsequent plots, with IDs in increasing order corresponding to `r paste(paste0(as.numeric(unique(d.PR.test$Item.Intended_proportion_category1)) * 100, "%"), collapse = ", ")` expected `r categories.PR[1]`-responses *prior to exposure* (e.g., in a test-only norming experiment). 

```{r PR-exposure-test-plot, fig.width= base.width * 3 + 1, fig.height= base.height, fig.cap="(ref:PR-exposure-test-plot)"}
p.PR.exposure <- d.PR.exposure %>%
  ggplot(aes(x = VOT, y = f0_Mel, color = Item.Category)) +
  geom_point(aes(shape = Item.Type), alpha = .9) +
  geom_segment(
    data = . %>%
      mutate(
        Condition = case_when(
          Item.Type == "typical" & Condition == conditions.PR[1] ~ conditions.PR[2], 
          Item.Type == "typical" & Condition == conditions.PR[2] ~ conditions.PR[1],
          T ~ as.character(Condition))) %>%
      group_by(Condition, Item.Type) %>%
      summarise(across(c(VOT, f0_Mel), mean)) %>% 
      pivot_wider(names_from = Item.Type, values_from = c(VOT, f0_Mel)) %>%
      mutate(Item.Category = ifelse(Condition == conditions.PR[1], categories.PR[1], categories.PR[2])),
    aes(x = VOT_typical, xend = VOT_shifted, y = f0_Mel_typical, yend = f0_Mel_shifted),
    arrow = arrow(type = "closed", angle = "24", length = unit(5, "points")),
    alpha = .4, 
    show.legend = F) +
  scale_shape_manual("Item type", breaks = c("typical", "shifted"), values = shapes.category) +
  scale_color_manual("Category", breaks = categories.PR, values = colors.category) +
  scale_x_continuous(expression("VOT (ms)"), expand = expansion(mult = .1, add = 0)) +
  scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0),
                     breaks=seq(180, 300, by = 20)) +
  coord_cartesian(ylim = c(180, 300)) +
  facet_grid(~ paste("Exposure:", Condition)) 

limits <- get_plot_limits(p.PR.exposure)
breaks <- get_plot_breaks(p.PR.exposure)

p.PR.test <-
  d.PR.test %>%
  filter(Condition == conditions.PR[1]) %>%
  mutate(Condition = "Test") %>%
  ggplot(aes(x = VOT, y = f0_Mel, label = as.numeric(factor(ItemID)))) + 
  geom_line(color = "gray", alpha = .5) +
  geom_text(aes(color = as.numeric(as.character(Item.Intended_proportion_category1))), size = 3) +
  scale_x_continuous(expression("VOT (ms)"), expand = expansion(mult = .1, add = 0),
                     breaks = c(breaks$xbreaks), limits = c(limits$xmin, limits$xmax)) +
  scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0),
                     breaks=c(breaks$ybreaks), limits = c(limits$ymin, limits$ymax)) +
  coord_cartesian(ylim = c(180, 300)) +
  scale_shape_discrete("Item type") +
  scale_color_gradient("", low = colors.category[2], high = colors.category[1], guide = NULL) +
  facet_grid(~ Condition)

plot_grid(
  plotlist = list(p.PR.exposure, p.PR.test),
  align = "hv",
  axis = "btlr",
  labels = c("A)", "B)"),
  rel_widths = c(.6, .4))
```

```{r}
d.PR.exposure %<>%
  add_subjects_to_exposure(n.subject = n.subject)
```

### Test phase
```{r study-PR-test}
d.PR.test %<>%
  add_subjects_to_test(n.subject = n.subject)
```

The test phase consists of the six stimuli shown in Figure \@ref(fig:PR-exposure-test-plot)B, chosen to yield `r paste(paste0(as.numeric(rev(unique(d.PR.test$Item.Intended_proportion_category1))) * 100, "%"), collapse = ", ")` expected `r categories.PR[1]`-responses in a norming experiment without any prior exposure to the talker's speech. This closely resembles the test stimulus placement typical for perceptual recalibration experiments, which tend to examine tokens at the two ends of the continuum as well as those placed near the category boundary. 

## Results
Next, we use ASP to illustrate the extent to which changes in representations, response biases, or normalization can explain the typical behavioral signature of perceptual recalibration experiments: a shift in the category boundary in the same direction as the shifted tokens during exposure (optionally accompanied by a change in the slope of the categorization functions). The goal of these computational demonstrations is to illustrate the general type of pattern that the three different hypotheses can account for. 

All three change models start from the same prior state. Specifically, we assume that listeners have acquired category representations with expected category means ($\mathbf{E}(\mu_{/d/}), \mathbf{E}(\mu_{/t/})$) and variances ($\mathbf{E}(\sigma_{/d/}), \mathbf{E}(\sigma_{/t/})$) that reflect the talker-normalized distributions in Chodroff and Wilson (2018; see Figure \@ref(fig:show-representations-plots)). Prior to any exposure, decision-making is assumed to employ uniform response biases ($\pi_{/d/}=\pi_{/t/}=.5$). Here and throughout this article, we marginalize over the expected consequences of perceptual noise, attentional lapses, and decision-making (both for adaptation during exposure and for categorization during test). This can be seen as predicting the state of an 'average' listener after exposure to a specific set of stimuli.^[For any particular experiment, however, researcher have to *estimate*, e.g., participants' categorization functions from their categorization responses. This is important to keep in mind, for example, when using ASP to estimate the statistical power of an experiment. To support predictive power simulations, the ASP functions provided on OSF also allow simulations that *sample* the effects of perceptual noise, attentional lapses, and decision-making for adaptation and categorization. This allows simulations of trial-level behavior for individual participants.] The findings we present below do not qualitatively depend on these specific assumptions. 

Finally, we note that our simulations predict categorization at the *beginning* of the test phase, prior to repeated testing. This should be kept in mind when comparing our results against previous work, including the results of @kraljic-samuel2007 in Figure \@ref(fig:kraljic-samuel-2007-replotted). As is typical for experiments on perceptual recalibration, participants in Kraljic and Samuel's experiment heard the same six test tokens many times. In their visualizations and analyses, Kraljic and Samuel averaged participants' responses across these repetitions. While this approach continues to be the norm, it is now known to underestimate the true boundary shift, as repeated testing *reduces* the effects of exposure [@mitterer2011-perceptual-recalibration; @liu-jaeger2018; @liu-jaeger2019; @scharenborg-janse2013; @zheng-samuel2023; for early discussion, see @norris2003, p. 11].

### Changes in representations
We begin by modeling exposure-driven changes in the mapping from VOT and f0 to the /d/ and /t/ categories. This is arguably the mechanism that is most commonly assumed to underlie the boundary shift observed in perceptual recalibration experiments. Specifically, we use the $\mathcal{NW}^{-1}$ ideal adaptor model described in Section \@ref(sec:ideal-adaptor) while varying the $\kappa_{c,0}$ and $\nu_{c,0}$ parameters. We set the other two parameters of the $\mathcal{NW}^{-1}$ ideal adaptor ($\mathrm{m}$ and $\mathrm{S}$) so that they match the expected mean and covariance of the data in @chodroff-wilson2018 (as we did in Section \@ref(sec:ideal-adaptor)). Normalization and response biases did not change based on exposure ($\kappa_0 = \infty$; $\beta_{\pi}=0$), and attentional lapses were assumed to be zero ($\lambda = 0$).

```{r study-PR-models-changes-in-representations}
d.PR <- d.PR.exposure %>%
  add_prior_and_get_posterior_beliefs_based_on_exposure(prior = m.ia.VOT_f0.PR) %>%
  add_test_tokens(d.PR.test) %>%
  add_categorization_functions() %>%
  group_by(Condition, Subject, prior_kappa, prior_nu) %>%
  add_categorization() %>%
  ungroup() %>%
  mutate_at(
    vars(starts_with("prior_")),
    ~ factor(as.character(.x), levels = as.character(rev(sort(unique(.x))))))
```

Figure \@ref(fig:PR-result-changes-in-representations) shows the predicted categorization functions after `r paste(conditions.PR, collapse = " and ")` exposure, depending on the strength of the prior beliefs for the category means and variances. We show the results for $\kappa_{c,0}$s and $\nu_{c,0}$s ranging from 1024 (very slow learning) to 4 (very fast learning). These values are best understood in the context of the number of critical exposure stimuli. Given 20 exposure tokens for each category, a $\kappa_{c,0}=20$ would mean that the listeners' beliefs about the distribution of the category mean after exposure is a 50/50 mix of their beliefs prior to exposure and the mean of the exposure stimuli (see Equation \@ref(eq:niw-updating)). For $\kappa_{c,0}=4$, listeners' beliefs about the category mean are thus primarily determined by the mean of the exposure stimuli, whereas a $\kappa_{c,0}=1024$ essentially means that exposure does not affect listeners' belief about the category mean at all (and mutatis mutandis, for $\nu_{c,0}$). 

(ref:PR-result-changes-in-representations) Predictions of a learning model that derives perceptual recalibration as changes in category representations. Predicted categorization responses are shown for the `r length(test_locations)` test tokens after `r paste(categories.PR, collapse = "- and")`-shifted exposure, depending on the strength of the prior beliefs in categories means ($\kappa_{c,0}$) and covariances ($\nu_{c,0}$). Smaller $\kappa_{c,0}$ and $\nu_{c,0}$ indicate *faster* learning, weighting previous long-term experience less during the integration with the observations made during the exposure phase of the experiment (see Equation \@ref(eq:niw-updating)). The highlighted panel corresponds to the best-fitting $\kappa_{c,0}$ and $\nu_{c,0}$ observed in previous work within other types of paradigms [@kleinschmidt-jaeger2016cogsci; @kleinschmidt2020].

```{r PR-result-changes-in-representations, fig.width= base.width * 5, fig.height= base.height * 5 +.5, fig.cap="(ref:PR-result-changes-in-representations)"}
p.PR.results <-
  d.PR %>%
  filter(category == categories.PR[1]) %>%
  ggplot(aes(x = observationID, y = response, color = Condition, l = Condition)) +
  geom_line(aes(group = Condition), alpha = .75) +
  # , breaks = 1:length(unique(d.PR$observationID)), labels = c(categories.PR[1], rep("", length(unique(d.PR$observationID)) - 2), categories.PR[2])
  scale_x_continuous(name = "Test token") + 
  scale_y_continuous(paste0('Proportion ', categories.PR[1], '-responses'), limits = c(0, 1)) +
  scale_linetype_manual("Exposure condition", breaks = conditions.PR, values = linetypes.condition) +
  scale_color_manual("Exposure condition", breaks = conditions.PR, values = colors.condition) +
  facet_grid(
    prior_nu ~ prior_kappa, 
    labeller = label_bquote(
      cols = {kappa[.(categories.PR[1])~","~0] == kappa[.(categories.PR[2])~","~0]} == .(as.character(prior_kappa)), 
      rows = {nu[.(categories.PR[1])~","~0] == nu[.(categories.PR[2])~","~0]} == .(as.character(prior_nu)))) + # as.table = T doesn't seem to work
  theme(legend.position = "top") +
  myGplot.defaults(base_size = 14, set_theme = F)

p.PR.results +
  insert_layers(
    geom_rect(
      data = . %>%
        distinct(prior_kappa, prior_nu) %>%
        mutate(highlight = case_when(
          between(as.numeric(as.character(prior_kappa)), 75, 780) & between(as.numeric(as.character(prior_nu)), 160, 1000) ~ T,
          T ~ F)), 
      aes(fill = highlight), alpha = .2, inherit.aes = F,
      xmin = -Inf,xmax = Inf, ymin = -Inf,ymax = Inf)) +
  scale_fill_manual(breaks = c(T, F), values = c("gray20", "white")) +
  guides(fill = "none")

d.PR.representations.highlighted <- d.PR %>%
  filter(category == categories.PR[1]) %>%
        mutate(highlight = case_when(
          between(as.numeric(as.character(prior_kappa)), 75, 780) & between(as.numeric(as.character(prior_nu)), 160, 1000) ~ T,
          T ~ F)) %>%
  filter(highlight == T)

p.PR.representations.highlighted <- p.PR.results %+%
  d.PR.representations.highlighted
```

Figure \@ref(fig:PR-result-changes-in-representations) demonstrates that distributional learning can account for the typical result of perceptual recalibration experiments. Of note, the range of parameterizations that best fit human responses in a series of distributional learning experiments on /b/ and /p/ [gray panel in Figure \@ref(fig:PR-result-changes-in-representations), $\kappa_{/b/,0}=\kappa_{/p/,0}=160$ (95% CI: 75-780) and $\nu_{/b/,0}=\nu_{/p/,0}=510$ (95% CI: 160-1000), @kleinschmidt2020] also provides a good qualitative fit against the /d/-/t/ perceptual recalibration results by Kraljic and Samuel [-@kraljic-samuel2007]. 

<!--As an additional reference, we note the best-fitting $\kappa_{/b/,0}=\kappa_{/p/,0}=243$ (95% CI: 163-495) and $\nu_{/b/,0}=\nu_{/p/,0}=772$ (95% CI: 493-1180) obtained by @kleinschmidt-jaeger2016cogsci were based on a series of distributional learning experiments on changes only in VOT within a paradigm similar to @clayards2008. Subsequent improvements to these analyses changed the best-fitting estimates to $\kappa_{/b/,0}=\kappa_{/p/,0}=160$ (95% CI: 75-780) and $\nu_{/b/,0}=\nu_{/p/,0}=510$ [95% CI: 160-1000, @kleinschmidt2019].-->

Figure \@ref(fig:PR-result-changes-in-representations) further shows that boundary shifts can result from either changes in the beliefs about category means (small $\kappa_{c,0}s$, e.g. right column of Figure \@ref(fig:PR-result-changes-in-representations)) or changes in the beliefs about category covariances (small $\nu_{c,0}s$, e.g., bottom row in Figure \@ref(fig:PR-result-changes-in-representations)). This replicates an observation made by Kleinschmidt and Jaeger [-@kleinschmidt-jaeger2015, p. 168; see also @hitczenko-feldman2016; @theodore-monto2019], and highlights that "category expansion" and "category shifts" can both be seen as consequences of distributional learning [see also @bent-baeseberk2021; @schertz-clare2020]. Figure \@ref(fig:PR-result-changes-in-representations-categories) serves to further clarify how the different parameter settings for $\kappa_{c,0}$ and $\nu_{c,0}s$ affect the expected category likelihoods for a subset of the panels of Figure \@ref(fig:PR-result-changes-in-representations), leading to category shifts, expansion, shrinkage, or rotation, depending on both the model parameters and the input during exposure.

<!--In fact, for the specific experiment simulated here, changes in the beliefs about category covariances lead to comparatively larger boundary shifts. This is the result of the experimenter selecting 'typical' stimuli as the starting point for the phonetic manipulations. In our simulation, we emulated this by reducing the variance of the stimuli by a constant factor, simulating a process by which the experimenter discards 'atypical' recordings. Leaving these specifics aside, this demonstrates that terms like "category expansion" and "boundary shifts" are best understood as *descriptions* of results, rather than hypotheses about the underlying *mechanisms*. <!--[for related discussion, see @schertz-clare2020-ORNOT]. The ideal adaptor and similar learning mechanisms can explain either type of result [for additional support and discussion, see also @kleinschmidt-jaeger2015, p. 168; @bent-baeseberk2021; @hitczenko-feldman2016; @theodore-monto2019].-->

(ref:PR-result-changes-in-representations-categories) Expected category likelihoods (based on expected category mean and covariance matrices) after exposure, depending on the exposure condition and the strength of the prior beliefs in categories means ($\kappa_{c,0}$) and covariances ($\nu_{c,0}$). Only a illustrative subset of the $\kappa_{c,0}$ and $\nu_{c,0}$ values from Figure \@ref(fig:PR-result-changes-in-representations) are shown. Specifically, we compare scenarios with very slow changes in representations (first column), fast changes in beliefs about the category means (leading 'category shifts', second and last column), and fast changes in beliefs about the category covariances (leading to 'category expansions', 'category shrinkage', and/or 'category rotation', third and last column).

```{r PR-result-changes-in-representations-categories, fig.width= base.width * 4, fig.height= base.height * 2 +.5, fig.cap="(ref:PR-result-changes-in-representations-categories)"}
levels <- c(.68, .95)

plot_expected_categories_contour2D(
  x = d.PR %>% 
    distinct(Condition, Subject, prior_kappa, prior_nu, posterior) %>%
    filter(prior_kappa %in% c(4, 1024), prior_nu %in% c(4, 1024)) %>%
    unnest(posterior), 
  levels = levels, 
  facet_rows_by = prior_nu, facet_cols_by = prior_kappa) +
  scale_x_continuous("VOT (msec)") +
  scale_y_continuous("f0 (Mel)") +
  scale_fill_manual("Category", breaks = categories.PR, values = colors.category) +
  scale_alpha_continuous("Cumulative probability", range = c(.1, .5), breaks = 1 - levels, labels = round(levels, 2)) +
  facet_grid(
    Condition ~ prior_nu + prior_kappa, 
    labeller = label_bquote(
      cols = atop({kappa[0~","~ .(categories.PR[1])] == kappa[0~","~ .(categories.PR[2])]} == .(as.character(prior_kappa)), 
                  {nu[0~","~ .(categories.PR[1])] == nu[0~","~ .(categories.PR[2])]} == .(as.character(prior_nu))))) +
  theme(legend.position = "top", panel.spacing = unit(1, "lines"))

# plot_expected_categories_contour2D(
#   x = d.PR %>% 
#     distinct(Condition, Subject, prior_kappa, prior_nu, posterior) %>%
#     filter(Condition == conditions.PR[2]) %>%
#     unnest(posterior), 
#   levels = levels, 
#   facet_rows_by = prior_nu, facet_cols_by = prior_kappa) +
#   scale_fill_manual("Exposure condition", breaks = categories.PR, values = colors.category) +
#   scale_alpha_continuous("Cumulative probability", range = c(0, .3), breaks = round(levels, 2)) +
#   facet_grid(
#     prior_nu ~ prior_kappa, 
#     labeller = label_bquote(
#       cols = {kappa[0~","~ .(categories.PR[1])] == kappa[0~","~ .(categories.PR[2])]} == .(as.character(prior_kappa)), 
#       rows = {nu[0~","~ .(categories.PR[1])] == nu[0~","~ .(categories.PR[2])]} == .(as.character(prior_nu)))) +
#   theme(legend.position = "top")
```

Finally, Figure \@ref(fig:PR-result-changes-in-representations) shows that distributional learning not only predicts shifts in the category boundary but can also predict changes in the slope of those boundaries. This is evident, for example, in the top-right panel. Although such changes in slopes are often not discussed, they are present in many perceptual recalibration experiments [e.g., @drouin2016; @liu-jaeger2018; @liu-jaeger2019; @myers-mesite2014].

### Changes in decision-making
Next, we model the effects of changes in decision-making. Such changes have rarely been considered as an explanation for boundary shifts [but see @clarkedavidson2008]. In addition to varying the rate at which response biases change $\beta_{\pi}$, we also vary the lapse rate parameter $\lambda$. Normalization and representations did not change based on exposure ($\kappa_0 = \kappa_{c,0} = \nu_{c,0} = \infty$). Since the change model for decision-making is sensitive to the order of stimuli during exposure (see Section \@ref(sec:change-bias)), each panel of the figure averages over 50 simulations, each of them randomizing the order of exposure. We confirmed that this number of simulations was more than sufficient to achieve reproducible estimates for the range of $\beta_{\pi}$s considered here (the order-sensitivity of the change model increases for faster change rates, i.e., larger $\beta_{\pi}$).

(ref:PR-result-changes-in-decision-making) Predictions of a model that derives perceptual recalibration as changes in decision-making. Predicted categorization responses are shown for the `r length(test_locations)` test locations after `r paste(categories.PR, collapse = "- and")`-shifted exposure, depending on the rate at which response biases change ($\beta_{\pi}$) and the rate of attentional lapses ($\lambda$). The response biases towards `r categories.PR[1]` that results from $\beta_{\pi}$ are shown in the bottom row for each of the two exposure conditions, and are identical within each column (when all trials are lapses, as in the bottom row, the response distribution is identical to the response biases). 

```{r model-PR-changes-in-decision-making, echo=FALSE, message=FALSE, warning=FALSE}
simulation.control <-
  list(
    min.simulations = 2, 
    max.simulations = 50, 
    step.simulations = 1, 
    target_accuracy_se = .002)

if(SET_SEED) set.seed(42007)

if (RESET_MODELS || !file.exists(get_path(paste0("../models/d.PR.results.bias.rds")))){
  d.PR <- d.PR.exposure %>%
  nest(data = -c(Condition, Subject)) %>%
  crossing(
    posterior.lapse_rate = c(.0005, .005, .05, .5, 1),
    beta_pi = c(0, .01, .05, .1, .2, .8)) %>% 
  crossing(
    m.ia.VOT_f0.PR %>%
      filter(prior_kappa == max(prior_kappa), prior_nu == max(prior_nu)) %>%
      nest(prior = everything())) %>%
  group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
  group_modify(
    ~ update_bias_and_categorize_test(
      prior = .x$prior[[1]],
      lapse_rate = .x$posterior.lapse_rate,
      beta_pi = .x$beta_pi,
      exposure = .x$data[[1]],
      test = d.PR.test %>% mutate(Item.Intended_category = "/d/") %>% # here there is no intended category, but we would like to get the probability of /d/ response
        filter(Condition == .x$Condition),
      control = simulation.control), 
    .keep = TRUE, verbose = T) %>%
  mutate_at(vars(starts_with("prior_")), ~ factor(.x)) %>%
  mutate_at(vars(starts_with("prior_")), fct_rev) %>%
  ungroup() %>%
  filter(category == categories.PR[1]) %>%
  mutate(
   posterior.lapse_bias1 = unlist(map(posterior, ~ .x %>% filter(category == categories.PR[1]) %>% pull(lapse_bias))),
   posterior.lapse_bias2 = unlist(map(posterior, ~ .x %>% filter(category == categories.PR[2]) %>% pull(lapse_bias)))) %>%
  ungroup() %>%
  group_by(Condition, Subject, posterior.lapse_rate, beta_pi, observationID, x, category) %>%
  summarise(
    response.n_sims = n_distinct(sim),
    response.se = sd(response) / sqrt(response.n_sims),
    response = mean(response),
    posterior.lapse_bias1.se = sd(posterior.lapse_bias1) / sqrt(response.n_sims),
    posterior.lapse_bias2.se = sd(posterior.lapse_bias2) / sqrt(response.n_sims),
    posterior.lapse_bias1 = mean(posterior.lapse_bias1),
    posterior.lapse_bias2 = mean(posterior.lapse_bias2)
  ) %>%
  relocate(
   !!! syms(setdiff(names(.), c("response", "response.se", "response.n_sims"))),
   response, response.se, response.n_sims) %>%
  ungroup()
  
  saveRDS(d.PR, get_path(paste0("../models/d.PR.results.bias.rds"))) # save results for bias because it takes a long time to run
} else {
  d.PR <- readRDS(get_path(paste0("../models/d.PR.results.bias.rds")))
}

```


```{r PR-result-changes-in-decision-making, fig.width= base.width * 5, fig.height= base.height * 5 +.5, fig.cap="(ref:PR-result-changes-in-decision-making)"}
p.PR.results.bias <- p.PR.results %+% 
  d.PR +
  geom_text(
    data = . %>%
      group_by(Condition, posterior.lapse_rate, beta_pi) %>%
      summarise(
        minObservationID = min(observationID),
        maxObservationID = max(observationID),
        lapse_bias_expression = list(bquote(pi[.(categories.PR[1])] == .(first(posterior.lapse_bias1))))) %>%
      ungroup() %>%
      filter(posterior.lapse_rate == max(posterior.lapse_rate)),
    mapping = aes(
      x = ifelse(Condition == conditions.PR[2], minObservationID, maxObservationID),
      y = ifelse(Condition == conditions.PR[2], 0, 1),
      hjust = ifelse(Condition == conditions.PR[2], 0, 1),
      vjust = ifelse(Condition == conditions.PR[2], 0, 1),
      label = unlist(lapse_bias_expression)),
    size = 4,
    parse = T,
    show.legend = F) +
  facet_grid(
    posterior.lapse_rate ~ beta_pi, 
    labeller = label_bquote(
      cols = beta[pi] == .(beta_pi),
      rows = lambda == ~.(posterior.lapse_rate))) +
  myGplot.defaults(base_size = 14, set_theme = F)
p.PR.results.bias

d.PR.bias.highlighted <- d.PR %>%
  filter(category == categories.PR[1]) %>%
        mutate(highlight = case_when(
          beta_pi == 0.01 & posterior.lapse_rate == .0005 ~ T,
          T ~ F)) %>%
  filter(highlight == T)

p.PR.bias.highlighted <- p.PR.results.bias %+%
  d.PR.bias.highlighted

```
Figure \@ref(fig:PR-result-changes-in-decision-making) shows the effects of changes in response biases for lapse rates ranging from negligible lapse rates ($\lambda = .0005$, top row) to a scenario in which listeners always lapse ($\lambda = 1$). In the latter---rather implausible---edge case, listeners' responses only reflect response biases and are completely stimulus-independent (bottom row). For reference, lapse rates for perception experiments like the ones modeled here typically fall between 1-10% [e.g., @clayards2008 constrained lapse rates to be $<5$%; @kleinschmidt-jaeger2016cogsci report a best-fitting value of 5%].

The primary insight from Figure \@ref(fig:PR-result-changes-in-decision-making) is that changes in response biases can account for the type of boundary shift that is observed in perceptual recalibration experiments. This shows that the signature result of perceptual recalibration experiments is ambiguous between two explanations that evoke very different cognitive architectures. For this case study, perhaps the closest match to the findings from Kraljic and Samuel (2006) is observed for plausible lapse rates up to 5% and small changes in response bias (second or third column, top rows). 

Figure \@ref(fig:PR-result-changes-in-decision-making) also shows that changes in response biases do not necessarily result in effects that are symmetric around the uniform response biases assumed to hold prior to the experiment ($\pi_{/d/}=\pi_{/t/}=.5$). For example for $\beta_{\pi} = .8$, the /d/-shifted condition results in a /d/-bias of $\pi_{/d/}=.97$, whereas /t/-shifted condition results in $\pi_{/d/}=.16$ ($\neq 1 - .97$). This type of asymmetry is a consequence of the assumption that listeners change their response biases only to the extent that their categorization is in conflict with the category label indicated in the input (e.g., when the listener hears a /t/ but the lexical context is *lemona_e*). This means that the degree to which listeners change their response biases depends on the degree to which the acoustic properties of the exposure stimuli are in conflict with the lexical contexts in which they appear. 

Furthermore, Figure \@ref(fig:PR-result-changes-in-decision-making) illustrates the effects of lapse rates. The higher the lapse rate, the more strongly response biases, rather than stimulus-dependent aspects, determine the overall response pattern. This also means that it is important to keep lapse rates in mind when analyzing behavioral data. Consider, for example, a scenario in which listeners' lapse rates differ after `r paste(conditions.PR, collapse = " and ")` exposure---e.g., because one exposure condition resulted in a more difficult or less engaging task, or in less plausible stimuli. Compare, say, the blue line for $\beta_{\pi} = .05$, $\lambda = .005$ against the red line for $\beta_{\pi} = .05$, $\lambda = .5$. Analyses that fail to take into account the difference in lapse rates would wrongly conclude that the category representations have changed since the slopes of the categorization functions resulting from the two exposure conditions seem to differ.

### Changes in normalization
Finally, we compare a model that normalizes test tokens based on the phonetic inputs experienced during exposure against a model that continues to apply normalization based on previous long-term experience. Essentially, the model that changes normalization based on the exposure tokens subtracts the cue means experienced during exposure---which varies based on the exposure condition---from that of each of the test tokens. We implement this by adjusting the cue values of the test tokens by the difference between the prior mean of cues based on previously experienced input (estimated in the SI, \@ref(sec:SI-chodroff)) and the mean of cues experienced during exposure. All other parameters have the same setting as in the previous sections, except that no changes in representations or response biases are considered. 

To illustrate the consequences of normalization, we vary the only parameter of our change model for normalization, $\kappa_0$ (i.e., strength of prior beliefs about cue mean). Figure \@ref(fig:PR-test-normalization) illustrates the effect of normalization on the cue values of the test tokens. Figure \@ref(fig:PR-result-changes-in-normalization) shows the results of different speed in the changes of normalization. Paralleling the results for changes in representations and response biases, changes in normalization can account for the signature boundary shift of perceptual recalibration experiments.

(ref:PR-test-normalization) Effects of changes in normalization on the perception of test locations, depending on the relative weighting of previous experience ($\kappa_0$) during the inference of the cue mean during exposure. Normalization based on exposure results in a shift in the perception of the test stimuli. The magnitude of that shift depends on $\kappa_0$, with larger shifts (more learning) for smaller $\kappa_0$ (see Equation \@ref(eq:normalization-change)). 

```{r PR-test-normalization, fig.width= base.width * 2, fig.height= base.height * (if (ANIMATE_FIGURES) 1 else 3) + (if (ANIMATE_FIGURES) 1 else 0), fig.cap="(ref:PR-test-normalization)", fig.show= if (ANIMATE_FIGURES) 'animate' else 'asis'}
d.PR <- d.PR.exposure %>%
  add_prior_and_normalize_test_tokens_based_on_exposure(data.test = d.PR.test, prior.normalization = prior_marginal_VOT_f0_stats, prior.categories = m.ia.VOT_f0.PR) %>%
  group_by(Condition, Subject, prior_kappa.normalization) %>%
  add_categorization_functions() %>%
  add_categorization() %>%
  mutate(Prior_proportion_category1 = map(x, ~ get_experimenter_posterior_of_category1(matrix(.x, ncol = 2), m.io.VOT_f0.PR)) %>% unlist()) %>%
  ungroup() %>%
  mutate(
    prior_kappa.normalization = factor(as.character(prior_kappa.normalization), levels = as.character(rev(sort(unique(prior_kappa.normalization))))))

limits <- get_plot_limits(p.PR.exposure)
if (ANIMATE_FIGURES) {
  p.animation <- d.PR %>%
    mutate(VOT = map(x, ~ .x[1]) %>% unlist(), f0_Mel = map(x, ~ .x[2]) %>% unlist()) %>%
    group_by(Condition, Subject, prior_kappa.normalization) %>%
    ggplot(aes(x = VOT, y = f0_Mel, label = as.numeric(factor(observationID)))) + 
    geom_line(color = "gray", alpha = .5) +
    scale_x_continuous(expression("VOT (msec)"), expand = expansion(mult = .1, add = 0)) +
    scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0)) +
    scale_shape_discrete("Item type") +
    scale_color_gradient("", low = colors.category[2], high = colors.category[1], guide = NULL) +
    coord_cartesian(xlim = c(limits$xmin, limits$xmax), ylim = c(limits$ymin, limits$ymax)) +
    facet_grid(. ~ Condition) +
    # ease_aes('linear') +
    # enter_fade() + exit_fade() +
    transition_states(prior_kappa.normalization) +
    ggtitle(label = expression(kappa[0]), subtitle = "{closest_state}")
  p.animation + geom_text(aes(color = Prior_proportion_category1), size = 3)
  if (SAVE_ANIMATED_VIDEO) {
    anim_save("demonstrate-PR-test-normalization.webm", animation = animate(p.animation +                                                                 geom_text(aes(color = Prior_proportion_category1), size = 10) + theme(
      plot.title = element_text(size = 20),
      strip.text.x = element_text(size = 20),
                           axis.text.x = element_text(size = 20),
                           axis.title.x = element_text(size = 20),
                           axis.text.y = element_text(size = 20),
                           axis.title.y = element_text(size = 20),
                           legend.text = element_text(size = 20),
                           legend.title = element_text(size = 20)), renderer = av_renderer(), width = 1280, height = 720), path = '../figures/knitted/')
  }
} else {
  d.PR %>%
    filter(prior_kappa.normalization %in% c(4, 64, 1024)) %>%
    mutate(VOT = map(x, ~ .x[1]) %>% unlist(), f0_Mel = map(x, ~ .x[2]) %>% unlist()) %>%
    group_by(Condition, Subject, prior_kappa.normalization) %>%
    ggplot(aes(x = VOT, y = f0_Mel, label = as.numeric(factor(observationID)))) +
    geom_line(color = "gray", alpha = .5) +
    geom_text(aes(color = Prior_proportion_category1), size = 3) +
    scale_x_continuous(expression("VOT (msec)"), expand = expansion(mult = .1, add = 0)) +
    scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0)) +
    scale_shape_discrete("Item type") +
    scale_color_gradient("", low = colors.category[2], high = colors.category[1], guide = NULL) +
    coord_cartesian(xlim = c(limits$xmin, limits$xmax), ylim = c(limits$ymin, limits$ymax)) +
    facet_grid(
      prior_kappa.normalization ~ Condition,
      labeller = label_bquote(
        rows = ~kappa[0] == .(as.character(prior_kappa.normalization))))
}
```

(ref:PR-result-changes-in-normalization) Predictions of a model that derives perceptual recalibration from changes in normalization. Predicted categorization responses are shown for the `r length(test_locations)` test locations after `r paste(categories.PR, collapse = "- and")`-shifted exposure, depending on the relative weighting of previous experience ($\kappa_0$) during the inference of the cue mean during exposure. Smaller $\kappa_0$ indicate faster learning. 

```{r PR-result-changes-in-normalization, fig.width= base.width * 5, fig.height= base.height + 1, fig.cap="(ref:PR-result-changes-in-normalization)"}
p.PR.results.normalization <- p.PR.results %+%
  (d.PR %>%
     filter(category == categories.PR[1])) +
  facet_grid(
    . ~ prior_kappa.normalization,
    labeller = label_bquote(
      cols = ~kappa[0] == .(as.character(prior_kappa.normalization)))) +
  myGplot.defaults(base_size = 14, set_theme = F)
p.PR.results.normalization

d.PR.normalization.highlighted <- d.PR %>%
  filter(category == categories.PR[1]) %>%
        mutate(highlight = case_when(
          prior_kappa.normalization == 1024 ~ T,
          T ~ F)) %>%
  filter(highlight == T)

p.PR.normalization.highlighted <- p.PR.results.normalization %+%
  d.PR.normalization.highlighted
```

## Summary
We applied the three change models developed in Section \@ref(sec:framework) to a (simulated) perceptual recalibration experiment. We found that all three change models can qualitatively predict the type of boundary shift characteristic of such experiments. To us, and we imagine to many other researchers, this finding is surprising. On the one hand, all three change models were designed to explain adaptive speech perception. As such, it is not surprising that all three models predict changes in the 'right' direction, compared to the absence of an adaptive response---i.e., that all models change the categorization function in a way that is meant to accommodate the acoustic properties of the exposure stimuli.^[That the predicted changes in categorization functions indeed *improve* recognition accuracy will become more evident in Case Study 2.] On the other hand, the three change mechanisms assume very different cognitive architectures, and each of the three change models is subject to different computational limitations. Specifically, the two simpler change models for normalization and decision-making involve fewer degrees of freedom, and have access to less information, compared to the change model for category representations. Recall, for example, that normalization affects cue values regardless of category membership, or that changes in decision-making can only predict additive changes in the log-odds of categorization responses (when listeners have lapse rates of 0 or 1). 

Given these differences between the models, it seemed plausible that the simpler models would have insufficient functional flexibility to account for the types of boundary shifts associated with perceptual recalibration. Case Study 1 suggests that this is not the case. At least at the level of analysis that is typically applied in these experiments, the signature result of perceptual recalibration experiments therefore does *not* constitute decisive evidence for changes in representations. Rather, boundary shifts could result from any of the three types of change mechanisms. This does not, of course, mean that perceptual recalibration experiments are *necessarily* uninformative about the mechanisms underlying adaptive speech perception. To anticipate the general discussion, if adequate approaches to data analysis are chosen, it is possible to distinguish between the three different change mechanisms (and even combinations of them). Before we discuss what would be required to achieve this goal, we present the second case study using another common experimental paradigm.
