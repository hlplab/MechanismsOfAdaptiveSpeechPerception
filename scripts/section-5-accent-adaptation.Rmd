# Case Study 2: accent adaptation {#sec:AA}
The second paradigm we consider focuses on *naturally* accented speech---e.g., dialectal [@smith2014], varietal [@shaw2018], or second language (L2) accents [@bradlow-bent2008; @eisner2013; @sidaras2009; @weil2001a]. Typically, exposure to an unfamiliar accent is compared to a control condition in which listeners are exposed to a familiar accent, most often the 'standard' variety of listeners' first language (L1). Following exposure, listeners in either group are tested on the unfamiliar accent. In an influential study, @bradlow-bent2008 had listeners transcribe a total of 160 sentences of either Mandarin-accented US English or L1-accented US English, distributed over two sessions on two separate days. In a subsequent test phase, both groups transcribed Mandarin-accented sentences. Participants who were first exposed to Mandarin-accented US English were significantly more accurate during test (over 90% accuracy compared to about 80%). This finding has since been replicated and extended [for review, see @baeseberk2020]. We now know that substantially shorter exposure can lead to similarly large improvements in accuracy [e.g., 80 sentences in a single session, about 2-5 minutes of speech, @xie2021jep], that these can persist over hours and days [@witteman2015; @xie2018lcn], and that accent adaptation can sometimes generalize across talkers of the same or similar accents [e.g., @baeseberk2013; @tzeng2016; @xie2021jep]. <!-- Facilitatory effects of exposure have also been demonstrated in tasks that tap into online processing of naturally accented speech, including cross-modal matching [@clarke-garrett2004; @xie2018jasa], phonological priming [@eisner2013; @xie2017], and visual world eye-tracking [@dahan2008; @trude2012talker].-->

As in the case of perceptual recalibration, accent adaptation is often attributed to changes in category representations [e.g., @bent-baeseberk2021; @sidaras2009; @eisner2013; @sumner2009; @tzeng2016; @xie2016jep]. This might in part be due to the intuition that the other change mechanisms are not sufficiently flexible to explain adaptation to complex differences between accents. For example, a common assumption seems to be that changes in decision/response biases can only explain *trade-offs* in accuracy: as the accuracy for one category improves, it has to inevitably decrease for all other categories. Under this trade-off assumption (which we show below to be false), changes in decision-making could not possibly explain *overall* improvements in recognition accuracy. Similarly, pre-linguistic normalization is rarely considered a plausible mechanism for accent adaptation. However, previous work has never actually put these intuitions to a test. Case Study 2 provides this test.

Compared to perceptual recalibration paradigms, experiments on accent adaptation exhibit more heterogeneity in their designs and tasks. Case Study 2 focuses on experiments in which treatment exposure employs a single talker with an unfamiliar accent, and the test phase heard by both groups of participants employs previously unheard stimuli from the same accented talker [e.g., @eisner2013; @schertz2015; @xie2017].^[The approach we take here can also be applied to cross-talker generalization---i.e., paradigms in which speech from different talkers of the same (or different) accent as the exposure talker is used during test [@baeseberk2013; @bradlow-bent2008; @sidaras2009].] For example, in an exposure-test experiment, @xie2016jep exposed two groups of L1-US English listeners to Mandarin-accented US English speech. In exposure, participants in the treatment group heard 30 words ending in /d/ (e.g., _overload_) together with 60 word fillers and 90 nonword fillers. Participants in the control group did not hear any words that contain /d/. After exposure, participants in both conditions categorized recordings of 60 minimal /d/-/t/ pairs (e.g., _seed_ vs. _seat_) spoken by the same Mandarin-accented talker. Participants who heard /d/-final words during exposure were more likely to categorize /d/-final words correctly during test, compared to the control condition. At the same time, a non-significant numerical *de*crease in accuracy was observed for /t/-final words [see also @xie2018lcn]. Related paradigms have yielded similar results for other contrasts and other L1-L2 pairs [e.g., @zheng-samuel2020; @eisner2013].

For Case Study 2, we construct a hypothetical experiment that closely follows this type of design. For the sake of continuity, we focus on the same syllable-initial /d/-/t/ contrast used in Case Study 1. Specifically, we qualitatively simulate exposure to Korean-accented L2 English, which tends to weight the two cues to /d/-/t/ considered here (VOT and f0) differently than L1-US English. Like in Case Study 1, we analyze the data following the conventions of the field. For Case Study 2, this means that we assess how exposure affects L1 listeners' subsequent categorization accuracy for L2-accented test tokens. As in Case Study 1, we ask whether any of the three change mechanisms can qualitatively explain the signature results found in @xie2016jep and other similar work. As in any experiment on accent adaptation, the results of our simulated experiment are expected to depend on the acoustic-phonetic characteristics of both the L1 and the L2 accents. In the discussion of Case Study 2, we thus consider other common differences between L1 and L2 accents, and show how these accent properties are expected to affect the three different change models.

## Data
The stimulus generation procedure is described in detail in the SI (\@ref(sec:SI-AA)).

```{r study-AA-setup, message=FALSE}
# Set plotting aesthetics
geom_text.size <- theme_get()$text$size *5/14 # change from mm to point scale; keeping the size of geom_text the same as the rest of text in ggplot
pos <- position_dodge(.6)

# Random seed for this study
set.seed(123498765)

# Which categories is this experiment about?
categories.AA <- c("/d/", "/t/")
conditions.AA <- c("L1-accented", "L2-accented")
colors.category <- colors.condition <- colors.voicing
shapes.category <- shapes.condition <- c(15, 17)
linetypes.condition <- c(1,2)

# Number of subjects
n.subject <- 1

# Number of exposure tokens for each category, shifted and typical
n.exposure.token <- 30

# arbitrarily assume that the variance of the speech stimuli in the experiment is $\frac{1}{`r my_experimenter.variability_reduction`}$th of that observed in natural productions (applying to L1-accented speech only)
my_experimenter.variability_reduction <- 1

# Number of test blocks
n.test_block <- 1
# Number of test tokens per category
n.test.token <- 60


m.io.VOT_f0.AA <-
  m.VOT_f0_MVG  %>%
  filter(category %in% categories.AA) %>%
  droplevels() %>%
  make_stop_VOTf0_ideal_observer() %>%
  arrange(category)


prior_kappa.plot = 4^(1:6) # specify what parameters to plot
prior_nu.plot = 4^(1:6) 
 
m.ia.VOT_f0.AA <-
  crossing(
    prior_kappa = prior_kappa.plot,
    prior_nu = prior_nu.plot) %>%
  rowwise() %>%
  mutate(ideal_adaptor = map2(prior_kappa, prior_nu, ~ make_stop_VOTf0_ideal_adaptor(m = m.io.VOT_f0.AA, kappa = .x, nu = .y))) %>%
  unnest(ideal_adaptor) %>%
  arrange(category)
```

### Exposure phase
Figure \@ref(fig:study-AA-exposure-test-plot)A shows the stimuli for the exposure and test phases of the experiment. In the L2-accented exposure condition, listeners hear word recordings containing L2-accented initial /d/ and /t/ (30 tokens per category). In the control condition with L1-accented exposure, listeners hear the same words but from an L1-accented talker.^[The use of L1-accented exposure, rather than L2-accented exposure without /d/, as control follows a typical design in accent adaptation studies [e.g., @bradlow-bent2008], rather than @xie2016jep. Xie and colleagues instead employed L2-accented exposure without /d/, using the same L2-accented talker as during test [see also @eisner2013]. For the present purpose, both types of control conditions lead to identical predictions (as long as the L2-accented control exposure successfully avoids conveying non-negligible amount of information about the categories considered during the test phase) since the current implementation of the three change models do not consider talker-switch costs.] For L1-accented exposure, the category likelihoods were set to match those observed in @chodroff-wilson2018, after C-CuRE normalization (i.e., the distributions shown in Figure \@ref(fig:demonstrate-normalization)B). This follows the same approach we took for the typical tokens in Case Study 1. For L2-accented exposure, /d/ and /t/ categories were created following the VOT and f0 statistics of Korean-accented US English reported in @schertz2015. The /d/ and /t/ categories in this L2 accent differ from L1-accented US English in both category means and variances. First, the VOTs of both stop categories are longer in the L2 accent, making the /d/s more similar to /t/s for native-English listeners who rely on VOT as a primary cue to voicing distinction. As a result, the recognition accuracy is expected to be lower for /d/ than for /t/ for the L1-accented exposure condition. Second, the L2-accented categories exhibit the same ordering along f0 as in L1-accented US English (lower f0 for /d/ than for /t/) but this difference along f0 is enhanced in Korean-accented US English. This makes syllable-initial stop voicing in Korean-accented US English a representative case of cue reweighting, a common difference between L2 and L1 accents that has received much attention in research on accent adaptation [e.g., @escudero2009; @kim2020; @schertz2016; see also @idemaru-holt2011; @idemaru-holt2020; @harmon2019]. 

```{r define-AA-accent-category-distribution}
# How separable the categories are in the L2 accent?
# e.g., dist.L2_category.cue1  -- The ratio of category mean difference along the primary cue in L2 to that in L1 (the primary cue does not have to be the same cue dimension across accents); 1 means the distance is equal to that in L1 accent.

# # Case 1: presented in the main text, with clear cue-weighting (e.g., word-initial stops in Korean-accented English).
dist.L2_category.cue1 <- 1
dist.L2_category.cue2 <- 0.22
shift.cue1 <- 0.5
shift.cue2 <- 0.22
var.ratio = 1 # Multiple category variance by a constant; var.ratio = 1 means no change in category variance

min_VOT = 5 # Set the minimal values for randomly generated test token cue values to make sure it is physiologically possible
min_f0_Mel = 20
example_label = "Cue_reweighting"
```

```{r study-AA-exposure-functions}
# Get acoustic locations that correspond to targeted response proportions.
get_parameters_phonetic_contrast.AA <- function(ideal.observer,
                                                category_dist_ratio1,
                                                category_dist_ratio2,
                                                shift_mean_1 = 0,
                                                shift_mean_2 = 0) {

   # native categories
   mu_native_A <- ideal.observer %>% filter(category == "/d/") %>% pull(mu) %>% .[[1]]
   Sigma_native_A <- ideal.observer %>% filter(category == "/d/") %>% pull(Sigma) %>% .[[1]]
   mu_native_B <- ideal.observer %>% filter(category == "/t/") %>% pull(mu) %>% .[[1]]
   Sigma_native_B <- ideal.observer %>% filter(category == "/t/") %>% pull(Sigma) %>% .[[1]]

   # nonnative categories
   mu_nonnative_B <- mu_native_B
   Sigma_nonnative_B <- matrix(c(Sigma_native_B[1]*var.ratio, Sigma_native_B[2], Sigma_native_B[3], Sigma_native_B[4]), nrow=2)

   # The larger the category_dist_ratio, the greater distance along that cue dimension (1 = VOT, 2 = f0) between the two categories in the L2 accent.
   # If category_dist_ratio1 = category_dist_ratio2 = 0, then the two categories overlap entirely in the L2 accent.
   # If category_dist_ratio1 = category_dist_ratio2 = 1, then /d/-/t/ will have the same distance as that in L1-accent along the primary cue dimension.

     if(example_label != "Contrast_collapse"){
       mu_nonnative_A <- c(mu_nonnative_B[1] * (1 + category_dist_ratio1 * (mu_native_A[1]- mu_native_B[1])/mu_native_B[1]),
                         mu_nonnative_B[2] * (1 + category_dist_ratio2 * (mu_native_A[1]- mu_native_B[1])/mu_native_B[1]))
  
     Omega_native_A <- cov2cor(Sigma_native_A)
     Omega_nonnative_A <- matrix(c(Omega_native_A[1], Omega_native_A[2], Omega_native_A[3], Omega_native_A[4]), nrow=2) # change omega if orientation of category dispersion changes
     Sigma_nonnative_A <- cor2cov(Omega_nonnative_A, sqrt(diag(Sigma_native_A))*var.ratio)
     }else{
       mu_nonnative_A <- mu_nonnative_B
       Sigma_nonnative_A <- Sigma_nonnative_B}
     
   # Allow a shift of nonnative category means
    shift1 <- (mu_nonnative_A[1]+ mu_nonnative_B[1])/2*shift_mean_1
    shift2 <- (mu_nonnative_A[2]+ mu_nonnative_B[2])/2*shift_mean_2
    mu_nonnative_A <- c(mu_nonnative_A[1]+ shift1,
                        mu_nonnative_A[2]+ shift2)
    mu_nonnative_B <- c(mu_nonnative_B[1]+ shift1,
                        mu_nonnative_B[2]+ shift2)
    
    tau_native_A <-  sqrt(diag(Sigma_native_A))
    Omega_native_A <- cov2cor(Sigma_native_A)
    tau_native_B <-  sqrt(diag(Sigma_native_B))
    Omega_native_B <- cov2cor(Sigma_native_B)
    tau_nonnative_A <-  sqrt(diag(Sigma_nonnative_A))
    Omega_nonnative_A <- cov2cor(Sigma_nonnative_A)
    tau_nonnative_B <-  sqrt(diag(Sigma_nonnative_B))
    Omega_nonnative_B <- cov2cor(Sigma_nonnative_B)

    # plot and check whether the two categories are simulated as intended
    d.model <- tibble(
      speech = c(rep("native", 2), rep("nonnative", 2)),
      category = rep(c("A", "B"), 2),
      mu = list(mu_native_A, mu_native_B, mu_nonnative_A, mu_nonnative_B),
      tau = list(tau_native_A, tau_native_B, tau_nonnative_A, tau_nonnative_B),
      Omega = list(Omega_native_A, Omega_native_B, Omega_nonnative_A, Omega_nonnative_B),
      Sigma = list(Sigma_native_A, Sigma_native_B, Sigma_nonnative_A, Sigma_nonnative_B)) %>%
      group_by(speech, category) %>%
      mutate(Model = list(list(
        mu = Reduce("+", mu) / length(mu),
        Sigma = Reduce("+", Sigma) / length(Sigma))))

    return(d.model)
}

# make the design of an exposure phase
make_accent_adaptation_exposure_design = function(
  # Ideal observer describing the true perceptual system for two categories.
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,

  # How separate are the two categories in L2 accent?
  category_dist_ratio1 = dist.L2_category.cue1,
  category_dist_ratio2 = dist.L2_category.cue2,
  shift_mean_1 = 0,
  shift_mean_2 = 0,
  exposure.tokens.L1.n = n.exposure.token,
  exposure.tokens.L2.n = n.exposure.token,
  quiet = F
) {
  if (!quiet) message(
    paste0("Making exposure design with ", exposure.tokens.L1.n, " L1-accented tokens and ", exposure.tokens.L2.n, " L2-accented tokens."))

  d.model.both <- 
    get_parameters_phonetic_contrast.AA(
      experimenter.ideal_observer,
      category_dist_ratio1,
      category_dist_ratio2,
      shift_mean_1,
      shift_mean_2)
  
  d.model.native <- d.model.both %>%
    filter(speech == "native") %>%
    droplevels()
  d.model.nonnative <- d.model.both %>%
    filter(speech == "nonnative") %>%
    droplevels()

  # Create basic tibble
  tibble(
    Phase = "exposure",
    ItemID = as.character(1:(exposure.tokens.L1.n + exposure.tokens.L2.n)),
    Item.Type = factor(c(rep("L1-accented", exposure.tokens.L1.n), rep("L2-accented", exposure.tokens.L2.n)))) %>%
    # Add all unique design combinations
    crossing(
      Item.Category = factor(experimenter.ideal_observer$category)) %>%
    # Get cue value of item: if L2-accented then use L2 category parameters. If not sample from L1-accented category distribution
    mutate(
      Condition = Item.Type,
      x = map2(
        Item.Type,
        Item.Category,
        ~ case_when(
          .x == "L2-accented" & .y == experimenter.ideal_observer$category[[1]] ~ rmvnorm(1, d.model.nonnative$mu[[1]], d.model.nonnative$Sigma[[1]] / experimenter.variability_reduction),
          .x == "L2-accented" & .y == experimenter.ideal_observer$category[[2]] ~ rmvnorm(1, d.model.nonnative$mu[[2]], d.model.nonnative$Sigma[[2]] / experimenter.variability_reduction),
          .x == "L1-accented" & .y == experimenter.ideal_observer$category[[1]] ~ rmvnorm(1, d.model.native$mu[[1]], d.model.native$Sigma[[1]] / experimenter.variability_reduction),
          .x == "L1-accented" & .y == experimenter.ideal_observer$category[[2]] ~ rmvnorm(1, d.model.native$mu[[2]], d.model.native$Sigma[[2]] / experimenter.variability_reduction),
          T ~ NA_real_))) %>%
    mutate(
      VOT = unlist(map(x, ~ max(min_VOT, .x[1]))), 
      f0_Mel = unlist(map(x, ~ max(min_f0_Mel, .x[2]))),  
      x = map2(VOT, f0_Mel, ~ c("VOT" = .x, "f0_Mel" = .y)))
}
```

```{r study-AA-exposure-make-data, message=FALSE}
if(SET_SEED){
  set.seed(42007)
}

d.AA.exposure <- make_accent_adaptation_exposure_design(experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)

# calculate sample variance along each cue dimension for each exposure condition
sample_var <- d.AA.exposure %>%
  group_by(Condition, Item.Category) %>%
  summarise(s_var.f0 = var(f0_Mel),
    s_var.vot = var(VOT))

# get prior variance along each cue dimension for each exposure condition
population_var <- data.frame(
  c("/d/", "/d/", "/t/", "/t/"),
  c("VOT", "f0", "VOT", "f0"),
  c(m.VOT_f0_MVG$Sigma[2][[1]][1], m.VOT_f0_MVG$Sigma[2][[1]][4],
    m.VOT_f0_MVG$Sigma[6][[1]][1], m.VOT_f0_MVG$Sigma[6][[1]][4]))
colnames(population_var) = c("category", "cue", "prior_variance")
```

### Test phase
During test, listeners from both exposure conditions hear the same recordings from the same L2-accented talker used in the L2-accented exposure condition. 60 test tokens are randomly sampled from the distribution of each L2-accented category, half from the /d/ category and half from the /t/ category (Figure \@ref(fig:study-AA-exposure-test-plot)B).

```{r study-AA-test-functions}

# make the design of an test phase
make_accent_adaptation_test_design = function(
  experimenter.ideal_observer,
  experimenter.variability_reduction = my_experimenter.variability_reduction,
  category_dist_ratio1 = dist.L2_category.cue1,
  category_dist_ratio2 = dist.L2_category.cue2,
  shift_mean_1 = 0,
  shift_mean_2 = 0,
  test.n_block = n.test_block, 
  n_test = n.test.token     
) {
  d.model.both <- 
    get_parameters_phonetic_contrast.AA(
      experimenter.ideal_observer,
      category_dist_ratio1,
      category_dist_ratio2 ,
      shift_mean_1,
      shift_mean_2)
  
  ## --------------------------------------------------------------------------------
  # create a *natural* non-native test set
  ## --------------------------------------------------------------------------------
  x <- 
    rbind(
      rmvnorm(
        n_test,
        d.model.both %>% filter(speech == "nonnative", category == "A") %>% pull(mu) %>% .[[1]],
        d.model.both %>% filter(speech == "nonnative", category == "A") %>% pull(Sigma) %>% .[[1]] / experimenter.variability_reduction),
      rmvnorm(
        n_test,
        d.model.both %>% filter(speech == "nonnative", category == "B") %>% pull(mu) %>% .[[1]],
        d.model.both %>% filter(speech == "nonnative", category == "B") %>% pull(Sigma) %>% .[[1]] / experimenter.variability_reduction))
  
  d.test <-
    tibble(
      Item.Intended_category = sort(rep(c("A", "B"), n_test)),
      VOT = unlist(map(x[,1], ~ max(min_VOT, .x))), # cue 1
      f0_Mel = unlist(map(x[,2], ~ max(min_f0_Mel, .x))), # cue 2
      x = map2(VOT, f0_Mel, ~ c("VOT" = .x, "f0_Mel" =  .y))) %>%
    mutate(
      Item.Intended_category = ifelse(Item.Intended_category == "A", "/d/", "/t/"),
      Phase = "test",
      ItemID = row_number(),
      Item.Type = "test",
      Item.Category = NA,
    ) %>%
    crossing(  
       Condition = factor(conditions.AA),
      Block = 1:test.n_block) %>%
    droplevels()
}

add_subjects_to_test.AA <- function(
  d,
  n.subject,
  quiet = F
) {
  assert_that(!("Subject" %in% names(d)),
              msg = "This data frame already seems to contain subjects")
  if (!quiet) message(paste("Adding", n.subject, "subjects per exposure condition."))

  d %>%
    group_by(Condition) %>%
    crossing(Subject = factor(1:n.subject)) %>%
    mutate(Subject = paste(Condition, Subject, sep = ".")) %>%
    select(Condition, Subject, Phase, ItemID, Item.Category, Item.Intended_category, Item.Type, x, everything()) %>%
    ungroup()
}
```

```{r study-AA-test-make-data}
d.AA.test <- make_accent_adaptation_test_design(
  experimenter.ideal_observer = m.io.VOT_f0.AA, shift_mean_1 = shift.cue1, shift_mean_2 = shift.cue2)
```

(ref:study-AA-exposure-test-plot) **Panel A - Exposure:** Distribution of the stimuli used during the exposure phase of the accent adaptation experiment (`r n.exposure.token` tokens each of `r conditions.AA[1]` and `r conditions.AA[2]` `r paste(categories.AA, collapse = " and ")`, respectively). **Panel B - Test:**  Distribution of the stimuli used during the test phase of the accent adaptation experiment. The test tokens come from L2-accented speech (`r n.test.token` tokens per category) and are identical for the two exposure conditions. Ellipses show the 95% probability mass for the two categories in L2-accented exposure speech.

```{r plot-AA-exposure-test-functions}
# Function to plot exposure and test data
make_exposure_test_plot <- function(exposure.data, 
                                    test.data,
                                    categories = categories.AA){
  p.AA.exposure <- 
    exposure.data %>%
    mutate(Condition = paste0("Exposure: ", Condition)) %>%
    ggplot(aes(x = VOT, y = f0_Mel, color = Item.Category)) +
    geom_point(alpha = 0.5) +
    geom_rug(data = . %>%
                 group_by(Condition, Item.Category) %>%
                 summarise(VOT = mean(VOT), f0_Mel = mean(f0_Mel)), show.legend = FALSE) +
    scale_x_continuous(expression("VOT (ms)"), 
                        expand = expansion(mult = .1, add = 0)) +
    scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0), breaks=seq(0, 500, by = 100)) +
    coord_cartesian(ylim = c(0, 500)) +
    scale_color_manual("Category", breaks = categories, values = colors.category) +
    scale_fill_manual("Category", breaks = categories, values = colors.category) +
    facet_grid(~ Condition)
  
  limits <- get_plot_limits(p.AA.exposure)
  breaks <- get_plot_breaks(p.AA.exposure)

  p.AA.test <- 
    test.data %>%
    filter(Condition == "L2-accented") %>%
    mutate(SpeechLabel = "Test: L2-accented") %>%
    ggplot(aes(x = VOT, y = f0_Mel, label = as.numeric(factor(ItemID)))) +
    geom_point(aes(color = Item.Intended_category), alpha = 0.5) +
    stat_ellipse(
      data = 
        exposure.data %>%
        filter(Condition == "L2-accented"), 
      mapping = aes(fill = Item.Category), 
      level = .95, alpha = 0.3, geom = "polygon") +
    scale_x_continuous(
      expression("VOT (ms)"),
      expand = expansion(mult = .1, add = 0),
      breaks = c(breaks$xbreaks),
      limits = c(limits$xmin, limits$xmax)) +
    scale_y_continuous(
      expression("f0 (Mel)"),
      breaks = c(breaks$ybreaks),
      expand = expansion(mult = .1, add = 0),
      limits = c(limits$ymin, limits$ymax)) +
    scale_color_manual("Category", values = colors.category) +
    scale_fill_manual("Category", values = colors.category, guide = "none") +
    coord_cartesian(xlim = c(limits$xmin, limits$xmax), ylim = c(limits$ymin, limits$ymax), expand = FALSE) +
    facet_grid(~ SpeechLabel)

  prow = plot_grid(
    p.AA.exposure + theme(legend.position="none"),
    p.AA.test + theme(legend.position="none"),
    labels = c('A)', 'B)'), nrow = 1, rel_widths = c(2,1.1))
  
  legend_prow <- 
    get_legend(
      p.AA.exposure +
        guides(color = guide_legend(title.position = "left", nrow = 1)))

  p.output <-  plot_grid(legend_prow, prow, ncol = 1, rel_heights = c(.1, 1))
  return(p.output)
}
```

```{r study-AA-exposure-test-plot, fig.width=base.width*3 + .5, fig.height=base.height + .5, fig.cap="(ref:study-AA-exposure-test-plot)", warning=FALSE}
make_exposure_test_plot(exposure.data = d.AA.exposure, test.data = d.AA.test)
```

```{r add-subjects-AA-exposure-test-data}
d.AA.exposure %<>% add_subjects_to_exposure(n.subject = n.subject)
d.AA.test %<>% add_subjects_to_test.AA(n.subject = n.subject)

d.AA.exposure.main <- d.AA.exposure # save this exposure dataset to be used by the figures in the general discussion which zooms in on specific parameterization of this case
d.AA.test.main <- d.AA.test # similarly save this test dataset
```

## Results

```{r AA-table-models-parameter-setting, warning=FALSE}
collapse_rows_df <- function(df, variable){
  group_var <- enquo(variable)
  df %>%
    group_by(!! group_var) %>%
    mutate(groupRow = 1:n()) %>%
    ungroup() %>%
    mutate(!!quo_name(group_var) := ifelse(groupRow == 1, as.character(!! group_var), "")) %>%
    select(-c(groupRow))
}

# set parameter ranges considered by the optimization algorithm
## parameters for representations model
range.prior_kappa <- c(1, 10000)
range.prior_nu <- c(4, 10000) 
## parameters for bias model
range.lapse_rate <- c(0, 1)
range.beta_pi <- c(10^-3, 10^2)

## parameters for normalization model
range.prior_kappa.normalization <- c(1, 10000)

d.parameters <- data.frame(c("Changes in decision-making", "Changes in decision-making", "Changes in representations", "Changes in representations", "Changes in normalization"),
  c("$\\lambda$", "$\\beta_\\pi$", "$\\kappa_{c,0}$", "$\\nu_{c,0}$", "$\\kappa_0$"),
                           c(c(paste0(range.lapse_rate[1], " to " ,range.lapse_rate[2])),
                           c(paste0(range.beta_pi[1], " to " ,range.beta_pi[2])),
                             c(paste0(range.prior_kappa[1], " to " ,range.prior_kappa[2])),
                           c(paste0(range.prior_nu[1], " to " ,range.prior_nu[2])),
                           c(paste0(range.prior_kappa.normalization[1], " to " ,range.prior_kappa.normalization[2]))))

colnames(d.parameters) = c("Model","Parameters", "Range")

knitr::kable(d.parameters %>% collapse_rows_df(Model), caption = "Parameter range considered for each change model", booktabs = TRUE, escape = FALSE) %>%
  collapse_rows(1, latex_hline = "major", valign = "middle") %>% 
  kable_styling()
```

Paralleling Case Study 1, we predict L1 listeners' response behavior under each of the three change models. For each change model, we evaluate whether it can explain the two types of signature results observed in, for example, Xie et al. [-@xie2016jep]: (1) with L1-accented exposure, /d/ test tokens are recognized with lower accuracy than /t/ test tokens, since the categories are shifted towards /t/ along the primary cue VOT, creating a deviation from L1 listeners' expectations; and (2) L2-accented exposure should lead to a significant increase in recognition accuracy for /d/ test tokens without a corresponding decrease in the accuracy for /t/ test tokens, resulting in an overall increase in accuracy. 

As in Case Study 1, we first model changes in representations, and then compare them against changes in decision-making and normalization. We consider a wider range of parameterizations than in Case Study 1, because the models with the highest accuracy for L2-accented exposure (henceforth, the best-performing parameterization) in some cases fell outside the parameter ranges considered in Case Study 1. These best-performing parameterizations were determined by bounded quasi-Newton optimization of the recognition accuracy during test after L2-accented exposure [@byrd1995, implemented in function \texttt{optim()} in R], and are indicated in all result plots we present below. Table \@ref(tab:AA-table-models-parameter-setting) summarizes the parameter ranges considered by the optimization algorithm [$\nu_{c,0}$ must be larger than the number of cues $k$ + 1, @murphy2012, p. 134].^[In Case Study 1, there was no measure like accuracy for which performance can unambiguously be interpreted as 'better' or 'worse'.] We emphasize that we use the term "best-performing" to refer to the parameterization that achieves the highest recognition accuracy for L2-accented test tokens, rather than the best fit against human responses in a perception experiment. In the general discussion, we describe how the same approach can be used to fit change models to the results of perception experiments.

```{r study-AA-models-common-functions}
basic_AA_result_plot <- function(data) {
  data %>%
    # Show the response only for intended category
    filter(category == Item.Intended_category) %>% 
    ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
    list(
      stat_summary(
        fun = mean,
        geom = "bar", position = pos, width = 0.6),
      stat_summary(
        aes(color = Item.Intended_category),
        fun.data = mean_cl_boot,
        geom = "uperrorbar",
        position = pos, width = 0.2),
      coord_cartesian(ylim = c(0, 1.1)),
      scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1)),
      scale_color_manual("Category", values = colors.category, aesthetics = c("color", "fill")),
      scale_alpha_discrete(range = c(0.2, 1), guide = "none"),
      scale_x_discrete(labels= c("L1-accented", "L2-accented")),
      xlab("Exposure condition"),
      ylab("Predicted categorization accuracy"),
      myGplot.defaults(base_size = 14, set_theme = F),
      theme(legend.position = "top", 
            axis.text.x = element_text(angle = 22.5, hjust = 1),
      panel.grid.major.x = element_blank()))
}
```

### Changes in representations

```{r study-AA-models-changes-in-representations-functions, echo=FALSE, message=FALSE, warning=FALSE}
update_representations_and_categorize_test <- function(
  prior,
  exposure,
  test,
  cues = c("VOT", "f0_Mel")
) {
  update_NIW_ideal_adaptor_incrementally(
    prior = prior,
    exposure = exposure,
    exposure.category = "Item.Category",
    exposure.cues = cues,
    noise_treatment = "marginalize",
    lapse_treatment = "marginalize",
    method = "label-certain",
    keep.update_history = FALSE,
    keep.exposure_data = FALSE) %>%
    nest(posterior = everything()) %>%
    add_test_and_categorize(test)
}

# This function is intended for the optimization run right below it
# (we also store the history of the optimizers search)
history.optimization_representations <- tibble(.rows = 0)
get_accuracy_from_updated_representations <- function(
    pars,
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), 
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1"))
) {
  kappa <- exp(pars[1])
  nu <- exp(pars[2])
  
  prior <- 
    model %>% 
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = kappa, nu = nu)
  
  u <-
    update_representations_and_categorize_test(
    prior = prior,
    exposure = exposure,
    test = test) %>%
    get_accuracy()
  
  history.optimization_representations <<- 
    bind_rows(
      history.optimization_representations, 
      tibble(kappa = kappa, nu = nu, accuracy = u))
  
  return(u)
}
```

```{r study-AA-models-changes-in-representations, echo=FALSE, message=FALSE, warning=FALSE}
find_best_model.representations.AA <- function(){
  if (RESET_MODELS || !file.exists(paste0("../models/best_performing_parameters.representations_", example_label,".rds"))) {
    best_performing_parameters.representations <-
    optim(
      # Parameters are kappa, nu
      par = c(mean(log(range.prior_kappa)), mean(log(range.prior_nu))),
      fn = get_accuracy_from_updated_representations,
      # The only method that works for multiple parameters with bounds
      method = "L-BFGS-B",
      # Stop searching outside of those bounds
      lower = c(min(log(range.prior_kappa)), min(log(range.prior_nu))),
      upper = c(max(log(range.prior_kappa)), max(log(range.prior_nu))),
      control = list(
        # trace = 1,
        fnscale = -1,   # negative scale since we are seeking to maximize accuracy
        factr = 10^8))  # smaller factr means higher optimum but slower convergence
  
    best_performing_parameters.representations$par <- exp(best_performing_parameters.representations$par)
    
    saveRDS(best_performing_parameters.representations, get_path(paste0("../models/best_performing_parameters.representations_", example_label,".rds")))
    
    saveRDS(history.optimization_representations, get_path(paste0("../models/d.AA.history.optimization.representations_", example_label,".rds")))
     
  } else {
    best_performing_parameters.representations <- readRDS(get_path(paste0("../models/best_performing_parameters.representations_", example_label,".rds")))
    history.optimization_representations <- readRDS(get_path(paste0("../models/d.AA.history.optimization.representations_", example_label,".rds")))
  }
  return(best_performing_parameters.representations)
}

get_representation_models_for_plot <- function(
    prior_kappa = prior_kappa,
    prior_nu = prior_nu
){
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/d.AA.results.representations_", example_label,".rds")))){
    m.ia.VOT_f0.AA.plot_sample <-
      crossing(
        prior_kappa = prior_kappa,
        prior_nu = prior_nu) %>%
      rowwise() %>%
      mutate(ideal_adaptor = map2(prior_kappa, prior_nu, ~ make_stop_VOTf0_ideal_adaptor(m = m.io.VOT_f0.AA, kappa = .x, nu = .y))) %>%
      unnest(ideal_adaptor) %>%
      arrange(category)
    
    representations.pred <- 
      d.AA.exposure %>%
      nest(data = -c(Condition, Subject)) %>%
      crossing(
        m.ia.VOT_f0.AA.plot_sample %>%
          nest(prior = -c(prior_kappa, prior_nu))) %>%
      group_by(Condition, Subject, prior_kappa, prior_nu) %>%
      group_modify(~ update_representations_and_categorize_test(prior = .x$prior[[1]], exposure = .x$data[[1]], test = d.AA.test %>% filter(Condition == "L2-accented")), .keep = TRUE) %>%
      mutate_at(vars(starts_with("prior_")), ~factor(.x)) %>%
      mutate_at(vars(starts_with("prior_")), fct_rev) %>%
      ungroup()
    
    saveRDS(representations.pred, get_path(paste0("../models/d.AA.results.representations_", example_label,".rds"))) #
  } else {
    representations.pred <- readRDS(get_path(paste0("../models/d.AA.results.representations_", example_label,".rds")))
  }
  
  return(representations.pred)
}


d.AA.representations <- get_representation_models_for_plot(prior_kappa = prior_kappa.plot, prior_nu = prior_nu.plot)
best_performing_parameters.representations <- find_best_model.representations.AA()
```

(ref:AA-result-changes-in-representations) Predictions of a learning model that derives accent adaptation as changes in category representations. Predicted categorization accuracies for the L2-accented test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, as a function of the strength of the prior beliefs in category means ($\kappa_{c,0}$) and covariances ($\nu_{c,0}$). The average accuracy across /d/ and /t/ is shown above the bars for each exposure condition. Error bars show 95% bootstrapped confidence intervals. The highlighted panel is the one closest to the best-performing parameterization ($\kappa_{0,c} = `r best_performing_parameters.representations$par[1]`$; $\nu_{c,0}=`r best_performing_parameters.representations$par[2]`$; overall accuracy $=`r best_performing_parameters.representations$value`$). 

```{r plot-AA-results-representations-functions}
make_results_plot_representations <- function(data, data_best) {
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, 
      data = . %>%
        group_by(Condition, prior_nu, prior_kappa) %>%
        summarise(mAcc = round(mean(response), digits = 2)), 
      mapping = aes(label = mAcc, x = Condition, y = 1.05), 
      size = geom_text.size) +
    facet_grid(
      prior_nu ~ prior_kappa,
      labeller = label_bquote(
        cols = {kappa[.(categories.AA[1])~","~0] == kappa[.(categories.AA[2])~","~0]} == .(as.character(prior_kappa)),
        rows = {nu[.(categories.AA[1])~","~0] == nu[.(categories.AA[2])~","~0]} == .(as.character(prior_nu)))) 
  
  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = data %>%
          filter(category == Item.Intended_category) %>%
          distinct(prior_kappa, prior_nu) %>%
          mutate(
            highlight = 
              ifelse(
                prior_kappa %in% prior_kappa[which.min(abs(as.numeric(as.character(prior_kappa)) - data_best$par[1]))] & #highlight the parameters (within the plotted range) that are closest to the best performing parameterization
                  prior_nu %in% prior_nu[which.min(abs(as.numeric(as.character(prior_nu)) - data_best$par[2]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf)) +
  scale_fill_manual(breaks = c(T, F), values = c("gray20", "white")) +
  guides(fill = "none")
}
```

```{r AA-result-changes-in-representations, fig.width= base.width * 6, fig.height= base.height * 6 + 1, fig.cap="(ref:AA-result-changes-in-representations)", warning=FALSE}
make_results_plot_representations(data = d.AA.representations, data_best = best_performing_parameters.representations)
```

Figure \@ref(fig:AA-result-changes-in-representations) shows the predicted categorization accuracy after exposure to L1- or L2-accented speech, as a function of the strength of the prior beliefs for the category means and variances. First consider the top left panel, where both $\kappa_{c,0}$s and $\nu_{c,0}$ have very high values. This simulates a very slow learner with strong prior beliefs for both category means and variances, so that the small number of exposure tokens has minimal influence on subsequent perception of the L2-accented test tokens. Therefore, the performance seen in the L1- and L2-accented exposure conditions is more or less identical, reflecting how a listener would draw primarily on their long-term prior experience to categorize the test tokens.

As $\kappa_{c,0}$s and $\nu_{c,0}$ get smaller (indicating weaker prior beliefs), the effects of exposure become more noticeable, and both of the signature results of experiments on accent adaptation begin to emerge. First, for L1-accented exposure, the accuracy is always predicted to be lower for /d/ than for /t/, indicating that /d/ is often misheard as /t/. Second, L2-accented exposure is predicted to improve recognition accuracy of /d/ compared to L1-accented exposure, and this improvement occurs without corresponding decreases in the recognition accuracy of /t/. Looking across the range of $\kappa_{c,0}$s and $\nu_{c,0}$s, we make two more observations. First, for the scenario studied here, faster learning of category means improves recognition accuracy. When listeners' beliefs about category covariances are held constant (i.e., looking within each row), smaller $\kappa_{c,0}$ values yield higher categorization accuracy. Second, faster updating of beliefs about the _(co)variance_ also leads to accuracy improvements, although these improvements are less pronounced. Both results follow from the way that the Korean-accented stimuli differ from the L1 accented stimuli: as shown in Figure \@ref(fig:study-AA-exposure-test-plot), the category means of Korean-accented /d/ and /t/ are shifted relative to the L1-accented stimuli. In comparison, differences in category (co)variances are relatively subtle between the two sets of stimuli. For the present exposure stimuli, adapting to the L2 accent thus primarily hinges on learning the new category means. Taken together, these results demonstrate how changes in representation can explain the types of results that are considered to be the signature of accent adaptation.

### Changes in decision-making

```{r study-AA-models-changes-in-decision-making-functions, echo=FALSE, message=FALSE, warning=FALSE}
simulation.control <-
  list(
    min.simulations = 50, 
    max.simulations = 100, 
    step.simulations = 1, 
    target_accuracy_se = .002)

get_average_bias_results_across_simulations <- function(data){
  output <- 
    data %>%
    group_by(sim) %>%
    mutate(response.mean = mean(response)) %>%
    ungroup() %>%
    group_by(!!! syms(setdiff(names(.), c("sim", "posterior", "response.mean", "response")))) %>%
    summarise(
      response.n_sims = n_distinct(sim),
      response = mean(response.mean),
      response.mean.se = sd(response.mean) / sqrt(n_distinct(sim))) %>%
    relocate(
     !!! syms(setdiff(names(.), c("response", "response.mean.se", "response.n_sims"))),
     response, response.mean.se, response.n_sims) %>%
    ungroup()
      
  return(output)
}

history.optimization_bias <- tibble(.rows = 0)
get_accuracy_from_updated_bias <- function(
    pars,
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), # find the best-performing parameterization for the L2-accented condition
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")),
    control = simulation.control,
    verbose = FALSE
) {
  lapse_rate <- pars[1]
  beta_pi <- exp(pars[2])
  max_kappa_nu <- 10000

  prior <-
    model %>%
    mutate(lapse_rate = .env$lapse_rate) %>%
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = max_kappa_nu, nu = max_kappa_nu)
  
  u <-
    update_bias_and_categorize_test( # Exposure token order is reshuffled on each run
    prior = prior,
    lapse_rate = lapse_rate, 
    beta_pi = beta_pi,
    exposure = exposure,
    test = test,
    verbose = verbose,
    control = control)
  
  # u <- get_average_bias_results_across_simulations(u) %>%
  #   get_accuracy()
  
  # history.optimization_bias <<-
  #   bind_rows(
  #     history.optimization_bias,
  #     tibble(lapse_rate = lapse_rate, beta_pi = beta_pi, accuracy = u))
  # 
  #  return(u)
  
  u.across_simulations <- get_average_bias_results_across_simulations(u) %>%
    distinct(response, response.mean.se, response.n_sims) 
  
  history.optimization_bias <<-
    bind_rows(
      history.optimization_bias,
      tibble(lapse_rate = lapse_rate, beta_pi = beta_pi, 
             n_sims = u.across_simulations$response.n_sims,
             accuracy = u.across_simulations$response.mean.mean, 
             accuracy.se = u.across_simulations$response.mean.se))
  
  accuracy = log(u.across_simulations$response.mean.mean)
  return(accuracy)
 
}


#DEBUG
get_accuracy_from_updated_bias1 <- function(
    pars,
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), # find the best-performing parameterization for the L2-accented condition
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")),
    control = simulation.control,
    verbose = FALSE
) {
  lapse_rate <- 0
  beta_pi <- exp(pars[1])
  max_kappa_nu <- 10000

  prior <-
    model %>%
    mutate(lapse_rate = .env$lapse_rate) %>%
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = max_kappa_nu, nu = max_kappa_nu)
  
  u <-
    update_bias_and_categorize_test( # Exposure token order is reshuffled on each run
    prior = prior,
    lapse_rate = lapse_rate, 
    beta_pi = beta_pi,
    exposure = exposure,
    test = test,
    verbose = verbose,
    control = control)
  
  # u <- get_average_bias_results_across_simulations(u) %>%
  #   get_accuracy()
  
  # history.optimization_bias <<-
  #   bind_rows(
  #     history.optimization_bias,
  #     tibble(lapse_rate = lapse_rate, beta_pi = beta_pi, accuracy = u))
  # 
  #  return(u)
  
  u.across_simulations <- get_average_bias_results_across_simulations(u) %>%
    distinct(response.mean.mean, response.mean.se, response.n_sims) 
  
  history.optimization_bias <<-
    bind_rows(
      history.optimization_bias,
      tibble(lapse_rate = lapse_rate, beta_pi = beta_pi, 
             n_sims = u.across_simulations$response.n_sims,
             accuracy = u.across_simulations$response.mean.mean, 
             accuracy.se = u.across_simulations$response.mean.se))
  
  accuracy = log(u.across_simulations$response.mean.mean)
  return(accuracy)
 
}
```

```{r study-AA-models-changes-in-decision-making, echo=FALSE, message=FALSE, warning=FALSE}
start_time <- Sys.time() #DEBUG
# cl <- makeCluster(detectCores())     # set the number of processor cores
# setDefaultCluster(cl=cl) # set 'cl' as default cluster

find_best_model.bias.AA <- function() {
  if(SET_SEED) set.seed(42007)
  
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))) {
    
    print(paste0("The optim program starts at", Sys.time())) #DEBUG
    best_performing_parameters.bias <-
      optim( # DEBUG
        # par = c(mean(range.lapse_rate), mean(log(range.beta_pi))), # DEBUG
        # fn = get_accuracy_from_updated_bias,
        par = c(log(0.1)),
        fn = get_accuracy_from_updated_bias1, #DEBUG
        method = "L-BFGS-B",
        # lower = c(min(range.lapse_rate), min(log(range.beta_pi))),
        # upper = c(max(range.lapse_rate), max(log(range.beta_pi))),
        lower = c(min(log(range.beta_pi))), # DEBUG
        upper = c(max(log(range.beta_pi))), # DEBUG
        control = list(
          fnscale = -1,
        #  factr = 10^8))
          #DEBUG
          factr = 10^5))
    
    print(paste0("The optim program ends at", Sys.time())) #DEBUG
    
    best_performing_parameters.bias$par <- c(best_performing_parameters.bias$par[1], exp(best_performing_parameters.bias$par[2]))
    saveRDS(best_performing_parameters.bias, get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))
    saveRDS(history.optimization_bias, get_path(paste0("../models/d.AA.history.optimization.bias_", example_label,".rds")))
  } else {
    best_performing_parameters.bias <- readRDS(get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))
    history.optimization_bias <- readRDS(get_path(paste0("../models/d.AA.history.optimization.bias_", example_label,".rds")))
  }
  
  return(best_performing_parameters.bias)
}

best_performing_parameters.bias <- find_best_model.bias.AA()

end_time <- Sys.time() #DEBUG
end_time - start_time #DEBUG



start_time <- Sys.time() #DEBUG
get_decision_making_models_for_plot <- function(
    lapse_rate = lapse_rate,
    beta_pi = beta_pi,
    control = simulation.control
) {
  if(SET_SEED) set.seed(42007)
  
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/d.AA.results.bias_", example_label,".rds")))) {
    bias.pred <- 
      d.AA.exposure %>%
      nest(data = -c(Condition, Subject)) %>%
      crossing(
        posterior.lapse_rate = lapse_rate, 
        beta_pi = beta_pi) %>% 
      crossing(
        m.ia.VOT_f0.AA %>%
          filter(prior_kappa == max(prior_kappa), prior_nu == max(prior_nu)) %>%
          nest(prior = everything())) %>%
      group_by(Condition, Subject, posterior.lapse_rate, beta_pi) %>%
      group_modify(
        ~ update_bias_and_categorize_test(
          prior = .x$prior[[1]],
          lapse_rate = .x$posterior.lapse_rate,
          beta_pi = .x$beta_pi,
          exposure = .x$data[[1]],
          test = d.AA.test %>% filter(Condition == "L2-accented"),
          control = control), 
        .keep = TRUE, verbose = T) %>%
      mutate_at(vars(starts_with("prior_")), ~ factor(.x)) %>%
      mutate_at(vars(starts_with("prior_")), fct_rev) %>%
      ungroup()
    
    saveRDS(bias.pred, get_path(paste0("../models/d.AA.results.bias_", example_label,".rds"))) # save results for bias because it takes a long time to run
  } else {
    bias.pred <- readRDS(get_path(paste0("../models/d.AA.results.bias_", example_label,".rds")))
  }
  
  return(bias.pred) 
}

# specify what parameters to plot
posterior.lapse_rate.plot <- c(0, 0.005, 0.05, 0.5, 1) 
beta_pi.plot <- c(0, 0.1, 0.5, 1, 4) 
  
d.AA.bias <- 
  get_decision_making_models_for_plot(
    lapse_rate = posterior.lapse_rate.plot,
    beta_pi = beta_pi.plot)

end_time <- Sys.time() #DEBUG
end_time - start_time #DEBUG

## DEBUGGING
d.AA.bias.summary <- d.AA.bias %>%
  group_by(Condition, posterior.lapse_rate, beta_pi, sim) %>%
  summarise(response.mean = mean(response)) %>%
  group_by(Condition, posterior.lapse_rate, beta_pi) %>%
  mutate(sim.max = max(sim),
         accuracy = mean(response.mean),
         response.se = sd(response.mean) / sqrt(n_distinct(sim))) %>%
  distinct(posterior.lapse_rate, beta_pi, sim.max, accuracy, response.se)
```

(ref:AA-result-changes-in-decision-making) Predictions of a model that derives accent adaptation as changes in decision-making. Predicted categorization responses for the test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, depending on the rate at which response biases change ($\beta_{\pi}$) and the rate of attentional lapses ($\lambda$). The highlighted panel is the one closest to the best-performing parameterization ($\beta_{\pi} = `r best_performing_parameters.bias$par[2]`$; $\lambda=`r best_performing_parameters.bias$par[1]`$; overall accuracy$=`r best_performing_parameters.bias$value`$).

```{r plot-AA-results-decision-making-functions}
make_results_plot_decision_making <- function(data, data_best){
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, data = . %>%
        group_by(Condition, posterior.lapse_rate, beta_pi) %>%
        summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1.05), size = geom_text.size) +
    facet_grid(
      posterior.lapse_rate ~ beta_pi,
      labeller = label_bquote(
        cols = beta[pi] == .(beta_pi),
        rows = lambda == ~.(posterior.lapse_rate))) 

  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = data %>%
          filter(category == Item.Intended_category) %>%
          distinct(posterior.lapse_rate, beta_pi) %>%
          ungroup() %>%
          mutate(
            highlight = 
              ifelse(
                posterior.lapse_rate %in% posterior.lapse_rate[which.min(abs(posterior.lapse_rate - data_best$par[1]))] & 
                  beta_pi %in% beta_pi[which.min(abs(beta_pi - data_best$par[2]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf)
    ) +
    scale_fill_manual(breaks = c(T, F), values = c("gray20", "white")) +
    guides(fill = "none")
}
```

```{r AA-result-changes-in-decision-making, fig.width= base.width * 5, fig.height= base.height * 5 + 1, fig.cap="(ref:AA-result-changes-in-decision-making)", warning=FALSE}
make_results_plot_decision_making(data = get_average_bias_results_across_simulations(d.AA.bias), data_best = best_performing_parameters.bias)
```

Figure \@ref(fig:AA-result-changes-in-decision-making) shows the effects of changes in decision-making as a function of the rate at which response biases change $\beta_{\pi}$ and the lapse rate $\lambda$, averaged across all simulations for each parameterization. As in Case Study 1, the figure shows the average across 50 repeated simulations, each time randomizing the order of exposure. This was sufficient to achieve reproducible estimates even for the largest $\beta_{\pi}$s considered here (SEs of the mean accuracy after L2-accented exposure across repeated simulations were smaller than $0.002$, meaning that the 95% CI across the simulations spanned less than 1% in recognition accuracy). 

For L1-accented exposure, we again see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens, regardless of the specific value of $\beta_{\pi}$. The second signature result---improved accuracy after L2-accented exposure for /d/---is obtained for sufficiently fast learning rates $\beta_{\pi}$. As $\beta_{\pi}$ increases, the overall recognition accuracy first increases and then plateaus. The largest improvements are found for the smallest lapse rates and relatively fast learning rates (gray panel in Figure \@ref(fig:AA-result-changes-in-decision-making)). 

Importantly, Figure \@ref(fig:AA-result-changes-in-decision-making) shows that changes in response biases do not necessarily result in the type of zero-sum trade-off that is sometimes attributed to them---accuracy increasing by some degree for one category while decreasing by the same degree for the other category. Rather, L2-accented exposure can predict improvements in the *overall* recognition accuracy across both categories (e.g., top right panel). Zero-sum trade-offs are only expected under the highly implausible assumptions that responses are entirely independent of stimulus properties (bottom row of Figure \@ref(fig:AA-result-changes-in-decision-making)). Indeed, for the specific L1-L2 accent combination considered here, the largest improvements observed for changes in decision-making are similar in magnitude to those observed for the representational change model, despite the fact that the former change model is computationally simpler.

### Changes in normalization
Finally, we compare models that normalize test tokens based on the phonetic input experienced during exposure against models that continue to apply normalization based on prior long-term L1 experience. Figure \@ref(fig:AA-result-changes-in-normalization) shows the predicted categorization accuracy following changes in normalization in the L1- and L2-accented exposure conditions. We again obtain both of the signature results of experiments on accent adaptation. For L1-accented exposure, we continue to see that /d/ test tokens are categorized substantially less accurately than /t/ test tokens. For L2-accented exposure, recognition accuracy is improved for /d/ test tokens without any decrease in the recognition accuracy for /t/. That is, like the other two change models, normalization can predict *overall* improvements in recognition accuracy after L2-accented exposure. Indeed, for the specific L1-L2 accent combination considered here, the computationally simple model for changes in normalization yields improvements similar to those predicted by changes in category representations.<!--similar to the results for changes in decision-making.-->

```{r study-AA-models-normalization-functions, echo=FALSE, message=FALSE, warning=FALSE}
update_normalization_and_categorize_test <- function(
  prior,
  mu_0 = first(prior_marginal_VOT_f0_stats$x_mean),
  kappa.normalization,
  exposure,
  test
) {
  # Get normalization parameters for exposure data
  exposure.normalization <- 
    exposure %>%
    summarise(
      x_N = length(x),
      x_mean = list(colMeans(reduce(x, rbind))),
      x_cov = list(cov(reduce(x, rbind))))
  
  # Apply normalization based on exposure to test
  mu_inferred <- 1 / 
            (kappa.normalization + exposure.normalization$x_N[[1]]) * 
            (kappa.normalization * mu_0 + exposure.normalization$x_N[[1]] * exposure.normalization$x_mean[[1]])

  test %<>%
    mutate(
      x_unnormalized = x,
      x = map(x, ~ .x - (mu_inferred - mu_0)))
  
  test %>%
    select(x_unnormalized, x, Item.Intended_category) %>%
    nest(x = x, Item.Intended_category = c(x, Item.Intended_category, x_unnormalized)) %>%
    mutate(posterior = list(prior)) %>%
    # Don't add test again since it's already in the data
    add_test_and_categorize(test = NULL) 
}

# This function is intended for the optimization run right below it
history.optimization_normalization <- tibble(.rows = 0)
get_accuracy_from_updated_normalization <- function(
    par, 
    model = m.io.VOT_f0.AA,
    exposure = d.AA.exposure %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1")), 
    test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1"))
) {
  kappa.normalization <- exp(par[1])
  max_kappa_nu <- 10000
  
  prior <-
    model %>% 
    droplevels() %>%
    lift_MVG_ideal_observer_to_NIW_ideal_adaptor(kappa = max_kappa_nu, nu = max_kappa_nu)

  u <- 
    update_normalization_and_categorize_test(
    prior = prior, 
    kappa.normalization = kappa.normalization,
    exposure = exposure,
    test = test) %>%
    get_accuracy()
  
  history.optimization_normalization <<- 
    bind_rows(
      history.optimization_normalization, 
      tibble(kappa.normalization = kappa.normalization, accuracy = u)) 
  
  return(u)
}
```

```{r study-AA-models-normalization, echo=FALSE, message=FALSE, warning=FALSE}
find_best_model.normalization.AA <- function(){
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))) {
    best_performing_parameters.normalization <-
      optim(
        par = log(mean(range.prior_kappa.normalization)),
        fn = get_accuracy_from_updated_normalization,
        method = "L-BFGS-B",
        lower = log(min(range.prior_kappa.normalization)),
        upper = log(max(range.prior_kappa.normalization)),
        control = list(
          fnscale = -1,
          factr = 10^8))
    best_performing_parameters.normalization$par <- exp(best_performing_parameters.normalization$par[1])
    
    saveRDS(best_performing_parameters.normalization, get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))
    saveRDS(history.optimization_normalization, get_path(paste0("../models/d.AA.history.optimization.normalization_", example_label,".rds")))
  } else {
    best_performing_parameters.normalization <- readRDS(get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))
    history.optimization_normalization <- readRDS(get_path(paste0("../models/d.AA.history.optimization.normalization_", example_label,".rds")))
  }
  
  return(best_performing_parameters.normalization)
}

get_normalization_models_for_plot <- function(
    prior_kappa.normalization = prior_kappa.normalization
){
  if (RESET_MODELS || !file.exists(get_path(paste0("../models/d.AA.results.normalization_", example_label,".rds")))){
     normalization.pred <- d.AA.exposure %>%
       nest(data = -c(Condition, Subject)) %>%
       crossing(
          prior_kappa.normalization = prior_kappa.normalization,
          m.ia.VOT_f0.AA %>%
            filter(prior_kappa == max(prior_kappa) & prior_nu == max(prior_nu)) %>%
            nest(prior = everything())) %>%
       group_by(Condition, Subject, prior_kappa.normalization) %>%
       group_modify(
         ~ update_normalization_and_categorize_test(
           prior = .x$prior[[1]],
           kappa.normalization = .x$prior_kappa.normalization,
           exposure = .x$data[[1]],
           test = d.AA.test %>% filter(Condition == "L2-accented", Subject == paste0(Condition, ".1"))), .keep = TRUE) %>%
       mutate_at(vars(starts_with("prior_")), ~factor(.x)) %>%
       mutate_at(vars(starts_with("prior_")), fct_rev) %>%
      ungroup()
     
     saveRDS(normalization.pred, get_path(paste0("../models/d.AA.results.normalization_", example_label,".rds")))
   } else {
     normalization.pred <- readRDS(get_path(paste0("../models/d.AA.results.normalization_", example_label,".rds")))
   }
  return(normalization.pred)
}

prior_kappa.normalization.plot <- c(1, 4, 16, 64, 256, 1024) # specify what parameters to plot
best_performing_parameters.normalization <- find_best_model.normalization.AA()
d.AA.normalization <- get_normalization_models_for_plot(prior_kappa.normalization = prior_kappa.normalization.plot)
```

(ref:AA-result-changes-in-normalization) Predictions of a model that derives accent adaptation from changes in normalization. Predicted categorization responses for the test tokens after `r conditions.AA[1]` and `r conditions.AA[2]` exposure, depending on the relative weighting of previous experience ($\kappa_0$) during the inference of the cue mean during exposure. The highlighted panel is the one closest to the best-performing parameterization ($\kappa_0 = `r best_performing_parameters.normalization$par[1]`$; overall accuracy $=`r best_performing_parameters.normalization$value`$).

```{r plot-AA-results-normalization-functions}
make_results_plot_normalization <- function(data, data_best){
  p.results <- 
    basic_AA_result_plot(data) +
    geom_text(
      inherit.aes = FALSE, 
      data = . %>%
        group_by(Condition, prior_kappa.normalization) %>%
        summarise(mAcc = round(mean(response), digits = 2)), 
      mapping = aes(label = mAcc, x = Condition, y = 1.05), 
      size = geom_text.size) +
    facet_grid(
      . ~ prior_kappa.normalization,
      labeller = label_bquote(
        cols = ~kappa[0] == .(as.character(prior_kappa.normalization)))) 
  
  p.results +
    ggnewscale::new_scale_fill() +
    insert_layers(
      geom_rect(
        data = 
          data %>%
          filter(category == Item.Intended_category) %>%
          distinct(prior_kappa.normalization) %>%
          mutate(
            highlight = 
              ifelse(
                prior_kappa.normalization %in% prior_kappa.normalization[which.min(abs(as.numeric(as.character(prior_kappa.normalization)) - data_best$par[1]))],
                T, F)),
        aes(fill = highlight), alpha = .2, inherit.aes = F,
        xmin = -Inf, xmax = Inf, ymin = -Inf ,ymax = Inf)) +
    scale_fill_manual(breaks = c(T, F), values = c("gray20", "white")) +
    guides(fill = "none")
}
```

```{r AA-result-changes-in-normalization, fig.width= base.width * 5, fig.height= base.height + 1.75, fig.cap="(ref:AA-result-changes-in-normalization)", warning=FALSE}
make_results_plot_normalization(d.AA.normalization, best_performing_parameters.normalization)
```

## Summary
Paralleling Case Study 1 on perceptual recalibration, we find that any of the three change mechanisms can qualitatively explain the signature results for accent adaptation---at least for the hypothetical L2 accent considered here. Although this L2 accent differs from L1 listeners’ prior expectations in complex ways---specifically, the relative weighting of VOT and f0---even computationally simple, non-representational change mechanisms (normalization and decision-making) predict improved recognition accuracy after L2-accented exposure. For example, contrary to common assumptions, changes in decision-making can lead to _overall_ improvements in recognition accuracy, not just zero-sum trade-offs. This supports the conclusion of Case Study 1: at the level of analysis that is commonly applied in previous work (and thus in our case studies), experiments on accent adaptation do not necessarily rule out any of the three mechanisms.

For the hypothetical L2 accent considered in Case Study 2, the three change models do not only make similar *qualitative* predictions (e.g., improved performance after L2-accented exposure), they also yield highly comparable *quantitative* performance: recognition accuracies of the best-performing models in Figures \@ref(fig:AA-result-changes-in-representations)-\@ref(fig:AA-result-changes-in-normalization) range from .92 for normalization to .94 for changes in decision-making. Intuitively, quantitatively similar performance makes it particularly difficult to distinguish between the three change models, or combinations thereof.<!-- ^[In this example, we are referring to the best-performing parameterizations of each change model. We emphasize again that this is not necessarily the same as the parameterization that best fits human behavior. Determining those best-*fitting* parameterization is the same as determining the relative engagement of the three mechanisms, and thus is the endeavor for future research that the present article aims to support through the development of ASP.]-->

Fortunately for research on adaptive speech perception, it is not always the case that the three change models can explain similar ranges of performance. A critical insight of ASP is that the predicted benefit of L2-accented exposure under each mechanism depends on the acoustic-phonetic statistics of the accent, relative to L1 listeners’ prior expectations. For experiments using naturally accented stimuli, this means that the results are expected to depend on the specific acoustic-phonetic properties of the exposure and test tokens. To further illustrate this point, we simulated three additional scenarios, shown in Figure \@ref(fig:AA-plot-results-additional-cases)A. Each of these three scenarios is representative of commonly attested differences between L1- and L2-accents. The top row of Figure \@ref(fig:AA-plot-results-additional-cases) shows an example of *contrast reduction*, in which the L2 accent shows a greater category overlap than the L1 accent. Here it is simulated by shifting /d/ category towards /t/ along the VOT continuum while keeping the latter unchanged [as qualitatively attested for, e.g., vowels in Spanish-accented English, @wade2007]. The middle row shows an example of *contrast shift*, where two categories in L2-accented speech are shifted along one or more of the cue dimensions as compared to L1-accented speech. Here it is simulated by shifting both /d/ and /t/ categories towards lower VOT [qualitatively attested for, e.g., word-initial stops in French-accented English, @Sumner2011a]. The bottom row of Figure \@ref(fig:AA-plot-results-additional-cases) shows a more extreme example, exhibiting almost complete *contrast collapse* [similar to the loss of the /s/-/`r linguisticsdown::cond_cmpl("θ")`/ contrast that can occur in Mandarin-accented English, @zheng-samuel2020]. Here it is simulated by making /d/ identical to /t/. 

```{r compare-AA-models-best-three-functions}
# Function to construct change model of each type and select the best performing parameter combination
construct_three_models <- function(
    example_label = "Cue_reweighting",
    experimenter.ideal_observer,
    category_dist_ratio1 = dist.L2_category.cue1,
    category_dist_ratio2 = dist.L2_category.cue2,
    shift_mean_1 = shift.cue1, 
    shift_mean_2 = shift.cue2,
    test.n_block = n.test_block,
    n_test = n.test.token
  ) {
  if(SET_SEED) set.seed(42007)
  
  d.AA.exposure <<-
    make_accent_adaptation_exposure_design(
      experimenter.ideal_observer = m.io.VOT_f0.AA, 
      category_dist_ratio1 = dist.L2_category.cue1, 
      category_dist_ratio2 = dist.L2_category.cue2, 
      shift_mean_1 = shift.cue1, 
      shift_mean_2 = shift.cue2) %>%
    add_subjects_to_exposure(n.subject = n.subject)

  d.AA.test <<- 
    make_accent_adaptation_test_design(
      experimenter.ideal_observer = m.io.VOT_f0.AA, 
      test.n_block = n.test_block, 
      n_test = n.test.token, 
      category_dist_ratio1 = dist.L2_category.cue1, 
      category_dist_ratio2 = dist.L2_category.cue2, 
      shift_mean_1 = shift.cue1, 
      shift_mean_2 = shift.cue2) %>%
    add_subjects_to_test.AA(n.subject = n.subject)
  
  p.exposure_test <- make_exposure_test_plot(d.AA.exposure, d.AA.test)
  ggsave(paste0('../figures/plotly/exposure_test_distribution_', example_label, '.png'), plot = p.exposure_test, width = base.width*3 + .5, height = base.height + .5, dpi = 300)


  history.optimization_representations <<- tibble(.rows = 0)
  best_performing_parameters.representations <- find_best_model.representations.AA()
  representations.pred <- get_representation_models_for_plot(
    prior_kappa = round(best_performing_parameters.representations$par[1], digits =0), # here one can select to plot just the best-performing parameterization or to visualize a range of parameters by specifying the values assigned to prior_kappa and prior_nu
    prior_nu = round(best_performing_parameters.representations$par[2], digits = 0))

  history.optimization_bias <<- tibble(.rows = 0)
  best_performing_parameters.bias <- find_best_model.bias.AA()
  bias.pred <- get_decision_making_models_for_plot(
    lapse_rate = round(best_performing_parameters.bias$par[1], digits = 2), # here one can select to plot just the best-performing parameterization or to visualize a range of parameters by specifying the values assigned to lapse_rate and beta_pi
    beta_pi = round(best_performing_parameters.bias$par[2], digits = 2))
  
  bias.pred %<>% get_average_bias_results_across_simulations() # average predictions across simulations for the bias model
  
  history.optimization_normalization <<- tibble(.rows = 0)
  best_performing_parameters.normalization <- find_best_model.normalization.AA()

  normalization.pred <-get_normalization_models_for_plot(
    prior_kappa.normalization = round(best_performing_parameters.normalization$par[1], digits = 0))

  # plot the predicted results for all parameterization and save plots
  p.AA.representations <- 
    make_results_plot_representations(
      data = representations.pred, 
      data_best = best_performing_parameters.representations)
  ggsave(
    paste0('../figures/plotly/representations_', example_label, '.png'), 
    plot = p.AA.representations, 
    width = 2*(base.width*length(levels(factor(representations.pred$prior_kappa))) + .5), 
    height = 2*(base.height*length(levels(factor(representations.pred$prior_nu))) + .5), 
    dpi = 300)

  p.AA.bias <- 
    make_results_plot_decision_making(
      data = bias.pred, 
      data_best = best_performing_parameters.bias)
  ggsave(
    paste0('../figures/plotly/bias_', example_label, '.png'), 
    plot = p.AA.bias, 
    width = 2*(base.width*length(levels(factor(bias.pred$beta_pi))) + .5), 
    height = 2*(base.height*length(levels(factor(bias.pred$posterior.lapse_rate))) + .5), 
    dpi = 300)

  p.AA.normalization <- 
    make_results_plot_normalization(
      normalization.pred, 
      best_performing_parameters.normalization)
  ggsave(
    paste0('../figures/plotly/normalization_', example_label, '.png'), 
    plot = p.AA.normalization, 
    width = 2*(base.width*length(levels(factor(normalization.pred$prior_kappa.normalization))) + .5), 
    height = 2*(base.height + .5), 
    dpi = 300)

   d <- bind_rows(
    representations.pred %>%
      mutate(ModelType = "representations"),
    bias.pred %>%
      mutate(ModelType = "decision-making"),
    normalization.pred %>%
            mutate(ModelType = "normalization")) %>%
    mutate(ModelType = factor(ModelType, levels = c("representations", "decision-making", "normalization")))

  return(d)
}
```

(ref:AA-plot-results-additional-cases) Predicted adaptation for three types of L2 accents, from top to bottom: contrast reduction, contrast shift and contrast collapse. Predictions were derived for one random experiment of the same number of exposure and test stimuli as in Case Study 2. Multiple simulations were performed following the same procedure described in the main scenario above. **Panel A - Distributions of L1-accented and L2-accented categories:** L1-accented categories, represented in light shades, are kept constant across the three scenarios and identical to that in the main scenario above; L2-accented categories, represented by solid ellipses, are varied across scenarios. **Panel B - Change model predictions:** Predicted categorization accuracies for the L2-accented test tokens after L1-accented and L2-accented exposure, for the best-performing parameterizations of each change model (cf. highlighted panels in Figure \@ref(fig:AA-result-changes-in-representations)-\@ref(fig:AA-result-changes-in-normalization)). The average accuracy across all test tokens for each condition is shown above the bars. Error bars show 95% bootstrapped confidence intervals.

```{r AA-result-comparison-best-three-models-contrast-shift, warning=FALSE}
# Examine a new scenario: contrast shift
min_VOT <- -100 # allows negative VOT when generating exposure and test data
dist.L2_category.cue1 <- 1
dist.L2_category.cue2 <- 0
shift.cue1 <- -1.2
shift.cue2 <- 0
var.ratio <- 1
example_label = "Contrast_shift"
d.AA.results.contrast_shift.best  <- construct_three_models(example_label = "Contrast_shift", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-best-three-models-contrast-reduction, warning=FALSE}
# Examine a new scenario: contrast reduction
min_VOT <- 5
dist.L2_category.cue1 <- 0.5
dist.L2_category.cue2 <- 0
shift.cue1 <- 0
shift.cue2 <- 0
var.ratio <- 1
example_label = "Contrast_reduction"
d.AA.results.contrast_reduction.best  <- construct_three_models(example_label = "Contrast_reduction", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-best-three-models-contrast-collapse, warning=FALSE}
# Examine a new scenario: contrast collapse
min_VOT <- -100 # allows negative VOT when generating exposure and test data
dist.L2_category.cue1 <- 0
dist.L2_category.cue2 <- 0
shift.cue1 <- 0
shift.cue2 <- 0
var.ratio <- 1
example_label = "Contrast_collapse"

d.AA.results.contrast_collapse.best  <- construct_three_models(example_label = "Contrast_collapse", experimenter.ideal_observer = m.io.VOT_f0.AA)
```

```{r AA-result-comparison-additional-cases, warning=FALSE}
cases = c("Cue_reweighting", "Contrast_reduction", "Contrast_shift", "Contrast_collapse")
best_performing_parameters.representations.par = data.frame()
best_performing_parameters.bias.par = data.frame()
best_performing_parameters.normalization.par = data.frame()

for (i in 1:length(cases)){
  example_label = cases[i]
  #load best-performing parameters for representation model
  best_performing_parameters.representations <- readRDS(get_path(paste0("../models/best_performing_parameters.representations_", example_label,".rds")))
  best_performing_parameters.representations.par.current = data.frame(prior_kappa = best_performing_parameters.representations$par[1],
           prior_nu = best_performing_parameters.representations$par[2])
  best_performing_parameters.representations.par %<>% bind_rows(., best_performing_parameters.representations.par.current %>% mutate(Case = example_label)) %>%
    mutate(ModelType = "representations")
  
  #load best-performing parameters for bias model
  best_performing_parameters.bias <- readRDS(get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))
  best_performing_parameters.bias.par.current = data.frame(lapse_rate = best_performing_parameters.bias$par[1],
           beta_pi = best_performing_parameters.bias$par[2])
  best_performing_parameters.bias.par %<>% bind_rows(., best_performing_parameters.bias.par.current %>% mutate(Case = example_label)) %>%
    mutate(ModelType = "decision-making")
  
  #load best-performing parameters for normalization model
  best_performing_parameters.normalization <- readRDS(get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds")))
  best_performing_parameters.normalization.par.current = data.frame(prior_kappa = best_performing_parameters.normalization$par[1])
  best_performing_parameters.normalization.par %<>% bind_rows(., best_performing_parameters.normalization.par.current %>% mutate(Case = example_label)) %>%
    mutate(ModelType = "normalization")
}

```

```{r AA-plot-target-exposure-additional-cases, message = FALSE, warning=FALSE}
# Plot target exposure ellipses.
# Rhis number of tokens are used to create the ellipses for the exposure categories in the 
# plots (a larger number makes the ellipse estimation used in visualization more accurate). 
# It does not affect the number of exposure tokens used in the simulation experiment
n.token_for_plot = 5000 

# simulating a case of contrast shift: like French-accented English stops, so allowing negative VOTs
min_VOT = -100
var.ratio = 1
example_label = "Contrast_shift"
d.AA.exposure.target.contrast.shift <- 
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 1, 
    category_dist_ratio2 = 0, 
    shift_mean_1 = -1.2, 
    shift_mean_2 = 0, 
    exposure.tokens.L1.n = n.token_for_plot, 
    exposure.tokens.L2.n = n.token_for_plot)

# simulating a case of contrast reduction
example_label = "Contrast_reduction"
min_VOT = 5
var.ratio = 1
d.AA.exposure.target.contrast.reduction <- 
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 0.5, 
    category_dist_ratio2 = 0, 
    shift_mean_1 = 0, 
    shift_mean_2 = 0, 
    exposure.tokens.L1.n = n.token_for_plot, 
    exposure.tokens.L2.n = n.token_for_plot)

# simulating a case of contrast collapse
example_label = "Contrast_collapse"
min_VOT = -100
var.ratio = 1
d.AA.exposure.target.contrast.collapse <-  
  make_accent_adaptation_exposure_design(
    experimenter.ideal_observer = m.io.VOT_f0.AA, 
    category_dist_ratio1 = 0,
    category_dist_ratio2 = 0,
    shift_mean_1 = 0,
    shift_mean_2 = 0,
    exposure.tokens.L1.n = n.token_for_plot, 
    exposure.tokens.L2.n = n.token_for_plot) 

# combine all three cases
d.AA.exposure.target.cases <- rbind(
  d.AA.exposure.target.contrast.reduction %>%
    mutate(Case = "Contrast reduction"),
  d.AA.exposure.target.contrast.collapse %>%
    mutate(Case = "Contrast collapse"),
  d.AA.exposure.target.contrast.shift %>%
    mutate(Case = "Contrast shift")) %>%
  mutate(Case = factor(Case, levels = c("Contrast reduction", "Contrast shift", "Contrast collapse")))

# plot ellipses for actual exposure
p.AA.exposure <- d.AA.exposure.target.cases %>%
  mutate(Title = "Exposure distributions") %>%
  filter(Condition == "L2-accented") %>%
  mutate(Group = factor(paste0(Condition, Item.Category))) %>%
  ggplot(aes(x = VOT, y = f0_Mel, alpha = Condition)) +
  stat_ellipse(aes(color = Group), level = .95, geom = "polygon") +
  stat_ellipse(data = d.AA.exposure.target.cases %>%
                 mutate(Title = "Exposure distributions") %>%
                 filter(Condition == "L1-accented") %>%
                 mutate(Group = factor(paste0(Condition, Item.Category))), aes(fill = Group), level = .95, geom = "polygon") +
  scale_x_continuous(expression("VOT (ms)"), expand = expansion(mult = .1, add = 0)) +
  scale_y_continuous(expression("f0 (Mel)"), expand = expansion(mult = .1, add = 0), breaks=seq(0, 500, by = 100)) +
  scale_alpha_discrete(range = c(0.2, 0), guide = "none") +
  coord_cartesian(ylim = c(0, 500)) +
  scale_fill_manual("Category",  values = c(colors.category), guide = "none") +
  scale_color_manual("Category", values = c(colors.category), guide = "none") +
  facet_grid(Case ~ Title, switch = "y") + theme(legend.position="top") 
```

```{r AA-plot-results-additional-cases, fig.width=base.width*4 + .5, fig.height=base.height*3 + .5, fig.cap = "(ref:AA-plot-results-additional-cases)", warning=FALSE}
d.AA.results.best.cases <- bind_rows(
  d.AA.results.contrast_reduction.best %>%
    mutate(Case = "Contrast reduction"),
  d.AA.results.contrast_collapse.best %>%
    mutate(Case = "Contrast collapse"),
  d.AA.results.contrast_shift.best %>%
    mutate(Case = "Contrast shift") 
) %>%
  mutate(Case = factor(Case, levels = c("Contrast reduction", "Contrast shift", "Contrast collapse"))) %>%  
  mutate(parameterization = case_when(
   ModelType == "representations" ~ paste0("kappa = ", prior_kappa, ", nu = ", prior_nu),
   ModelType == "decision-making" ~ paste0("lambda = ", posterior.lapse_rate, ", beta = ", beta_pi),
   ModelType == "normalization" ~ paste0("kappa = ", prior_kappa.normalization)))

p.results.best.cases <- d.AA.results.best.cases %>%
  mutate(
    ModelType = factor(paste0("Changes in ", ModelType), levels = c("Changes in representations", "Changes in decision-making", "Changes in normalization"))) %>%
  mutate(Label = interaction(ModelType, parameterization, sep = "\n")) %>%
  ggplot(aes(x = Condition, y = response, fill = Item.Intended_category, alpha = Condition)) +
  stat_summary(fun = mean,
                 geom="bar", position = pos,
                 width = 0.6) +
  stat_summary(aes(color = Item.Intended_category),
                 fun.data = mean_cl_boot,
                 geom = "uperrorbar",
                 position = pos, width = 0.2) +
  geom_text(inherit.aes = FALSE, data = . %>%
                 group_by(Case, Condition, ModelType) %>%
                 summarise(mAcc = round(mean(response), digits = 2)),
            aes(label = mAcc, x = Condition, y = 1.05), size = geom_text.size) +
  coord_cartesian(ylim = c(0, 1.1)) + 
  scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75, 1)) +
  scale_color_manual("Category", values = colors.category) +
  scale_fill_manual("Category", values = colors.category) +
  scale_alpha_discrete(range = c(0.2, 1)) +
  xlab("Exposure condition") +
  ylab("Predicted \ncategorization accuracy") +
  facet_grid(Case ~ ModelType) +
  theme(legend.position = "top", 
        strip.text.y = element_blank(),
        panel.grid.major.x = element_blank())

prow <- 
  plot_grid(
    p.AA.exposure, 
    p.results.best.cases + theme(legend.position="none"), 
    labels = c('A)', 'B)'), 
    ncol = 2, 
    rel_widths = c(1.2,3.3))

# extract a legend that is laid out horizontally
legend_prow <- get_legend(p.results.best.cases)
plot_grid(legend_prow, prow, ncol = 1, rel_heights = c(.2, 1)) 
```

```{r AA-table-best-performing-parameterization}
parameters.results.best.cases <- 
  bind_rows(
    best_performing_parameters.representations.par, 
    best_performing_parameters.bias.par, 
    best_performing_parameters.normalization.par) %>%
  mutate(Case = gsub("_", " ", Case)) %>%
  mutate(parameterization = case_when(
   ModelType == "representations" ~ paste0("$\\kappa_{c,0}$ = ", round(prior_kappa, digits = 0), ", $\\nu_{c,0}$ = ", round(prior_nu, digits = 0)),
   ModelType == "decision-making" ~ paste0("$\\lambda$ = ", round(lapse_rate, digits = 2),   ", $\\beta_\\pi$ = ", round(beta_pi, digits = 2)),
   ModelType == "normalization" ~ paste0("$\\kappa_0$ = ", round(prior_kappa, digits = 0)))) %>%
   mutate(
      ModelType = factor(paste0("Changes in ", ModelType), levels = c("Changes in representations", "Changes in decision-making", "Changes in normalization"))) %>%
    select(Case, ModelType, parameterization) %>%
    distinct() %>%
    pivot_wider(names_from = ModelType, values_from = parameterization)

colnames(parameters.results.best.cases) = c("L2 accent scenario", "Changes in representations", "Changes in decision-making", "Changes in normalization")

knitr::kable(parameters.results.best.cases, caption = "Best-performing parameterizations across different accent scenarios. Recall that best-performing here refers to the parameterization that achieves the highest accuracy on L2-accented test tokens, rather than best quantitative fit against human data (which we do not have).", align = 'l', booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = "scale_down")
```

Figure \@ref(fig:AA-plot-results-additional-cases)B shows how the three accent scenarios are predicted to affect L1 listeners' perception, both prior to informative exposure (L1-accented exposure condition) and after L2-accented exposure. Each panel shows the predictions of the best-performing parameterization for the change model, derived in the same way as for the cue-reweighting case described above (best-performing parameters are listed in Table \@ref(tab:AA-table-best-performing-parameterization)). The results of the three additional simulations serve to emphasize several important considerations for future research. 

Foremost of all, Figure \@ref(fig:AA-plot-results-additional-cases)B shows that the predictions of the three change mechanisms *can* differ qualitatively, depending on the accent scenario. While all three change models achieve qualitatively similar accuracy for two of the three simulated accent scenarios (top and bottom row of Figure \@ref(fig:AA-plot-results-additional-cases)B), this is not the case for our example of contrast shift (middle row). For this simulated scenario, changes in category representations and normalization can yield qualitative improvements after exposure, whereas changes in decision-making do not. This shows that experiments on accent adaptation *could* shed light on the underlying change mechanisms *provided experimenters were to design and analyze their experiments in ways that explicitly link the acoustic-phonetic properties of the exposure and test items to participants' responses* (e.g., by selecting accent properties that are predicted to yield different adaptive responses under the three different change mechanisms). This is, however, hardly ever attempted in research on accent adaptation. The few existing exceptions, while insightful in their own right, have been limited to informal discussions of how the acoustic-phonetic properties of L1 and L2 accents might have qualitatively contributed to the results, rather than explicit comparisons of models [e.g., @schertz2015; @tan2021; @xie2016jep]. In the general discussion, we make concrete recommendations as to how the field can move beyond this status quo.

Before we turn to these recommendations, we make two more observations about the results in Figure \@ref(fig:AA-plot-results-additional-cases)B that might be of interest to researchers who plan to use ASP to design and interpret their experiments. First, accent scenarios can differ in the difficulties they are predicted to cause for L1 listeners prior to informative exposure. For example, without L2-accented exposure, contrast reduction (top row) and contrast collapse (bottom row) are predicted to cause difficulties similar to those predicted for cue reweighting: L1 listeners unfamiliar with the accent are expected to struggle with the recognition of /d/ than the recognition of /t/. The opposite is observed for our example of contrast shift (middle row). This illustrates how different L2 accent properties can lead to different perceptual consequences for L1 listeners. 

Second, the three scenarios in Figure \@ref(fig:AA-plot-results-additional-cases)A also differ in how much L2-accented exposure is predicted to affect perception. This includes differences across the accent scenarios in terms of the maximum accuracy any particular change model can achieve. For example, changes in category representations can improve recognition accuracy to >90% for L2-accented exposure for both the example of cue reweighting in Case Study 2 and the example of contrast shift in Figure \@ref(fig:AA-plot-results-additional-cases)A. For our example of contrast reduction, however, the improvements achievable by changes in category representations are more modest (~72% after L2-accented exposure). There are even accent scenarios for which _none_ of the change models predicts any noteworthy benefits from L2-accented exposure (see bottom row of Figure \@ref(fig:AA-plot-results-additional-cases)B). This highlights an important, yet often under-appreciated, fact: sometimes null results are _predicted_ because of the specific stimulus statistics [see @tan2021; @zheng-samuel2020]. Put differently, an absence of improvements does not _necessarily_ constitute evidence against accent adaptation or any of the three mechanisms. 

The scenarios considered here only cover a subset of the L2 accent properties that exist in the world. They suffice, however, to illustrate a general challenge: different types of cross-talker differences---including L2 accents---pose different types of challenges to listeners (this is further reflected in the fact that the best-performing parameterizations for the same mechanism vary across the different accent scenarios, cf. Table \@ref(tab:AA-table-best-performing-parameterization)). It follows that, depending on the specific accent properties, listeners can benefit from different engagement of the three mechanisms [and different rates of adaptation, see discussion of the flexibility-stability trade-off in @kleinschmidt-jaeger2015, pp. 180-182]. This means that differences between accents can be a natural testbed for distinguishing between hypotheses about the change mechanisms affording adaptive speech perception. In the general discussion, we explore how such tests can be accomplished.

<!-- In summary, the expected benefits of L2-accented exposure under each change mechanism depend on the specifics of the L2-accented categories (and how they relate to L1 listeners' prior expectations). Beyond the accent scenarios examined here (cue-reweighting, contrast reduction, contrast shift, and contrast collapse), naturally produced L2 accents exhibit complex variations, which can pose difficulties to L1-listeners and researchers alike. At the same time, these differences may provide a useful opportunity to contrast the three change mechanism or even to investigate their *relative* engagement in adaptive speech perception.  -->

