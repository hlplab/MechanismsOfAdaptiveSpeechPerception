# General discussion {#sec:general-discussion}
Despite much progress in identifying properties of adaptive speech perception, the mechanisms underlying this ability remain poorly understood. Many lines of research continue to either eschew questions about the mechanisms altogether (focusing instead on the identification of properties) or *assume*, rather than test, one of three types of mechanisms we discussed. As reviewed in the introduction, contrastive (non-confirmatory) tests of competing hypotheses about the nature of these mechanisms continue to be lacking. This raises the question of whether findings that have been---implicitly or explicitly---assumed to support one of these mechanisms are, in fact, compatible with *any* of the mechanisms. Our case studies find that the signature results from two lines of research---perceptual recalibration and (non-native) accent adaptation---are indeed qualitatively compatible with any of pre-linguistic signal normalization, changes in category representations or changes in post-linguistic decisions, and thus do not distinguish between these hypotheses. Contrary to common beliefs, low-level, pre-linguistic signal transformation/normalization or changes in response biases during post-linguistic decision-making can be sufficient to explain signature results that have previously been tied to changes of representations. While we have focused here on two specific types of paradigms, the general take-home points of our case studies are likely to generalize to other types of exposure-test paradigms, such as distributional learning paradigms over a single phonetic cue [@clayards2008; @kleinschmidt-jaeger2016cogsci], a variety of paradigms employed in the study of adaptation to dialectal or L2 accents [for reviews, see @baeseberk2018], and---at least to some extent---dimension-based statistical learning paradigms that manipulate the relative informativity of two or more cues [e.g.,  @idemaru-holt2011; @idemaru-holt2020].^[There are, however, important differences in the extent to which different lines of research have already implemented the recommendations we make below. For example, some lines of research have long started to interpret their findings more explicitly relative to the distribution of phonetic cues in the speech input, and to use more fine-grained analyses [e.g., studies explicitly framed in terms of distributional learning, @bejjanki2011; @clayards2008; @kleinschmidt-jaeger2016pbr; @malone2022; for review, see @schertz-clare2020], whereas this remains the exception in other lines of research [but see, e.g., @hitczenko-feldman2016; @tan2021 for accent adaptation; @theodore-monto2019 for perceptual recalibration]. These paradigms are also discussed in more depth in Section \@ref(sec:computational-limitations).] <!-- TO DO: If this section is moved into SI, change reference here to SI -->

This indeterminacy of existing results is even more problematic when we take into account that research often seeks to extrapolate beyond specific paradigms. For example, one common motivation for perceptual recalibration and other paradigms that employ synthesized or otherwise manipulated speech is that they provide experimenters with increased control over the stimuli, and yet can shed light on the mechanisms that underlie adaptation to naturally occurring accents. However, this type of argument--—which we have also made in our own work--—requires certainty that the different paradigms examine the same underlying mechanism(s) [for related discussions, see @zheng-samuel2020]. One take-home point of our case studies is that currently the field is not well-equipped to achieve such certainty. <!--In fact, we know even less about what mechanisms yield the results of our experiments than seems to have often been assumed. And this means that there is value in extending future applications of these paradigms in ways that allow researchers to determine which mechanism drives their results. The recommendations we present in the next section are meant to move us closer to this goal.--> 
This does not, of course, mean that normalization, representation, and decision-making mechanisms always predict the same response patterns---they do not (as we illustrate below in Section \@ref(sec:methodological-advances)). Nor does it mean that the three mechanisms cannot be distinguished from each other through behavioral experiments. Rather, it means that we now know each mechanism can be _sufficient_ to explain human behavioral response under some stimulus and task conditions. We still lack an answer to the question: how can we contrast the mechanisms of speech adaptation?

In what follows, we discuss how future research can take advantage of the qualitative limitations of the two computationally more parsimonious change models (normalization and changes in decision-making). Given these limitations, it is possible to conduct experiments that can decisively reject the hypotheses that normalization or changes in decision-making are *sufficient* to explain adaptive speech perception. However, as already anticipated in the introduction, it is also possible---if not likely---that all three types of change mechanisms contribute to adaptive speech perception. Ultimately, it will thus be important to assess how *combinations* of the three mechanisms drive adaptive speech perception, and how the relative engagement of the different mechanisms depends on, for example, stimulus properties (e.g., whether an L2 accent involves simple shifts relative to the L1 accent, or the learning of new features), task demands (e.g., attentional load), and individual differences between listeners. This enterprise, we suspect, will benefit from---if not require---quantitative model comparisons, for example, within the ASP framework. We thus discuss recommendations as to how future experiments can facilitate such comparisons.

## Computational limitations of change models afford qualitative tests of their *sufficiency* {#sec:computational-limitations}
The three change models---as well as the three eneral hypotheses that they aim to implement---differ in their computational parsimony. For example, while both normalization and the representational change model assume that listeners are sensitive to talker-specific changes in the usage of phonetic cues, only the latter tracks those differences separately for each category (e.g., /d/ vs /t/). This makes normalization computationally more parsimonious than representational changes for both researchers and listeners: for the normalization model, the number of parameters that researchers need to determine (e.g., fit from the behavioral data of perception experiments), and the number of estimates that listeners need to infer and store from the speech input does not increase with the number of categories [for related discussion, see also @apfelbaum-mcmurray2015]. <!-- TO DO: Xin, this was stating something wrong. I hope it's now clearer --> For example, for the C-CuRE-based normalization model employed in our case studies, researchers need to determine only one parameter ($\kappa_0$) and listeners are assumed to infer and store only the *overall* means of all cues ($K$ values for $K$ cues). In contrast, the representational change model employed in our case studies requires researchers to determine two parameters *per category*, and listeners are assumed to infer and store the cue means and covariance matrices of *each category* ($JK + \frac{J}{2}(K^2+K)$ values for $K$ cues and $J$ categories).^[Even if C-CuRE normalization, which only centers cues, is made more comparable to the representational change model by extending normalization to include cue standardization [also known as z-scoring, as used in, e.g., Lobanov normalization, @lobanov1971], this would introduce just one additional parameter for researchers (the equivalent for $\kappa_0$ but for the estimation of the cue variances), and listeners would be assumed to learn and store $2K$ values for $K$ cues.] <!-- TO DO: Xin, Chigusa: I moved this into footnote. It felt like an aside. ok? --> 

While the parsimony of normalization offers a computational advantage in terms of efficiency, it also comes with limitations. We discuss three such limitations that future work can exploit to evaluate the sufficiency of this mechanism. First, normalization accounts predict that the effects of exposure on subsequent perception do not depend on the category membership inferred by listeners.<!--^[To be precise, some normalization accounts allow the category identities of *surrounding* segments of speech to affect the normalization of cues on the target segment. This is, for example, how C-CuRE normalizes for effects of surrounding phonetic context [@mcmurray-jongman2011].<!-- This makes me wonder how they explain the Holt findings that a sine tone can affect subsequent perception. Do McMurray & Jongman even discuss this finding? [CK] They do. They actually say that C-CuRE builds on the type of "auditory contrast accounts" driven by any surrounding speech. They suggest that the phonetic context can be used but are not always needed. e.g., "C-CuRE also builds on auditory contrast accounts (Lotto &Kluender, 1998, Holt, 2006; Kluender, Coady & Kiefte, 2003) by proposing that cues are interpreted relative to expectations, though these expectations can be driven by categories (perhaps in addition to lower-level expectations)"(Section 5.5)This does not, however, affect the point we make here.]--> For example, for C-CuRE, only the overall cue mean of the input can affect subsequent perception, regardless of whether the lexical context labels the input as one category or another. Although not originally discussed in the context of normalization, there is evidence that challenges this prediction. In their seminal study on perceptual recalibration, @norris2003 exposed participants to /f/- or /s/-biased inputs using the same general design that we discussed for Case Study 1. The critical conditions exposed participants either to words with typical /f/ and words with atypical (shifted) /s/ or to words with typical /s/ and words with atypical (shifted) /f/. This resulted in the signature perceptual recalibration effect.
Importantly though, no boundary shift was observed in control conditions where the shifted tokens were embedded in non-words instead of real words, everything else being identical. In short, the control conditions differed from the experimental conditions almost exclusively in whether the atypical inputs were lexically labeled to be of a particular category.<!--Importantly though, Norris and colleagues also included several control conditions. Two control conditions of relevance to the present discussion exposed participants either to words with typical /f/ and *non*-words with atypical (shifted) /s/ or to words with typical /s/ and *non*-words with atypical (shifted) /f/. The atypical /f/ and /s/ sounds in these control conditions were acoustically identical to those in the experimental conditions, and the non-word contexts in the control conditions matched the word contexts in terms of the phonetic context surrounding the critical /f/ and /s/ sounds (specifically, in terms of lexical stress and the vowel immediately /f/ or /s/). In short, the two control conditions differed from the experimental conditions almost exclusively in whether the atypical inputs were lexically labeled to be of a particular category. Unlike the experimental conditions, however, the control conditions did *not* elicit the signature boundary shift [@norris2003, Experiment 2].--><!-- As Norris and colleagues concluded, "[...] perceptual learning depended on exposure to an ambiguous speech sound in lexically biased contexts" (p. 227).--> The perceptual recalibration effect thus seems to depend on the category that the shifted atypical tokens are attributed to [@norris2003, p. 227]---contrary to what would be expected if adaptive speech perception was solely achieved through cue-level normalization.<!-- ^[Later work found that the label (biasing information) does not have to be provided by the *lexical* context. For example, visual information about lip position [@vroomen2007] can induce perceptual recalibration, too. Neither do the shifted atypical tokens have to be precisely "ambiguous" (i.e., located at the prior category boundary). For example, smaller shifts from the prior category mean towards the category boundary can also elicit (smaller) boundary shifts [@babel2019]. But this does not affect the take-home message of Norris et al.'s (2003) finding.]--> The fact that this finding was obtained for (Dutch) fricatives is of particular interest since C-CuRE normalization has been found to provide a good fit against the recognition of (American English) fricatives. While most of the tests of C-CuRE were non-contrastive [e.g., @mcmurray-jongman2011; @mcmurray-jongman2016], @apfelbaum-mcmurray2015 compared C-CuRE to a representational change model (specifically, exemplar models) and found very similar performance, with the latter providing a slightly better fit against human responses. <!-- TO DO: Xin or Chigusa: is this (preceding sentence) what you meant? I tried to make it more explicit. --> We consider the question of how labeling information affects adaptive speech perception as a productive venue for future research. 

A second computational limitation of normalization accounts is specific to accounts that only correct for differences in the overall mean of cues but not differences in cue variability (like C-CuRE). <!--Such normalization accounts seem to be in conflict with existing findings, although these findings have to the best of our knowledge not previously been discussed in these terms. -->In a ground-breaking study that was targeted at a separate question, @clayards2008 exposed participants to distributions of synthesized /b/ and /p/ tokens (as in, e..g, *beach*-*peach* continuum). Between participants, the VOT of these tokens had been manipulated to either form wide or narrow VOT distributions for both /b/ and /p/. The VOT means of /b/ and /p/ were identical in both conditions<!-- (0 and 50 msecs, respectively)-->. Since acoustic cues are encoded relative to the cue mean only in C-CuRE and similar normalization accounts, these accounts thus do *not* predict any differences in the effects of these exposure conditions. Clayards and colleagues, however, found that participants in the wide variance condition exhibited shallower categorization functions along the VOT continuum [conceptually replicated in @nixon2016]---as predicted by representational change models [see @clayards2008; @kleinschmidt-jaeger2015; @theodore-monto2019]. Future research should test whether this finding replicates for more natural-sounding (rather than resynthesized, robotic-sounding) stimuli and in situations where task demands more closely resemble those of everyday speech perception (less repetition, more lexical heterogeneity, words presented in sentential contexts rather than in isolation, etc.). <!-- TO DO: was Theodore & Monto not natural sounding? I think it was. So then the first part of this has been done. Let's discuss. --> If such replications are obtained, this would argue that normalization would at least have to include standardization or similar corrections for the variability of cues [as proposed in @johnson2020; @lobanov1971; @monahan-idsardi2010].

A third limitation of normalization accounts that can be productively explored in future research is the lack of category specificity. Consider for example, a possible extension of the study by Clayards and colleagues. In the study by @clayards2008, both /b/ and /p/ either had narrow variance along VOT (SD = 8 msecs) or wide variance (SD = 14 msecs). This confounds the category-specific variance with the overall variance of the cues along VOT. It is, however, possible to manipulate the variance of the two categories while keeping both the means of the two categories (and thus the overall cue mean) and the overall cue variance constant. Figure \@ref(fig:proposed-experiment-asymmetric-variance) depicts two exposure conditions that achieve this, along with changes in the categorization function predicted by normalization and representational change models. This demonstrates how researchers might use the lack of category-specificity to disentangle normalization and representational change accounts. Other possible manipulations in this vein could include category-specific changes in the *co*variance of cues.<!--^[This is not to be confused with another experimental manipulation that is sometimes describes in terms of covariance [@REF]: changes in which of two or more cues exhibits the largest relative distance between two category means [which changes the relative reliability of cues, as discussed in @schertz-clare2020]. ASP-based simulations would be required to assess whether such manipulations cannot also be explained by normalization. We suspect that they can.]--> Basically, any manipulation that leaves the *overall* cue mean and variance unaffected while predicting differences in the categorization function under the representational change model can test whether the human listeners exhibit more flexibility than expected by normalization accounts. We note, however, that any such test should take into account that listeners can have strong prior beliefs based on the speech input they have received previously, and that this might include strong prior beliefs about *how* the realization of categories varies across talkers [@kleinschmidt-jaeger2015]. If the manipulations employed by researchers strongly violate those expectations, this needs to be carefully considered in the derivation of predictions. 


(ref:proposed-experiment-asymmetric-variance) A possible way to contrast normalization and representational change accounts. Panel A: two exposure conditions with identical overall means and variances along VOT, but different category-specific variances. Panel B: the predictions of the best-performing normalization and representational change models. For this purpose, the normalization model was extended to both center and standardized the cues.^[We note that the specific predictions for the representational change model shown here assume that listeners have equally strong prior beliefs about the variance of both of the two categories (i.e., one $nu_{0,c}$ for both categories). This is what we assumed in our case studies---which aimed to show that even simplified versions of each change model can explain a wide range of findings in the literature---but it is not an inalienable assumption for representational change models [for discussion, see also @kleinschmidt-jaeger2015]. <!-- TO DO: can we find the page number where K&J discuss separate nus for each category and add it here? --> The general prediction of representational change models that listeners are sensitive to the category-specific variance should hold even if this assumption is removed. How easy to detect this predicted difference is, however, expected to depend on this assumption.] <!-- TO DO: Xin, any chance you could add such a figure. for this we can just perfectly center and standardize (i.e., kappa_0 and the newly introduced nu_0 are both 0); the best performing representational change model should be nu_0,c = 4 and kappa_0,c = 1024, assuming you put the category means on the a priori expected locations. -->

```{r proposed-experiment-asymmetric-variance, fig.cap="(ref:proposed-experiment-asymmetric-variance)"}
# Put figure here.
```

<!--Just like normalization accounts are limited in the types of changes they can account for, so are change models for decision-making. -->In Section \@ref(sec:framework), we showed that the limitations of change models for  decision-making are less well understood than sometimes assumed. One recommendation for future research is thus to further explore the mathematical limitations of decision-making change models. Designs that limit attentional lapses to basically zero [e.g., by employing more engaging tasks, as in gamified paradigms, @wade-holt2005; @lim-holt2011] would emphasize the computational limitations of decision-making change models. In such scenarios, changes in response biases can only explain changes that are additive in the posterior log-odds of categories (Section \@ref(sec:change-bias)). That is, changes in response biases cannot account for changes in the *slope* of categorization functions. Future studies should use exposure conditions for which such changes are predicted by representational or normalization models to test whether changes in response biases are sufficient to explain adaptive speech perception<!-- [e.g., if zero lapses had been observed in @clayards2008, which was, however, not the case]-->. Since it can be difficult to detect changes in categorization slopes---especially without making strong linearity assumptions, we suspect that this question is better explored by manipulating the relative reliability of two cues (see also Figure \@ref(fig:show-model-categorization-3D-plots)). Such manipulations are routinely used in a paradigm known as "dimension-based statistical learning" [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015]. With recent trends towards analyses that more transparently link phonetic cues to changes in participants' responses [@idemaru-holt2020; @schertz-clare2020, see also recommendation XXX in Section \@ref(sec:methodological-advances)]  <!-- TO DO: fill in --> this field of study is in a good position to test the sufficiency of changes in decision-making accounts. An ongoing project from one of our labs, uses ASP-like simulations in combination with experimental designs that are intended to directly address this question [@burchill-jaeger2022].

Conversely, there are ways to assess whether representational changes alone are sufficient to explain all forms of adaptive speech perception. For example, if auditory input that arguably carries no information about category statistics affects subsequent speech perception, this provides evidence that changes in representations alone cannot explain all aspects of adaptive speech perception. Perhaps one of the most convincing demonstrations of this type comes from the findings of auditory enhancement effects, wherein non-speech stimuli (e.g., pure tones) can systematically alter the perception of subsequently played speech stimuli [@chodroff-wilson2020; @holt2001; @holt2005; @holt2006; @huang-holt2011; for review, see also @weatherholtz-jaeger2016]. While these demonstrations might be challenged for lack of ecological validity (potentially inviting meta-reasoning about experimenters' intentions that is unlikely to be present during everyday speech perception), it is unclear how representational changes---or, for that matter, changes in response biases---can explain such findings. At the very least then, these findings suggest that normalization *can* be involved in adaptive speech perception.

In sum, by focusing on the computational assumptions of the different change models, it is possible to conduct behavioral experiments that can decisively determine whether either of the two computationally more parsimonious change models is *sufficient* to explain adaptive speech perception, or whether changes in representations are necessary to explain adaptive speech perception. 

## Beyond sufficiency tests: Studying how *combinations* of mechanisms contribute to adaptive speech perception
<!-- So far we have discussed questions about the competing mechanisms as an either-or question. This reflects our impression that the majority of research on adaptive speech perception continues to be conducted in separate lines of research, each focusing on a single mechanism at a time. However, --> As anticipated in the introduction and discussed next, both theoretical considerations and existing evidence suggest that adaptive speech perception can be the result of a *combination* of mechanisms. This means that future work needs to find effective ways to study how normalization, changes in representations, and changes in decision-making jointly explain adaptive speech perception, and what factors determine the relative engagement of each mechanism. 

<!-- TO DO: the next para can need some fine-tuning. The goal here is to spell out what is anticipated in the preceding para. -->
In addition to differences in cognitive and memory demands, the three mechanisms might differ in the amount of input that is required for successful adaptation. For example, less speech input is required to arrive at a precise estimate of a talker's *overall* cue mean (required for normalization) than for precise estimates of the talker's category means and covariance matrices (required for changes in category representations). This makes representational changes comparatively slow and taxing in terms of memory demands. On the other hand, changes in decision-making do not require any storage of acoustic-phonetic details, which might make them cognitively less demanding. It is possible that L1 listeners who encounter an unfamiliar accent initially resort to the decision-making changes to boost their recognition accuracy. As listeners accrue further input from a talker or accent, they may begin to rely more on the other two mechanisms.^[It is also possible that listeners engage in all three mechanisms to the same extent but that researchers can only *detect* the consequences of those mechanisms after different amounts of exposure. Future studies on this matter should take this alternative possibility into consideration.] 

If the relative engagement of the different mechanisms is affected by cognitive demands, this also raises questions about the extent to which findings from less taxing paradigms generalize to everyday speech perception. For example, the complexity of natural accents typically vastly exceeds the types of phonetic manipulations employed in experiments on "perceptual recalibration" [e.g., @norris2003; @kraljic-samuel2005; @vroomen2007], "dimension-based statistical learning" [e.g., @idemaru-holt2011; @idemaru-holt2020; @lehet-holt2020; @liu-holt2015], or "distributional learning" [e.g., @clayards2008; @maye2002]. In real life, speech input also rarely consists of clearly enunciated, isolated words (as used in many types of paradigms), introducing additional segmentation uncertainty but also affording additional contextual information about category identity [@luthra2021]. Finally, individual differences between listeners' perceptual or cognitive resources might result in greater or less reliance on different mechanisms---an important consideration, for example, in developing and choosing clinical interventions [@scharenborg2015].

Two complementary approaches seem particularly well-suited to support future research on these questions. The first is neuroimaging, the second is computationally guided behavior research that draws on frameworks like ASP. Neuroimaging research has found that adaptive changes in speech perception as a function of recent exposure can involve a number of different brain regions, including: early auditory regions [anterior planum temporale, @kilianhutten2011; superior temporal gyri @myers-blumstein2008] and regions responsible for representing phonemes and syllables [e.g., posterior  superior temporal gyri/superior temporal sulcus, @bonte2017; @myers-mesite2014; @ullas2020], as well as regions for talker recognition [right temporal regions, @luthra2020a]. The left parietal lobe and the insula, which are implicated in perceptual decision-making [e.g., @dacremont2013; @keuken2014], have also been shown to exhibit distinct activation for different exposure conditions.

However, existing neuroimaging results do not always single out neural networks that are responsible for the behavioral changes in response to recent exposure. Activation of a given brain region can be associated with functionally-distinct sources. For instance, the activation of left parietal lobe might reflect its general role in perceptual decision-making [e.g., @dacremont2013; @keuken2014], or it could be due to a more specific role in phonological processing [e.g., processing abstract category information, @guediche2014]. One limitation of these works is that their conclusions are primarily based on a binary distinction in a brain region’s activation pattern (e.g., activity between categorization trials that a /d/ response is made and those in which a /t/ response is made for the same stimulus). More recently, neuroimaging studies have started to approach questions about the underlying neural mechanisms through multivariate analyses. These analyses are crucial for identifying the encoding of perceptual experiences across a distinct array of experimental conditions and have been fruitful in distinguishing between cognitive models that make different predictions regarding neural representations [e.g., in response to noise-vocoded speech, @Blank2016]. Multivariate analyses are also more sensitive in detecting fine-grained patterns *within* regions responsible for distinct cognitive demands [e.g., @bonte2017; @luthra2020a]. <!--The use of multivariate analyses begins to address concerns that differences in how information is neurally coded in, e.g., lower-level perceptual areas vs. higher-level decision-making areas might make it systematically more difficult to *detect* exposure-induced effects in some brain regions.--><!--TO DO: I removed this sentence for now because I am not sure how multivariate analyses help to address this noted issue.-->

Another promising avenue is to pair temporally-sensitive techniques with imaging methods with good spatial resolution. For instance, studies using a combination of EEG and MEG have identified key regions that regulate perceptual learning of degraded speech by responding to manipulations of either low-level signal clarity or high-level prior knowledge of the speech content [@sohoglu-davis2016; @sohoglu-davis2020].
<!--Neuroimaging also offers a powerful tool for future research to study how the engagement of different mechanisms depends on task demands and affordances, stimulus properties, or individual differences between listeners.-->

Computationally guided behavioral research offers an complementary (and potentially less expensive) approach to these questions. Models based on ASP or similar frameworks can be fit to data from behavioral experiments, while simultaneously modeling the effect of all three mechanisms. Quantitative model comparison can then be used to determine the relative engagement of different change mechanisms, and how this engagement changes across different experimental conditions or between different types of listeners. In the next section, we close with recommendations as to how behavioral research can support such efforts.

## Methodological advances to facilitate more informative model comparisons {#sec:methodological-advances}
<!-- As we have discussed, contrastive tests of competing hypotheses have often been lacking from the study of adaptive speech perception---in particular, for behavioral research. There is certainly value to identifying the empirical properties of behavioral phenomena, and adaptive speech perception is no exception to this. However, something is arguably amiss when the majority of experiments in the field do not even *aim* to distinguish between fundamentally different cognitive architectures. When conclusions about mechanisms are drawn based on the findings of studies, this is often based on ad-hoc, or even post-hoc, reasoning rather than theoretical models that are sufficiently specified to be rejectable. This is problematic in particular for research questions that easily escape researchers' intuitions, as is the case for the domain of adaptive speech perception. 

Misunderstandings based on ad-hoc reasoning can stand unchallenged for significant periods of time. Consider the highly influential study “The Weckud Wetch of the Wast” [@maye2008]. The study exposed listeners to speech in which the vowel categories had undergone systematic (phonological) shifts. After just 20 minutes of exposure, listeners’ interpretation of subsequent speech input from the same talker had changed substantially [for replications and extensions, see also @weatherholtz2014]. Based on additional control conditions, Maye and colleagues reasoned that this finding could not be explained by “general relaxation of the criterion for what constitutes a good exemplar of the accented vowel category” (i.e., category expansion) but rather argued that participants had learned that the talker’s categories were systematically shifted. A recent study [@hitczenko-feldman2016] revisited this result by formalizing the competing accounts within the same distributional learning framework for changes in representations that we have integrated into ASP [@kleinschmidt-jaeger2015]. Hiczenko and Feldman found that the results of Maye et al. (2008) were just as well, if not better, predicted by a learning model that expanded the categories, rather than shifting them. We do not believe that this type of issue is particular to Maye et al.'s study. Rather, this happens to be one of the few studies for which later research revisited the reasoning provided in the original study, formalized it sufficiently to make it testable, and evaluated it.--> 

<!--TO DO consider implementing sth like this here or in the penultimate para of the preceding section: In our experience, the use of even simple models also facilitate deeper engagement with the results of our experiments, including a clearer understanding of which results ought to surprise us and which ought not to (see also apfelbaum-mcmurray2014?; Hitczenko and Feldman 2016; Kleinschmidt and Jaeger 2011; Schertz and Clare 2020; Tan, Xie, and Jaeger 2021). We have begun to use this approach in several ongoing projects, and have found it to be highly insightful. Simulations based on the hypothetical or actual phonetic properties of stimuli have allowed us to detect flaws in our reasoning prior to conducting the experiment, and to adjust designs accordingly. 
Furthermore, the framework we have introduced here can be used to inform comparisons across experiments, tasks, types of phonetic contrasts, and even languages (see Tan, Xie, and Jaeger 2021). One common approach to gain generalizable insights into a given mechanism is to examine it using multiple accents/languages (e.g., experiments with similar designs using Dutch-accented (Eisner et al., 2013) and Mandarin-accented (Xie et al., 2017) English). Researchers also attempt to contrast multiple paradigms (e.g., selective adaptation vs. perceptual recalibration vs. accent adaptation paradigms) in terms of how long-lasting the effects obtained in each of them may be (Samuel et al., 2021). One major caveat associated with these approaches is that it is not trivial to equate all aspects of experiments in a way so that a fair and meaningful comparison is possible. Even when researchers try to control everything but one factor—be it a particular phoneme contrast or a paradigm—they often come with a host of differences such as the exact acoustic features of the stimuli and their distributions. Given everything that is now known about speech perception, these distributional properties of both prior input and the present input will be critically and jointly affecting the listeners’ behavior. With this in mind, frameworks like the one presented here can also be used predictively prior to conducting experiments. This could include power simulations for existing designs, or more active guidance for the choice between different designs (e.g., the choice of exposure and test stimuli). [JF5]-->


### Recommendation 1: *Pre*diction based on sufficiently constraining theories instead of (only) informal reasoning
<!-- Reliance on intuitions, in particular if formed ad- or post-hoc, both risk prematurely interpreting differences in *results* to differences in mechanisms. For example, in the absence of clear linking hypotheses, researchers might find that exposure leads to significantly improved comprehension with regard to one behavioral measure (e.g., processing speed) but not another measure (e.g., recognition accuracy) or otherwise find that the improvements reflected in two different behavioral measures to do not correlate, and then conclude that the two measures reflect different mechanisms [@floccia; @zheng-samuel2020]. Or researchers might fail to find intuitively expected results---such as improvements after exposure to an L2 accent---and take those null results to point to limits of adaptive speech perception [@REF]. However, even successful adaptation can be *predicted* to not result in changes in behavior. -->

Our first recommendation is to *embrace computational models and use them to derive predictions*. <!--To anticipate the response we have received from some of our colleagues in the field, we are *not* recommending that This does not intend to recommend that everyone should become a computational modeler. However, just as computational researchers read experimental research and use it to inform their models, experimental psychologists and neuroscientists can benefit from increased familiarity of the computational models that are directly relevant to their research. And *using* a theoretical model to, for example, derive predictions under different assumptions can be a particularly effective way to gain a deeper understanding of the model's workings. In our own experience, the use of simulations has repeatedly identified gaps or mistakes in our understanding of how different hypothesized change mechanisms might drive adaptive speech perception.-->

Formal models of normalization have now existed for decades, and modern variants of them are no more complex than standard data analysis [e.g., linear regression for C-CuRE, @mcmurray-jongman2011]. Fully specified distributional learning models have been available for about a decade [e.g., exemplar models, @REF; ideal adaptors, @kleinschmidt-jaeger2015], along with freely available R libraries that implement them (e.g., \texttt{beliefupdatr}, Kleinschmidt, 2015). ASP implements these and  decision-making change models, and the R markdown code that this document is generated from can be used to derive predictions for future experiments. ASP and other analytical frameworks thus provide a readily available alternative to informal reasoning about changes in speech perception, and can be a powerful tool for principled hypothesis testing [see also @apfelbaum-mcmurray2015; @sohoglu-davis2016; @tan2021; @theodore-monto2019; @toscano2018; @xie2021cognition]. ASP and similar frameworks can be used to derive quantitative predictions based on particular linking hypotheses about mappings between acoustic cues and speech categories. ASP can be used, for example, to predict *when*, *to what acoustic-phonetic inputs* and *after how much exposure* listeners would show adapted responses under the three mechanistic hypotheses. We provide an initial demonstration of this next.

In the case studies, we demonstrated that the ASP change models for all three mechanisms can qualitatively account for the signature results of perceptual recalibration and accent adaptation paradigms. Beyond this, ASP also shows that the different change mechanisms can be distinguished behaviorally. Figure \@ref(fig:show-model-categorization-3D-plots) illustrates this point for the data from Case Study 2. For each of the three change models, we take the parameters that resulted in the highest post-L2-exposure accuracy during test, and plot the predicted categorization functions for both exposure conditions. These are the categorization functions that a listener would employ while categorizing test stimuli. Despite the fact that the three change models resulted in qualitatively similar categorization accuracy in Case Study 2, the three models differ in *how* they achieve this accuracy. Specifically, they predict different categorization functions over the VOT-f0 space. 

(ref:show-model-categorization-3D-plots) The three change models predict different categorization functions when applied to the data from Case Study 2. From **left to right:** predictions for in changes in representations, decision making, and normalization. **Top:** Predicted categorization functions after L1-accented exposure. **Middle:** Same but after L2-accented exposure. **Bottom:** Differences in predicted posterior log-odds of /d/ between the two exposure conditions. Blue indicates higher predicted posterior log-odds of /d/ in the L2-accented exposure condition, relative to the L1-accented exposure condition. Red indicates the opposite. Gray indicates a difference of 0. The three models make distinct predictions about how exposure affects the perception of specific tokens across the VOT-f0 space.

```{r compare-AA-models-results-across-three-mechanisms, warning=FALSE}
d.AA1 <- 
  d.AA.representations %>%
  filter(category == Item.Intended_category) %>%
  filter(prior_kappa == 4 & prior_nu == 1024) %>%
  droplevels() %>%
  distinct(Condition, Subject, data, prior_kappa, prior_nu, prior, posterior, posterior.categorization)

d.AA2 <- 
  d.AA.bias %>%
  filter(category == Item.Intended_category) %>%
  filter(posterior.lapse_rate == min(d.AA.bias$posterior.lapse_rate) & beta_pi == max(d.AA.bias$beta_pi)) %>%
   droplevels() %>%
  distinct(Condition, Subject, data, posterior.lapse_rate, beta_pi, prior, posterior, posterior.categorization)

d.AA3 <- 
  d.AA.normalization %>%
  filter(category == Item.Intended_category) %>% 
  filter(normalization == "centered based\non exposure") %>%
  filter(prior_kappa.normalization == 4) %>%
  droplevels() %>%
  distinct(Condition, Subject, data, prior_kappa.normalization, mu_inferred, prior, posterior, posterior.categorization)

d.model.compare <- 
  bind_rows(
    d.AA1 %>%
      mutate(model = "Representations"),
    d.AA2 %>%
      mutate(model = "Decision_making"),
    d.AA3 %>%
      mutate(model = "Normalization"))

# Set parameters
p.current <- d.AA.exposure %>%
    mutate(Condition = paste0("Exposure: ", Condition)) %>%
    ggplot(aes(x = VOT, y = f0_Mel, color = Item.Category)) +
    geom_point(alpha = 0.5)
limits <- get_plot_limits(p.current)
VOT_range = c(limits$xmin, limits$xmax)
f0_range = c(limits$ymin, limits$ymax)
n_points = 100
cue_names = c("VOT", "f0")
model_names = c("Representations", "Decision_making", "Normalization")
category.contrasts = c("/d/", "/t/")
# Set image size
p_width = 900
p_height = 900

for (j in 1:length(model_names)){
  data = d.model.compare %>%
    filter(model == model_names[j])
  
  for (i in 1:length(conditions.AA)){
    output.AA <- prepare_3D.categorization_from_results(data, d.AA.exposure, d.AA.test, VOT_range, f0_range, n_points)
    
    df.resp = output.AA[[i]]
    
    p.3d.categorization <- plot_3D.categorization(df.resp) %>%
      layout(
        margin = margin,
        scene = list(
          camera = list(
            eye = list(x = -0.5, y = -2.5, z = 0.1)) # perspective good for showing categorization curve
        ))
    
    save_figure_or_not(p.3d.categorization, paste0('p.3d.categorization.model_', model_names[j], '_', conditions.AA[i], '.png'))
  }
  
  # show differences between exposure conditions in log odds space
  df.diff = list()
  df.diff$d_diff = qlogis(output.AA[[2]]$d_prop) - qlogis(output.AA[[1]]$d_prop)
  df.diff$x = df.resp$x
  df.diff$y = df.resp$y
  
  p.3d.categorization.diff <- plot_3D.categorization.diff(df.diff) %>%
    layout(
      margin = margin,
      scene = list(
        camera = list(
          eye = list(x = -0.5, y = -2.5, z = 0.1))))
  
  save_figure_or_not(p.3d.categorization.diff, paste0('p.3d.categorization.difference.model_', model_names[j], '.png'))
}
```

```{r show-model-categorization-3D-plots, fig.ncol = 3, fig.show='hold',fig.align='center', out.width="33%", out.height="49%", fig.cap="(ref:show-model-categorization-3D-plots)", fig.subcap=c('Representations: L1-accented', 'Decision making: L1-accented', 'Normalization: L1-accented', 'Representations: L2-accented', 'Decision making: L2-accented', 'Normalization: L2-accented', 'Representations: difference', 'Decision making: difference', 'Normalization: difference')}
filename = c()

for (p in 1:length(model_names)) {
  filename[p] = file.path(get_path('../figures/plotly/'), paste0('p.3d.categorization.model_', model_names[p], '_L1-accented.png'))
}

for (p in 1:length(model_names)) {
  filename[p + length(model_names)] = file.path(get_path('../figures/plotly/'), paste0('p.3d.categorization.model_', model_names[p], '_L2-accented.png'))
}

for (p in 1:length(model_names)) {
  filename[p + 3 + length(model_names)] = file.path(get_path('../figures/plotly/'), paste0('p.3d.categorization.difference.model_', model_names[p], '.png'))
}

knitr::include_graphics(filename)
```

 * talk about how this assumes that parameters are known. but by starting to use these models iteratively, we can narrow down plausible parameter settings.


This points the way to approaches that can more clearly distinguish between the three alternative mechanisms. 


<!-- Below we make four concrete recommendations to achieve this goal. First, experiments should be desgined to densely sample exposure and test items that are expected to yield distinct behavioral responses under each mechanistic model (Recommendation 1). Second, model predictions need to be spelled out in terms of acoustic-phonetic details of the input (Recommendation 2). Third, predictive power-simulations using the analytical framework will ensure the statistical power needed for constrastive tests of the three hypothesized mechanisms (Recommendation 3). Finally, it is critical that listeners' responses are analyzed at the stimulus level (Recommendation 4). --> 


















The case studies highlighted the significant roles that listeners' longterm experience plays in adaptive speech perception. We submit that future experimenters ought to operate under the null hypothesis that speech perception is strongly affected by expectations about the distributional realization of linguistic categories based on long-term experiences. This assumption is now shared by most theories of speech perception. It is also emphasized by recent reviews of the field, which highlight the significance of distributional properties of the speech input for perception [@bent-baeseberk2021; @kurumada-roettger2021; @schertz-clare2020; @quam-creel2021]. However, informal reasoning about the consequences of this null hypothesis can quickly gain in complexity, making it more likely that *ad-hoc* or *post-hoc* reasoning results in misleading conclusions. Consider the highly influential study “The Weckud Wetch of the Wast” [@maye2008]. The study exposed listeners to speech in which the vowel categories had undergone systematic (phonological) shifts. After just 20 minutes of exposure, listeners’ interpretation of subsequent speech input from the same talker had changed substantially [for replications and extensions, see also @weatherholtz2014]. Based on additional control conditions, Maye and colleagues concluded that this finding could not be explained by “general relaxation of the criterion for what constitutes a good exemplar of the accented vowel category” but rather argued that participants had learned that the talker’s categories were systematically shifted. A recent study [@hitczenko-feldman2016] revisited this result by formalizing the competing accounts within the same distributional learning framework for changes in representations that we have integrated into ASP [@kleinschmidt-jaeger2015]. Hiczenko and Feldman found that the results of Maye et al. (2008) were just as well, if not better, predicted by a learning model that expanded the categories, rather than shifting them.

An analytical approach like ASP can be a powerful tool for principled hypothesis testing [see also @apfelbaum-mcmurray2015; @sohoglu-davis2016; @tan2021; @theodore-monto2019; @toscano2018; @xie2021cognition]. It provides an alternative to informal reasoning about changes in speech perception. It makes explicit the effects of two types of information that affect listeners’ interpretation of speech from an unfamiliar talker: (1) the listener’s prior expectations based on the statistics of previously experienced speech input; and (2) the statistics of the present speech input relative to those prior expectations. In an exposure-test experiment like those discussed in our case studies, (1) is further divided into two sub-components: (1a) expectations based on the speech statistics experienced prior to the experiment; and (1b) the input experienced during the exposure phase. In other words, the input received during an experiment incrementally changes the prior expectations that the listener brings into a subsequent perceptual response. The three change models we described spell out competing hypotheses about *how* (1b) is integrated with (1a). The categorization model (adapted based on exposure input) determines how these changes are expected to affect listeners’ responses during the test phase (see Figure \@ref(fig:overview-change)). In what follows we discuss how this predictive power of ASP can be most effectively used in future experiments.

<!--Beyond providing qualitative predictions, the current approach can be used to derive quantitative predictions based on particular linking hypotheses about mappings between acoustic cues and speech categories. Specifically, we can use the analytical framework to predict *when*, *to what acoustic-phonetic inputs* and *after how much exposure* listeners would show adapted responses under the three mechanistic hypotheses. Below we make four concrete recommendations to achieve this goal. First, experiments should be desgined to densely sample exposure and test items that are expected to yield distinct behavioral responses under each mechanistic model (Recommendation 1). Second, model predictions need to be spelled out in terms of acoustic-phonetic details of the input (Recommendation 2). Third, predictive power-simulations using the analytical framework will ensure the statistical power needed for constrastive tests of the three hypothesized mechanisms (Recommendation 3). Finally, it is critical that listeners' responses are analyzed at the stimulus level (Recommendation 4). --> 

<!-- As another example, consider a recent study by Zheng and Samuel investigates whether perceptual recalibration and accent adaptation involve the same mechanisms. Zheng and Samuel (2020) approach this question through the lens of individual differences between listeners: if perceptual recalibration and accent adaptation involve the same mechanisms, participant-specific differences in perceptual recalibration and accent adaptation should correlate. Zheng and Samuel had participants complete a series of experiments on the perception of the /?/- /s/ contrast (e.g., thin vs. sin), including both perceptual recalibration and accent adaptation paradigms. Zheng and Samuel found no significant correlation between participants’ performance in the two paradigms. Power analyses presented by Zheng and Samuel suggested that their study had more than 80% power to detect a moderate correlation (?? ≥ .4). Based on this, Zheng and Samuel concluded that “the null effect is not plausibly due to insu?icient power” (p. 1281) and cautiously conclude their “results provide no support for the view that recalibration of phonemic boundaries plays a central role in natural accent accommodation.” (p. 1286). 

But do their results really provide evidence against this hypothesis? Taking at face value the assertion that the study provided 80% power to detect a moderate correlation, the real question is what magnitude of correlation researchers should expect if the hypothesis that both perceptual recalibration and accent adaptation share the same mechanism is true. The to-be-expected magnitude of such a correlation depends on many factors that jointly affect the test-retest reliability of the two paradigms (and thus the researchers’ ability to reliable estimate individual differences in these tasks; see also discussion in Zheng and Samuel, 2020). This in turn depends on assumptions about the perceptual noise, attentional lapses, and other factors that affect the trial-to-trial consistency of participants. It also depends on the amount of data collected from participants, and whether it is elicited and analyzed in ways taking into account that repeated testing can reduce the effects of recent exposure, reducing power if not appropriately considered by the analyses (L. Liu and Jaeger 2018, 2019; Luthra et al. 2020; Theodore 2021; see also, discussion in Norris, McQueen, and Cutler 2003). Finally, the actual statistical power of analysis like that by Zheng and Samuel also depends on the statistical properties of the exposure and test stimuli employed in the perceptual recalibration and accent adaptation experiments: how and how much exposure is expected to affect subsequent speech perception depends on the specific placement of exposure and test stimuli relative to listeners’ prior expectations. In short, there are many variables that interact with each other in a complex manner.[JF2] -->

### Recommendation 2: Dense and targeted sampling of the stimulus space
While the ASP change models for all three mechanisms can account for the signature results of perceptual recalibration and accent adaptation paradigms, they do also make different predictions. <!-- The predictions of all three of the models are constrained by: 1) the prior state of listeners; 2) the acoustic/phonetic properties as well as category labels of the exposure stimuli; and 3) the acoustic/phonetic properties of the test stimuli. For any combination of 1)-3), each change model predicts a response distribution across test tokens that can be compared to actual human responses. Given (1) an estimate of listeners state prior to the experiment and (2) a set of exposure and test stimuli, these predictions are mediated only through the parameters of the change model (e.g., $\beta_{\pi}$ for changes in response biases), limiting the range of results a change model can account for. That is, instead of merely comparing the ability in which each mechanism captures the *qualitative* pattern of behavioral changes, we can compare the three mechanisms by assessing their accuracy in *quantitatively* predict and explain human responses across a variety of experimental conditions. -->

Figure \@ref(fig:show-model-categorization-3D-plots) illustrates this point for the data from Case Study 2. For each of the three change models, we take the parameters that resulted in the highest post-L2-exposure accuracy during test, and plot the predicted categorization functions for both exposure conditions. These are the categorization functions that a listener would be predicted to employ while categorizing test stimuli. Despite the fact that the three change models resulted in qualitatively similar categorization accuracy in Case Study 2, the three models differ in *how* they achieve this accuracy. Specifically, they predict different categorization functions over the VOT-f0 space. 

This points the way to approaches that can more clearly distinguish between the three alternative mechanisms. Intuitively, paradigms should elicit 'rich' data---i.e., data that is informative about the functional shape of changes in the stimulus-to-response mapping---and analyses should take advantage of the 'richness' of the data. Most immediately, this is achieved by moving beyond comparisons that are limited to changes in overall categorization accuracy or processing speed, towards analyses that directly assess the link between the acoustic/phonetic properties of the speech input and listeners' responses (see Recommendation 3). Such approaches remain the exception but are now increasingly common [e.g., @clare2019; @idemaru-holt2020; @kartushina2015; @kim2020; @liu-holt2015; @schertz2015; @wade2007]. In order to reliably detect changes in categorization functions, it can be beneficial to more densely sample the acoustic/phonetic space during the test phase. Strategic placement of test stimuli can be used to increase the statistical power to test predicted differences between the different change models [@burchill-jaeger2022]. For example, the bottom row of Figure \@ref(fig:show-model-categorization-3D-plots) suggests that denser sampling of certain VOT-f0 combinations should make it possible to distinguish between the three change models (with the specific parameterizations assumed in the figure). 

Moving beyond binary contrasts between two exposure conditions can further facilitate comparison between change models. Experiments that employ more exposure conditions---especially, exposure conditions that sample a wide range of different exposure scenarios---are likely to facilitate more accurate and reliable estimation of the change model parameters. This, in turn, can increase the statistical power of model comparison. For instance, @kleinschmidt-jaeger2016cogsci created six different between-subject exposure conditions with distinct VOT distributions for /b/ and /p/, using a similar paradigm as @clayards2008. Kleinschmidt and Jaeger used these data to contrast different parameterizations of the same change model [for additional analyses and insightful discussion, see also @kleinschmidt2020]. Future research could use similar approaches to contrast the competing change models, or to infer the relative weighting of the three mechanisms. While some previous work has employed multiple exposure conditions [e.g., for perceptual recalibration and similar paradigms, @babel2019; @sumner2011], this approach remains under-utilized, and computational analyses of the results of experiments with multiple exposure conditions remain lacking. In an ongoing project, we use meta-analyses to address this gap. We have constructed a database of over 100,000 categorization responses from dozens of perceptual recalibration experiments on the English /s/-/`r linguisticsdown::cond_cmpl("ʃ")`/ contrast. Each of these experiments employs slightly different exposure and test stimuli, potentially increasing the statistical power to distinguish between the competing models. <!-- Such item-level meta-analyses benefit immensely from open science standards that facilitate (or even require) the sharing of data in well-documented formats. -->

Similarly, incremental testing *within* subjects can increase statistical power to contrast the predictions of different change models. Even when all three change models predict the same *outcome* of adaptation, they can differ in the trajectory of changes they predict. While incremental testing paradigms exist [e.g., @bertelson2003; @bonte2017; @vroomen2007], they remain under-utilized, and model comparisons based on such experiments remain lacking [but see @kleinschmidt-jaeger2012]. An ongoing study from one of our labs combines incremental testing with multiple exposure conditions to maximize the statistical power to fit change models [for initial results, see @tan2022]. Initial results suggest that this approach can indeed reveal deficiency of change models that would not otherwise be apparent.

While the approaches we have discussed here have so far primarily been limited to paradigms that employ (re)synthesized or otherwise phonetically manipulated stimuli (e.g., perceptual recalibration or standard distributional learning paradigms), they can also be employed in combination with naturally accented stimuli. For example, @chodroff-wilson2020 took advantage of naturally occurring variability, and created different exposure conditions by selecting different subsets of natural stimuli. This approach strikes a particularly intriguing balance between ecological validity and experimental control that deserves further attention in future work.




### Recommendation 3: Advance standards of data annotation and sharing
In order to derive and contrast predictions under competing hypotheses about change mechanisms, researchers need estimates of 1) the prior state of listeners; 2) the acoustic/phonetic properties as well as category labels of the exposure stimuli; and 3) the acoustic/phonetic properties of the test stimuli. This is the case for ASP and any similar framework aiming to capture changes in the expectations that are assumed to affect speech perception. 

Estimate 1) can be obtained from sufficiently large phonetically annotated databases that capture the type of speech input a typical participant in the experiment is likely to have received throughout their life. Fortunately, large and phonetically annotated databases can offer good estimates of the speech input of listeners receive. Such databases are now available for an increasing number of phonetic contrasts and languages [e.g., @chodroff-wilson2018; @clopper-pisoni2006; @hillenbrand1995; @newman2001; @theodore2009; @xie-jaeger2020, among many others]. One caveat is that many of these databases either are representative only of a subset of the speech varieties that an average listener of a language has plausibly been exposed to, or contain very little data for each variety. In particular, databases that contain both a large number of talkers and a large number of tokens per talker continue to be the exception. Researchers need to carefully consider these implications when employing databases. In some cases, researchers might find that the best way forward is to collect phonetically annotated data that meets the specific requirements for their study. @xie2021cognition, for instance, collected ~3000 production tokens of prosodic categories and used them to model listeners’ responses in a perception experiment.

An alternative option is to eschew phonetically annotated data in favor of computational methods that work with the raw signal or some automatically obtained transformation of the raw data [e.g., Mel-Frequency Cepstral Coefficents, (MFCCs) @Mermelstein1976]. Such models have been developed for automatic speech recognition [for an earlier review, see @jurafsky-martin2000] and have been employed to model human language acquisition [@dupoux2018; @feldman2013; @toscano-mcmurray2010; @vallabha2007]. More recently, they have also been applied to address questions about the effects of recent exposure [@richter2017]. The ASP framework can, at least in theory, be combined with such models.

Estimates for 2) and 3) require researchers to annotate the stimuli they employ in their perception experiments. In our view, such annotations entail a manageable effort: perception experiments typically employ a small number of speech stimuli that are repeated for each participant. A typical perceptual recalibration experiment would require the annotation of less than 100 isolated word recordings. A large study on accent adaptation like Bradlow and Bent’s (2008) Experiment 2 would require the annotation of about 1000 sentences. Studies on phonetic production regularly annotate data sets many times larger. These efforts towards a more open, collaborative science can further be supported by clear standards for reproducibility and software developments that aid phonetic annotation and data sharing [e.g., @cassidy-schmidt2017; @picoral2021; @roettger2019; @winkelmann2017]. If annotations are not reported and shared---and ideally even if they are---then all audio recordings should be shared in an open and accessible way (e.g., OSF). This will require perception researchers to use human subject protocols that gives them the consent to distribute stimulus recordings for the purpose of scientific inquiry. In other words, perception researchers should follow the same standards that adopted by researchers working on language production. 

### Recommendation 4: Predictive power analyses prior to conducting experiments
Like other computational frameworks, ASP can be used to estimate expected effect sizes and statistical power before conducting an experiment. Unlike commonly used power estimates, which *assume* effect sizes [e.g., "moderate" effects, @zheng-samuel2020], predictive power-analyses of the type we envision are based effect sizes that are derived from the acoustic properties of the exposure and test stimuli. This makes these predictive power simulations considerably more informative. Taking a step beyond even predictive power analyses, ASP could even be used to guide experimental design---specifically, to select exposure and test stimuli that are expected to increase statistical power.

There are, however, challenges that will have to be met so that ASP can be used for predictive power simulations. Figure \@ref(fig:predictive-power-simulations) illustrates these challenges. For a specific set of exposure and test stimuli, and specific parameter settings for all change models, ASP predicts an expected categorization function. This is what we showed in, for example, in Figure \@ref(fig:show-model-categorization-3D-plots) and the result figures for Case Study 1 [see also, e.g., @tan2021; @theodore-monto2019; @xie2021 for similar approaches applied to only representational change models]. This categorization function can be used to derive a predictive power estimate by repeatedly sampling responses for the planned test stimuli and the planned number of participants. This can be done for any individual exposure conditions, for combinations of exposure conditions, and/or the predicted differences between exposure conditions.

\begin{figure}[h]
\begin{center}
\includegraphics[width=.7\columnwidth]{`r get_path("../figures/diagrams/predictive-power-simulations.png")`}
\caption{CAPTION. REPLACE WITH NEW FIGURE BY CHIGUSA}\label{fig:predictive-power-simulations}
\end{center}
\end{figure}

However, the parameterization of the different models are typically *not* known to researchers---if they were, there would be not need for the present article. The best that researchers can do is thus to calculate statistical power while marginalizing (averaging) over a plausible *distribution* of ASP parameters for the models they wish to contrast. In Bayesian terminology, this is achieved by defining priors over the ASP parameters. Initially, when relatively little is known about the relevant distribution of parameters, very weak priors are recommended that will consider a wide range of parameters as probable. As additional studies become available, this will further inform the range of plausible priors. Even now though, the plausible range of parameters is constrained by the fact *that* listeners can adapt within a certain number of observations. This constrains the joint distribution of ASP parameters since any plausible parameterization needs to give at least one of the change models sufficient flexibility.

If researchers further want to know which exposure or test stimuli would increase the predicted difference between hypotheses, that would further require comparison of different stimulus selections. Comparable approaches exist for simpler computational problems, and have been used extensively in psychometric research [incl. online stimulus selection *during* the experiment, depending on subject-specific performance, e.g., @vul2011; @prins2013; @wichmann-hill2001].

### Recommendation 5: Data analysis beyond overall accuracy and speed 
To take advantage of the richer types of data described in the previous sections, it is also necessary to conduct analyses that go beyond changes in the overall accuracy or speed of comprehension. In particular, analyses that directly link relevant acoustic or phonetic properties of the input to listeners’ responses strike us as promising. This should take into account that listeners rely on a multitude of cues [e.g., @mcmurray-jongman2011] even when the experiment manipulates only one of them. Such analyses could employ the type of models we have described in this study. Alternatively, even simple regression can suffice as long as it employs appropriate linking functions [e.g., logistic regression for categorization, @agresti2019; @jaeger2008]. This approach is now increasingly common, taking advantage of stimulus-level variability within and across conditions to test hypotheses. These regression analyses can be expanded to accommodate lapse rates and response biases [e.g., @clayards2008; @kleinschmidt2020], and can be fit in standard statistics software [e.g., through \texttt{brms} @burkner2017 or other libraries in \texttt{R}]. A comprehensive introduction to such models is provided in @wichmann-hill2001.^[Indeed, there are direct connections between the model we have employed here and lapsing logistic regression. For example, two Gaussian categories along a phonetic continuum result in linear effect of the continuum on the posterior log-odds of the categories if the two categories have identical variance, but linear and quadratic if the two categories have unequal variance [e.g., @kleinschmidt-jaeger2015; @kronrod2016]. This prediction can only be appreciated if logistic regression is employed for the analysis of 2AFC categorization tasks, rather than ANOVA or similar methods. @schertz-clare2020 provide an excellent overview of how standard regression analyses can be---and have been---used to assess the effects multiple phonetic cues on listeners’ responses.] 

### Recommendation 6: Further model development and integration with models of neural models
Last but not finally, we emphasize the need for further model development. Perhaps most pressingly this includes the need to integrate frameworks like ASP with neural processing models, to further constrain their predictions [see also @guediche]. But additional developments are also needed to better characterize the mathematical limits of existing models, in particular once additional variants of the three change models are considered (such as the inclusion of standardization in normalization models; the introduction of an independence assumption for representational change models, cf. footnote \@ref(fn:alternative-representational-changes); or the development of normalization and representational change models that capture that the learning of *new* cues usually entails additional challenges [see also @schertz-clare2020].

## Conclusion
Through formalizing and simulating three distinct mechanisms of adaptive speech perception, we explored how these mechanisms may be empirically distinguished from one another. Even after all the possible steps we envision are implemented, however, our inferences would always be constrained by the assumptions we derive about the data as well as about the underlying cognitive, perceptual and computational architecture. As researchers, we must therefore be aware of our own theoretical biases or dispositions to make them explicit in our theory development. A general conclusion we can draw from the current investigation is that, for processes as complex as human speech perception, it is hard to make strong inference without an aid of analytical tools like the one we put forward here.
