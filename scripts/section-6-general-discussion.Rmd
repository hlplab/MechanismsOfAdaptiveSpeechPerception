# General discussion {#sec:general-discussion}
The two case studies we have presented together provide two important insights. First, the signature results from two influential lines of research---often taken to lend support to changes in category representations---are actually compatible with representationally and computationally more parsimonious change mechanisms (pre-linguistic signal normalization and changes in post-perceptual decision-making). Additional case studies not reported here suggest that this finding is likely to generalize at least partly to other types of exposure-test paradigms, such as distributional learning paradigms over a single phonetic cue [@clayards2008; @kleinschmidt-jaeger2016cogsci] and dimension-based statistical learning paradigms that manipulate the relative informativity of cues [@idemaru-holt2011; @idemaru-holt2020].^[There are, however, important differences in the extent to which different research traditions have already implemented the recommendations we make below. For example, some lines of research tend to interpret findings relative to the distribution of phonetic cues in the speech input [e.g., studies on distributional learning, @bejjanki2011; @clayards2008; @kleinschmidt-jaeger2016pbr; @malone2021; studies on dimension-based statistical learning, @idemaru-holt2020; @schertz2015; for review, see @schertz-clare2020], whereas this remains the exception in other lines of research [but see, e.g., @hitczenko-feldman2016; @tan2021 for accent adaptation; @drouin2016; @theodore-monto2019 for perceptual recalibration]. See also SI (\@ref(sec:sufficiency)).]

As already mentioned, this does not mean that normalization, representation, and decision-making mechanisms always predict the same response patterns—they do not. Nor does it mean that the relative involvement of the three mechanisms cannot be empirically determined through experiments. Rather, as we discuss in more detail below, it means that the _design_ and _level of analysis_ employed in many studies severely limit what the resulting experiments can tell us about the underlying change mechanisms. This, we argue, calls for changes to the paradigms and analyses that are commonly employed in research on adaptive speech perception, and we make specific recommendations below.

Key to these recommendations is the second insight derived from our case studies: as we illustrated at the end of Case Study 2, each of the three mechanisms is subject to different computational limitations, and thus yields different predictions depending on the specific acoustic-phonetic properties of the exposure and test stimuli. Below we discuss how future experiments can be designed and analyzed so as to maximize researchers' ability to detect the differences in the predictions of the different change models, and to determine the relative engagement of each mechanism. Central to our recommendations is the use of ASP or similar frameworks to inform experiment design and analysis through simulations and quantitative model comparisons. First, however, we discuss *why* we believe that computational simulations and model comparisons will be critical to advancing the field.

## Beyond sufficiency tests
For our two case studies, we considered each of the three change models of the ASP framework _separately_. This served two purposes. The first was presentational: by presenting the predictions of each model while the other two change models were 'switched off', we hope to have provided readers with clearer intuitions about the inner workings of each model. The second purpose was the test of _(in)sufficiency_ of each change mechanism in explaining the signature results from the two paradigms. We asked whether each of the mechanisms alone is sufficient to predict adaptive changes of speech perception. Ultimately, however, research on adaptive speech perception will have to go beyond such sufficiency tests. With a phenomenon as complex as human speech perception, it is most plausible that multiple mechanisms, from early perception to decision-making, _jointly_ contribute to the observed adaptive behavior.

Indeed, existing evidence---summarized in more detail in SI (\@ref(sec:sufficiency))---supports the idea that none of the three mechanisms alone is sufficient to explain the wide range of adaptive behaviors that listeners exhibit in response to recent exposure. For example, the finding that non-speech stimuli---such as sine tones---can affect subsequent vowel perception [e.g., @holt2001; @holt2006; @huang-holt2011] is easily explained in terms of pre-linguistic normalization but difficult to explain through changes in linguistic representations or decision-making [see also @chodroff-wilson2020]. Other findings, however, cannot easily be accommodated by an account that solely relies on pre-linguistic normalization. One example comes from research on perceptual recalibration: the effects of exposure to fricative consonants (/f/ vs. /s/) do not only depend on the acoustic-phonetic cues of the exposure tokens but also their category labels (e.g., Norris et al., 2003, Experiment 2; for details, see SI, \@ref(sec:sufficiency)). Further support for the hypothesis that adaptive speech perception is the result of multiple mechanisms comes from neuroimaging studies: across paradigms, adaptive speech perception implicates brain regions ranging all the way from pre-cortical structures [e.g., in the brain stem, @chandrasekaran2009; @polonenko2021; @zhao2018] to the (pre-)frontal cortex [@hickok-poeppel2007; @blanco-elorriera2021; @defenderfer2021].

<!-- In short, research on adaptive speech perception is at an important juncture. On the one hand,--> Existing findings thus strongly suggest that no single change mechanisms can explain the full variety of adaptive responses that humans exhibit. It seems obvious that the field will have to move beyond (in)sufficiency tests, towards experiments that determine how multiple change mechanisms *jointly* achieve adaptive speech perception. This will likely require research on how the relative engagement of different change mechanisms depends on stimulus properties, cue and contrast types, task demands, or individual differences between listeners. For example, simple shifts of categories along a single continuum (as simulated in perceptual recalibration experiments) may engage change mechanisms different from those that are used to navigate more complex shifts as seen in natural accents [for discussion, see @bent-baeseberk2021; @samuel-kraljic2009; @zheng-samuel2020]. Another, mutually compatible, hypothesis holds that the earliest moments of exposure to an unfamiliar talker primarily reflect change mechanisms that are computationally less flexible but simpler. With increasing input, computationally more complex mechanisms, which ultimately can support higher recognition accuracy, would increasingly come to determine listeners' behavior.^[In ASP, this is most naturally be accounted for in terms of slow 'learning rates' for the less parsimonious change model (e.g., high $\kappa_{c,0}$s and $\nu_{c,0}$s for changes in representations).] For listeners, this would reduce the risk of overfitting parameters to the input, a risk that applies more strongly to change models with a larger number of parameters [for discussion, see @apfelbaum-mcmurray2015; @kleinschmidt-jaeger2015; @toscano-mcmurray2010]. Similarly, it is possible that the relative engagement of different change mechanisms depends on the type of phonetic contrast, or even the type of cue. This would be expected, for example, because different types of cues---e.g., spectral cues like f0 vs. durational cues like VOT---exhibit different degrees of within- and between-talker variability [see discussions in @kleinschmidt-jaeger2015, p. 179-180; @kraljic-samuel2007; @xie2021cognition].

Most existing paradigms and analysis approaches employed in studies on adaptive speech perception are not well-suited to address these questions. This problem is most obvious for the behavioral paradigms we have discussed in our case studies: if an analysis approach cannot distinguish between three mechanisms _even if one assumes that only one of these mechanisms is at work_, that approach cannot possibly determine the relative engagement of multiple mechanisms. However, neuroimaging studies are not exempt from this problem. While they might determine the involvement of multiple brain regions, they often leave open what types of computations are performed in each. It is not known, for example, whether subcortical areas involved in auditory processing are necessarily restricted to normalization or whether these areas can take category identity into consideration [e.g., through documented feedback projections from cortical areas, @Erb2013]. Similarly, it is an open question whether differential activation of early cortical areas indicates changes in category representations or normalization [e.g., the involvement of Heschl's gyrus in adaptive speech perception, @sjerps2019], and whether differential activation in frontal areas reflect changes in decision-making [as seems to be assumed in, e.g., @myers-mesite2014] or rather the cumulative upstream effect of distributed changes in representations. Observations like these about the current state of the field motivate the recommendations we present next.

## Methodological advances to facilitate more informative model comparisons {#sec:methodological-advances}
We offer five concrete recommendations as to how future research on speech perception can benefit from computationally-guided experiment design and data analysis methods. In addition, we emphasize here that improved standards of data annotation and sharing will be critical in supporting this endeavor, and to the field en large. Although listeners' responses are known to depend on the acoustic-phonetic properties of the speech input, studies on adaptive speech perception rarely provide annotations of those properties. We cannot hope to understand speech perception without moving beyond this status quo (for concrete suggestions, see SI \@ref(sec:DataSharing)).

### Recommendation 1: *Pre*diction based on sufficiently constraining theories instead of (only) informal reasoning
Our first recommendation is, as we have aimed to do here, to develop and employ strong theories and models, following Platt’s call for a strong inference approach to scientific inquiry [@platt1964].  The majority of published research on adaptive speech perception---including some of our own work---continues to employ informally formulated, under-specified hypotheses. This leaves too much room for *ad-hoc* or *post-hoc* reasoning, with all the downsides that have been discussed in the context of the replicability crisis and elsewhere [e.g., @guest-martin2021; @starns2019; @vasishth2021; @yarkoni2022]. Even for research explicitly framed in terms of more or less clearly specified theories like "distributional learning" or "exemplar theory", reliance on informal reasoning alone is both risky and likely to miss insights that can be gained if computational frameworks are employed.

We have experienced this in our own work---for example, in @kleinschmidt-jaeger2015 we were initially surprised that standard perceptual recalibration results can also be explained through changes in beliefs about category _variances_ rather than only through changes in beliefs about category means. We likely would not have arrived at this insight without the use of a computational model with clearly specified linking hypotheses (about the mapping from stimulus properties to participants' categorization responses). Another example from our own work is @tan2021. In that study, we initially were surprised by a failure to replicate the benefit of L2 accent exposure observed in @xie2016jep. The experiment by Tan and colleagues focused on word-final stop voicing in Flemish-accented Swedish, rather Mandarin-accented English. This choice was motivated by the fact that, at least at first blush, word-final stop voicing in both of these accents differs from L1 listeners' expectations in qualitatively similar ways. However, post-hoc computational simulations found that the same representational change model used by @kleinschmidt-jaeger2015 predicts both the facilitatory effect of exposure found in @xie2016jep, and the lack thereof in @tan2021 ---despite the qualitative similarities of the L2 accents employed in the two studies. We surmise that the literature contains many other null findings that are the consequence of stimulus and paradigm choices, rather than necessarily being informative about the underlying mechanisms [see e.g., @floccia2006; @zheng-samuel2020]. The use of ASP and similar models has the potential to reduce the tendency to interpret every difference in significance as meaningful to theory.

A third demonstration of how computational and theoretical rigor can revise intuitions comes from the now classic study "The Weckud Wetch of the Wast" [@maye2008]. The study exposed listeners to speech in which the vowel categories had undergone systematic (phonological) shifts and observed adaptive changes in subsequent vowel recognition. Based on additional control conditions, Maye et al. (2008) reasoned that this finding could not be explained by "general relaxation of the criterion for what constitutes a good exemplar of the accented vowel category" (p.543). They argued that listeners instead had learned shifts in vowel representations. @hitczenko-feldman2016 recently revisited this result, using the representational change model of @kleinschmidt-jaeger2015. Hitczenko and Feldman found that category expansion ("relaxation") explained the results of Maye et al. (2008) just as well as category shift.

Based on examples like these, we submit that computational models are not only useful but likely indispensable for research on adaptive speech perception, given the complexity of multiple, jointly active, mechanisms. Consider, for example, that all major theories of adaptive speech perception agree that two types of information affect listeners' interpretation of speech from an unfamiliar talker: (1) listeners' prior expectations based on the statistics of previously experienced speech input; and (2) the statistics of the present speech input (i.e., in test) relative to those prior expectations. In exposure-test experiments like those discussed in our case studies, (1) is further divided into two components: (1a) expectations based on the long-term speech statistics experienced prior to the experiment; and (1b) the input experienced during the exposure phase, where (1b) is assumed to incrementally change (1a) that a listener brings into a subsequent perceptual response. In short, there is broad agreement that the commonalities and differences between prior experience, exposure, and test jointly affect both listeners' neural processing and their observable behavior. This points to complex interactions, the predicted consequences of which are often difficult to understand without the use of computational models [for further examples and discussion, see @apfelbaum-mcmurray2015; @sohoglu-davis2016; @tan2021; @theodore-monto2019; @toscano2018; @xie2021cognition].   

To sum up, experimental psychologists and neuroscientists stand to benefit from increased familiarity with the computational models that are directly relevant to their research (just as computational researchers benefit from familiarity with experimental paradigms and methods). Fully specified computational models of normalization and changes in representations have now been available for more than a decade, and yet continue to be frequently ignored in the discussion of experimental results. An increasing number of these models is now freely available in the form of R libraries [e.g., for normalization: \texttt{phonTools}, @barreda2015; for changes in category representations: \texttt{beliefupdatr} and its extension \texttt{MVBeliefUpdatr}, @kleinschmidt-jaeger2015]. The OSF repository for this article (https://osf.io/q7gjp/) contributes to these efforts. Our ASP code integrates models of all three change mechanisms in R. The three change models of ASP implement competing---but mutually compatible---hypotheses about how exposure (1b) is integrated with prior experience (1a). The categorization model (updated based on the exposure input) determines how these changes are expected to affect listeners’ responses to test items (2). These resources can help researchers derive predictions prior to conducting an experiment, based on the acoustic-phonetic properties of the planned exposure and test stimuli (Recommendation 4), and to compare the fit of different combinations of change models to listeners' behavior once data collection has been completed (Recommendations 2 and 3).


### Recommendation 2: Analyses that link the acoustic-phonetic properties of stimuli and participants' responses

```{r set-3D-plot-parameters, warning=FALSE}
# retrieve the exact dataset used for the cue-reweighting case
d.AA.exposure <- d.AA.exposure.main
d.AA.test <- d.AA.test.main

p.current <- d.AA.exposure %>%
    mutate(Condition = paste0("Exposure: ", Condition)) %>%
    ggplot(aes(x = VOT, y = f0_Mel, color = Item.Category)) +
    geom_point(alpha = 0.5)

limits <- get_plot_limits(p.current)
VOT_range = c(limits$xmin, limits$xmax)
f0_range = c(limits$ymin, limits$ymax)
n_points = 100
cue_names = c("VOT", "f0")
model_names = c("Representations", "Decision_making", "Normalization")
category.contrasts = c("/d/", "/t/")

# Set image size
p_width = 900
p_height = 900
```

```{r compare-AA-models-results-functions, warning=FALSE}
compare_models.AA <- function(
    data1, data2, data3,
    VOT_range, f0_range, n_points
) {
  d.model.compare <-
    bind_rows(
      data1 %>%
        filter(category == Item.Intended_category) %>%
        droplevels() %>%
        distinct(Condition, Subject, prior_kappa, prior_nu, posterior) %>%
        mutate(model = "Representations"),
      data2 %>%
        filter(category == Item.Intended_category) %>%
        filter(posterior.lapse_rate == lapse_rate.selected & beta_pi == beta_pi.selected) %>%
        droplevels() %>%
        distinct(Condition, Subject, posterior.lapse_rate, beta_pi, posterior, sim) %>%
        mutate(model = "Decision_making"),
      data3 %>%
        filter(category == Item.Intended_category) %>%
        filter(prior_kappa.normalization == prior_kappa.normalization.selected) %>%
        droplevels() %>%
        distinct(Condition, Subject, prior_kappa.normalization, posterior) %>%
        mutate(model = "Normalization"))

  for (j in 1:length(model_names)) {
    message(paste("Preparing", model_names[j]))
    data <- 
      d.model.compare %>%
      filter(model == model_names[j])
    
    fn.check <- paste0('p.3d.categorization.model_', model_names[j], '_', conditions.AA[1], '_', example_label, '.png')
    if (RESET_FIGURES || !file.exists(file.path(get_path('../figures/plotly/'), fn.check))) {
      output.AA <- prepare_3D.categorization_from_results(data, VOT_range, f0_range, n_points)
      
      for (i in 1:length(conditions.AA)) {
        message(paste("...", conditions.AA[i]))
        fn <- paste0('p.3d.categorization.model_', model_names[j], '_', conditions.AA[i], '_', example_label, '.png')
        df.resp <- output.AA[[i]]
        message(paste("Creating and overriding"), fn)
        p.3d.categorization <-
          plot_3D.categorization(df.resp, width = p_width, height = p_height) %>%
          # select perspective that is good for showing categorization curve
          layout(
            margin = margin,
            scene = list(
              camera = list(
                eye = list(x = -0.5, y = -2.5, z = 0.1))))
        
        save_3Dfigure(p.3d.categorization, fn)
      }
    }
    
    # show differences between exposure conditions in log odds space
    fn <- paste0('p.3d.categorization.difference.model_', model_names[j], '_', example_label, '.png')
    if (RESET_FIGURES || !file.exists(file.path(get_path('../figures/plotly/'), fn))) {
      message(paste("Creating and overriding"), fn)
      df.diff <- list()
      df.diff$difference_in_logodds_d <- qlogis(output.AA[[2]]$proportion_d) - qlogis(output.AA[[1]]$proportion_d)
      df.diff$VOT <- df.resp$VOT
      df.diff$f0 <- df.resp$f0
      
      p.3d.categorization.diff <-
        plot_3D.categorization.diff(df.diff, width = p_width, height = p_height) %>%
        layout(
          margin = margin,
          scene = list(
            camera = list(
              eye = list(x = -0.5, y = -2.5, z = 0.1))))
      
      save_3Dfigure(p.3d.categorization.diff, fn)
    }
  }
  
  # show the bar graph demonstrating the model parameters and recognition accuracy for each model
  p.accuracy1 <- basic_AA_result_plot(data1) +
    geom_text(
      inherit.aes = FALSE,
      data = . %>%
        group_by(Condition, prior_nu, prior_kappa) %>%
        summarise(mAcc = round(mean(response), digits = 2)),
      mapping = aes(label = mAcc, x = Condition, y = 1.05),
      size = geom_text.size) +
    facet_grid(
      prior_nu ~ prior_kappa,
      labeller = label_bquote(
        cols = {kappa[.(categories.AA[1])~","~0] == kappa[.(categories.AA[2])~","~0]} == .(as.character(prior_kappa)),
        rows = {nu[.(categories.AA[1])~","~0] == nu[.(categories.AA[2])~","~0]} == .(as.character(prior_nu))))
  
  p.accuracy2 <- basic_AA_result_plot(data2) +
    geom_text(
      inherit.aes = FALSE, data = . %>%
        group_by(Condition, posterior.lapse_rate, beta_pi) %>%
        summarise(mAcc = round(mean(response), digits = 2)), aes(label = mAcc, x = Condition, y = 1.05), size = geom_text.size) +
    facet_grid(
      posterior.lapse_rate ~ beta_pi,
      labeller = label_bquote(
        cols = beta[pi] == .(beta_pi),
        rows = lambda == ~.(posterior.lapse_rate)))
  
  p.accuracy3 <- basic_AA_result_plot(data3) +
    geom_text(
      inherit.aes = FALSE,
      data = . %>%
        group_by(Condition, prior_kappa.normalization) %>%
        summarise(mAcc = round(mean(response), digits = 2)),
      mapping = aes(label = mAcc, x = Condition, y = 1.05),
      size = geom_text.size) +
    facet_grid(
      . ~ prior_kappa.normalization,
      labeller = label_bquote(
        cols = ~kappa[0] == .(as.character(prior_kappa.normalization))))
  
  p.output <-
    plot_grid(
      p.accuracy1 + theme(legend.position="none"),
      p.accuracy2 + theme(legend.position="none"),
      p.accuracy3 + theme(legend.position="right"),
      nrow = 1, rel_widths = c(1.1, 1.1, 1.3))
  
  ggsave(paste0('../figures/plotly/p.model_comparison_', example_label, '.png'), plot = p.output, width = 2*(base.width*3*1.2 + .5), height = 2*(base.height + .5), dpi = 300)
}

make_plot_compare_models.AA <- function(
    model_names = c("Representations", "Decision_making", "Normalization"),
    compare_model_label
) {
  filename = c()
  for (p in 1:length(model_names)) {
    filename[p] <- 
      file.path(
        get_path('../figures/plotly/'), 
        paste0('p.3d.categorization.model_', model_names[p], '_L1-accented', '_', compare_model_label, '.png'))
    filename[p + length(model_names)] <- 
      file.path(
        get_path('../figures/plotly/'), 
        paste0('p.3d.categorization.model_', model_names[p], '_L2-accented', '_', compare_model_label, '.png'))
    filename[p + 3 + length(model_names)] <- 
      file.path(
        get_path('../figures/plotly/'), 
        paste0('p.3d.categorization.difference.model_', model_names[p], '_', compare_model_label, '.png'))
  }
  
  filename = c(file.path(get_path(paste0('../figures/plotly/p.model_comparison_', example_label, '.png'))), filename)
  return(filename)
}
```

(ref:show-model-categorization-3D-plots-best-performing) Using the best-performing parameterization for each model, the three change models predict different categorization functions when applied to the data from Case Study 2. From **left to right:** predictions for changes in representations, decision-making, and normalization. **Top row:** Predicted recognition accuracy from the three change models. **Second row:** Predicted categorization functions after L1-accented exposure. **Third row:** Same but after L2-accented exposure. **Bottom row:** Differences in predicted posterior log-odds of /d/ between the two exposure conditions. Blue indicates higher predicted posterior log-odds of /d/ in the L2-accented exposure condition, relative to the L1-accented exposure condition. Red indicates the opposite. Gray indicates a difference of 0. The three models make distinct predictions about how exposure affects the perception of specific tokens across the VOT-f0 space.

```{r compare-AA-models-results-across-three-mechanisms-best-performing, warning=FALSE}
# load best-performing parameterization for the cue-reweighting case
example_label = "Cue_reweighting"
best_performing_parameters.representations <- 
  readRDS(get_path(paste0("../models/best_performing_parameters.representations_", example_label,".rds")))
best_performing_parameters.bias <- 
  readRDS(get_path(paste0("../models/best_performing_parameters.bias_", example_label,".rds")))
best_performing_parameters.normalization <- 
  readRDS(get_path(paste0("../models/best_performing_parameters.normalization_", example_label,".rds"))) 

# rename the example_label to distinguish the saved files from those used in the main presentation of the 'Cue_reweighting' case; 
# here only plot the model predictions using the best-performing parameterization
compare_model_label = "best-performing"
example_label = paste0(example_label, "_best-performing") 

prior_kappa.selected <- best_performing_parameters.representations$par[1]
prior_nu.selected <- best_performing_parameters.representations$par[2]
bias.parameters.select <- 
  d.AA.bias %>% 
  filter(posterior.lapse_rate %in% posterior.lapse_rate[which.min(abs(posterior.lapse_rate - best_performing_parameters.bias$par[1]))] & 
           beta_pi %in% beta_pi[which.min(abs(beta_pi - best_performing_parameters.bias$par[2]))]) %>%
  distinct(posterior.lapse_rate, beta_pi)
lapse_rate.selected <- bias.parameters.select$posterior.lapse_rate
beta_pi.selected <- bias.parameters.select$beta_pi
prior_kappa.normalization.selected <- best_performing_parameters.normalization$par[1]


data1 <- 
    get_representation_models_for_plot(
      prior_kappa = prior_kappa.selected,
      prior_nu = prior_nu.selected)

data2 <- 
    get_decision_making_models_for_plot(
      lapse_rate = lapse_rate.selected,
      beta_pi = beta_pi.selected)

data3 <- 
    get_normalization_models_for_plot(prior_kappa.normalization = prior_kappa.normalization.selected) %>%
    filter(prior_kappa.normalization == prior_kappa.normalization.selected)

compare_models.AA(
  data1,
  data2,
  data3,
  VOT_range, f0_range, n_points)
```

```{r show-model-categorization-3D-plots-best-performing, fig.sep = c('\\newline', '', '', '\\newline', '', '', '\\newline', '', '', '\\newline'), fig.show='hold',fig.align='center', out.width= c("100%", rep("33%",9)), out.height=c(rep("15%",4)), fig.cap="(ref:show-model-categorization-3D-plots-best-performing)", fig.subcap=c('Predicted recognition accuracy from changes in representations, decision-making and normalization', 'Representations: \\newline{}L1-accented', 'Decision making: \\newline{}L1-accented', 'Normalization: \\newline{}L1-accented', 'Representations: \\newline{}L2-accented', 'Decision making: \\newline{}L2-accented', 'Normalization: \\newline{}L2-accented', 'Representations:\\newline{} difference', 'Decision making: \\newline{}difference', 'Normalization: \\newline{}difference')}
filename <- make_plot_compare_models.AA(compare_model_label = "Cue_reweighting_best-performing")
knitr::include_graphics(filename)
```

In our case studies, we followed the approaches that continue to be employed in the majority of experiments on adaptive speech perception. In Case Study 1, we analyzed changes in categorization responses at six different test items. Following the majority of research on perceptual recalibration [for exceptions, see @drouin2016; @saltzman-myers2021], we did not further relate these changes to the _acoustic or phonetic properties_ of the exposure or test stimuli [for related discussion, see also @clayards2018; @theodore2021]. Similarly, in Case Study 2, we analyzed changes in accuracy without relating these changes to the acoustic-phonetic properties of the test stimuli [as remains the norm in the field, e.g., @bradlow-bent2008; @clarke-garrett2004; @sidaras2009; @tzeng2016; @xie2018jasa; @zheng-samuel2020]. <!-- Like some previous studies [e.g., @xie2016jep; @zheng-samuel2020], we showed these changes separately for the two categories investigated in the case study. Other studies on accent adaptation further simplify the dependent measure and analyze only overall improvements in accuracy [@bradlow-bent2008; @sidaras2009; @tzeng2016] or processing speed [@clarke-garrett2004].-->

Although in common use, these types of analyses over aggregated data constitute a missed opportunity. They discard stimulus-level data that could otherwise provide valuable information on the nature of the mechanisms underlying adaptive speech perception: theories and models that make the same predictions for aggregated data (like accuracy) might be distinguishable when compared against more fine-grained data. For experiments on adaptive speech perception, this means that researchers stand to benefit from analyzing changes in the _categorization function_ in response to recent exposure---i.e., changes in the mapping from acoustic-phonetic properties of the test stimuli to participants' responses.

We illustrate this point in Figure \@ref(fig:show-model-categorization-3D-plots-best-performing), which shows the predicted categorization functions after L1- and L2-accented exposure for each of the three change models (specifically, for the best-performing parameterizations of the three models). While all three change models predict highly similar categorization accuracies (Figure \@ref(fig:show-model-categorization-3D-plots-best-performing)a), they differ in the predicted categorization functions after L2-accented exposure (third row of Figure \@ref(fig:show-model-categorization-3D-plots-best-performing)). The bottom row of Figure \@ref(fig:show-model-categorization-3D-plots-best-performing) further shows that the three change models differ qualitatively in what _type_ of change in the categorization function they predict after L2-accented exposure. Whereas the effect of L2-accented, compared to L1-accented, exposure can be complex for the representational model (bottom left panel), it is more constrained for changes in decision-making (bottom center panel) and normalization (bottom right panel). These differences reflect the computational limitations of each change models.

Critically, differences such as those shown in Figure \@ref(fig:show-model-categorization-3D-plots-best-performing) only become apparent when the data is analyzed at a sufficiently fine-grained level---i.e., by assessing changes in the mapping from acoustic-phonetic properties of stimuli to participants’ responses. Fitting ASP and similar models to the results of experiments is one way to conduct such analyses. Previous work has done so for competing normalization models [see, e.g., @mcmurray-jongman2011; @apfelbaum-mcmurray2015; @persson-jaeger2022; @richter2017; @xie2021cognition] and competing variants of representational change models [e.g., @harmon2019; @kleinschmidt-jaeger2016cogsci; @kleinschmidt2020; @tan2022]. Future work can employ ASP to compare the fit of all three change models---including combinations of them---to the results of experiments: with minimal modifications, the code we used to find the best-performing parameterizations in Case Study 2 can be used to find the parameterizations that best fit listeners' responses from a perception experiment. This will allow researchers to investigate what factors contribute to the relative engagement of different change mechanisms.

Alternatively, standard approaches to data analysis can be used to investigate changes in the mapping from acoustic-phonetic properties to participants’ responses [for an excellent review, see @schertz-clare2020]. This includes regression models as long as they employ appropriate linking functions [e.g., logistic or multinomial regression for categorization, @jaeger2008; @winter-wieling2016]. This approach is now increasingly common, taking advantage of stimulus-level variability within and across conditions to test hypotheses [e.g., @clayards2018; @idemaru-holt2020; @schertz2015]. Regression analyses can further be expanded into psychometric models [@wichmann-hill2001] by adding lapse rates and response biases [e.g., @clayards2008; @kleinschmidt2020], and can be fit in standard statistics software [e.g., \texttt{brms}, @burkner2017]. Optionally, such regression analyses can be enriched with predictive ASP simulations of the type we have employed in our case studies. Such simulations are computationally less demanding than _fitting_ ASP models to perception experiments, and can help guide the interpretation of results [for examples of approaches that mix predictive simulation with standard data analysis, see @bejjanki2011; @clayards2008; @hitczenko-feldman2016; @tan2021; @theodore-monto2019; @xie2021cognition; see also discussion in @bent-baeseberk2021]. To fully take advantage of the types of analyses we have discussed here, it will, however, be important to plan experiments with these analyses in mind. This leads to our next recommendation.


### Recommendation 3: Targeted sampling of the stimulus space
Our third recommendation is to obtain data that better characterize incremental changes in categorization functions that occur with exposure. Simply put, the more data an experiment yields on how different types of exposure change the shape and location of the categorization function, the easier it is to determine the relative engagement of the different change mechanisms. Each change model is subject to different computational limitations, and these limitations affect what types of changes in categorization functions the different change models can account for (see, e.g., Figure \@ref(fig:show-model-categorization-3D-plots-best-performing) in the previous section).

As described under Recommendation 1, the changes in categorization functions predicted by the different change models depend, among other things, on both the exposure and test stimuli. Yet, stimulus selection in research on adaptive speech perception continues to be rarely guided by explicit consideration of how to most effectively contrast hypotheses about the underlying mechanisms. By selecting exposure and test stimuli for which different change models make different predictions, researchers can increase the informativeness of future experiments. For exposure, both the _stimulus location_ in the acoustic-phonetic space---relative to listeners' prior expectations---and the _number of exposure stimuli_ affect the predictions of change models. Of these, the location of exposure stimuli determines the _type_ of change that is predicted to occur (e.g., shifts vs. changes in the slope of categorization function; changes in the relative weighting of different cues). By contrasting multiple exposure conditions, it is thus possible to further constrain what combinations of change models are likely to provide a good fit against listeners' behavior. Consider, for example, the additional simulations we provided at the end of Case Study 2. These simulations showed that the three change models differed in how well they can accommodate different accent properties (like contrast shift or cue reweighting). Future studies could thus employ targeted exposure to different accent properties to narrow down what combinations of change mechanisms underlie adaptive speech perception. Another example demonstrating the benefits of multiple exposure conditions comes from @kleinschmidt-jaeger2016cogsci: different groups of participants were exposed to different magnitudes of shifts in the VOT distribution for /b/ and /p/. This made it possible to compare how well different parameterizations of a representational change model fit participants' behavior across the different exposure conditions [for additional analyses and discussion, see also @kleinschmidt2020]. <!-- While some studies have employed multiple exposure conditions in perceptual recalibration and similar paradigms [e.g., @babel2019; @sumner2011], this approach remains under-utilized. -->

\begin{figure}[h]
\begin{center}
\includegraphics[width=1 \columnwidth]{`r get_path("../figures/diagrams/repeated-sampling.png")`}
\caption{Illustrating how repeated targeted sampling of categorization responses can shed light on the relative engagement of different change mechanisms. {\bf Panel A:} Listeners’ categorization functions in a hypothetical experiment with repeated tests after increasing exposure. {\bf Panel B:} Predicted categorization behavior (summarized as: changes in PSE \& slope at PSE) for different parameterization of ASP’s combined change models. Parameterizations that are compatible with at least one test (dashed lines) can be ruled out by multiple tests. {\bf Panel C:} Similarly, some subset combinations of change models (blue, e.g., only changes in decision-making) might be unable to fit all tests under {\it any} parameterization.}\label{fig:repeated-sampling}
\end{center}
\end{figure}
<!-- An example in Model 1 shows  the case of {$ \kappa_{/t/,0}$} = {$ \kappa_{/d/,0} $} = 256 and {$ \nu_{/t/,0} $} = {$ \nu_{/d/,0} $} = 64. Likewise, different parameters will be used for Models 2, 3, and for the case in which two of the models are used in combination. In all of them, many lines match (i.e., predict) responses in the first and the second tests; Much fewer would match all the tests. It follows that the more tests there are, the better we can constrain which models provide good fit for human responses, illustrating the benefit of dense sampling of human responses through repeated testing (Recommendation 3).-->

Similarly, the amount of exposure determines _how much_ change of that type is predicted.
Future work could use incremental testing *within* subjects to further increase the ability to contrast the predictions of different change models (Figure \@ref(fig:repeated-sampling)). Even when all three change models predict more or less the same outcome of adaptation, they can differ in the trajectory of changes they predict. ASP can be used to simulate how human listeners' behavior is predicted to change after different types and amounts of exposure, depending on the hypothesized combination of change mechanisms. Repeated and incremental sampling of human responses (e.g., as indicated as Tests 1-3 in Figure \@ref(fig:repeated-sampling)) can thus boost researchers' abilities to determine the relative engagement of the three different change mechanisms [@xie-kurumada2023].

Finally, researchers can benefit from selecting test stimuli to be maximally informative about changes listeners' categorization functions. Paralleling our recommendations for exposure stimuli, this includes considerations about both the location and the number of test stimuli. For example, to detect differences between the different categorization functions, an experiment needs to employ test stimuli that are sufficiently distributed across the acoustic-phonetic space. This is worth emphasizing since it is typically _not_ the case in experiments on perceptual recalibration and accent adaptation. For perceptual recalibration, experimenters tend to target stimuli that are expected to be maximally ambiguous. This makes sense if the goal is to detect the _existence_ of a shift in the categorization function. But it is far from optimal when the goal is to understand the relative engagement of different change mechanisms, which requires identifying the nature of changes in the categorization function (Recommendation 2).

Similarly, the standard approach to selecting test stimuli in experiments on accent adaptation is likely to 'waste' statistical power. When test tokens are selected randomly from a database of L2-accented recordings, they are likely to cluster closely around the L2-accented category means. While this approach can be argued to maintain ecological validity, it makes it less likely that the experiment will distinguish between competing hypotheses: any change model that can predict general improvements in accuracy will also tend to correctly predict the categorization for stimuli close to the category means, making these types of stimuli relatively uninformative about the underlying mechanisms. Future experiments might instead employ more targeted selection of test stimuli by sampling test stimuli from regions of the acoustic-phonetic space for which the different change models make maximally distinct predictions [for initial efforts, see @burchill-jaeger2022].^[The optimal choice of stimuli depends on the specific goals of the experiment. For example, stimuli locations that are optimal for estimating the categorization function after one exposure condition, are not necessarily optimal for estimating the categorization function in another exposure condition, and neither choice might be optimal if the goal is to detect _differences_ between the two conditions or _differences_ between the different change models. Predictive (power) simulations can help determine the optimal stimulus locations and repetitions, for any of these goals (Recommendation 4).] While such targeted sampling is most easily achieved for paradigms that employ (re)synthesized or otherwise manipulated stimuli [for examples, see @bejjanki2011; @burchill-jaeger2022; @idemaru-holt2020], it can also be used in combination with naturally accented stimuli [see @chodroff-wilson2020].


### Recommendation 4: Predictive power analyses prior to conducting experiments
ASP can be used to estimate expected effect sizes and statistical power _before_ conducting an experiment. Unlike commonly used power estimates, which *assume* effect sizes [e.g., "moderate" effects, @zheng-samuel2020], predictive power-analyses of the type we envision are based on effect sizes that are derived from the acoustic properties of the very exposure and test stimuli used in the given experiment. This makes power simulations considerably more informative.

There are, however, challenges that need to be addressed before ASP can be used for predictive power simulations. For a specific set of exposure and test stimuli, and specific parameter settings for all change models, ASP predicts an expected categorization function. This is what we showed in, for example, in Figure \@ref(fig:show-model-categorization-3D-plots-best-performing) [see also, e.g., @tan2021; @theodore-monto2019; @xie2021cognition for similar approaches]. This categorization function can be used to derive a predictive power estimate by repeatedly sampling responses for the planned test stimuli and the planned number of participants. This can be done for any individual exposure conditions, for combinations of exposure conditions, and/or the predicted differences between exposure conditions.

However, the parameterization of the different models is typically not known to researchers—if they were, there would be no need for the present article. One important direction for future research will thus be to better characterize the functional consequences of each model's computational limitations.  For example, in the SI (\@ref(sec:consequences-of-lambda)), we show that changes in decision-making can only explain additive effects of exposure on the log-odds of categorization responses (for lapse rates of 0 or 1) or effects that resemble a step-function (for all other lapse rates). Further study of this and similar constraints will shed light on what range of adaptive behaviors each model can predict [for discussions of global, qualitative model comparisons, see @pitt2006; @apfelbaum-mcmurray2015]. Another related approach is to derive predictions and calculate statistical power while marginalizing (i.e., averaging) over a plausible _distribution_ of ASP parameters for the models they wish to contrast. In Bayesian terminology, this is achieved by defining "priors" over the ASP parameters. Initially, when relatively little is known about the relevant distribution of parameters, very weak priors are recommended that will consider a wide range of parameters as probable. As additional studies become available, this will further inform the range of plausible priors. Even now though, the plausible range of parameters is constrained by the fact _that_ listeners can adapt within a certain number of observations. This constrains the joint distribution of ASP parameters since any plausible parameterization needs to give at least one of the change models sufficient flexibility.

If researchers wish to know which exposure or test stimuli would increase the predicted difference between hypotheses, this further requires comparison of different stimulus selections. Comparable approaches exist for simpler computational problems and have been used extensively in psychometric research [incl. online stimulus selection during the experiment, depending on subject-specific performance, e.g., @vul2011; @prins2013]. We believe that this is an area of research with considerable potential.


### Recommendation 5: Integration with neural models of speech perception
Our final recommendation is to further integrate ASP with neural models of speech perception. ASP as put forward here is a model that makes behavioral predictions. Similarly fully specified models of the neural computations underlying adaptive speech perception are a bigger challenge and remain lacking [but see @sohoglu-davis2016; @sohoglu-davis2020; for discussion, see @guediche2014]. In the introduction, we outlined that models like ASP can support neuroimaging research by making more concrete---and thus more testable---the types of computations that are hypothesized to take place in different brain regions or networks. At the same time, future neuroimaging research can further constrain the hypothesis space, and inform frameworks like ASP. One notable step towards integrating computational models of speech adaptation with evidence from neuroimaging is presented in @sohoglu-davis2016. Sohoglu and Davis describe the possible neural implementations of both a categorization model and a change model for category representations [see also @sohoglu-davis2020]. The categorization model involves two interacting areas. One is the frontal areas associated with decision-making. These areas encode the relative evidence for the different categories, p(category | cues, context), which eventually is used to decide that a category has been 'recognized'. The other is the superior temporal gyrus that encodes predictions about phonetic inputs. These predictions are a function of the mapping from categories to cues, p(category | cues), weighted by the relative expectations for the categories coded in the frontal areas, p(category | context). The difference between these predictions and the observed phonetic inputs results in a prediction error that is used to update the activation of categories in frontal decision-making areas, changing p(category | cues, context). This part of Sohogulu and Davis' model---initially validated by experiments in @sohoglu2014---thus can be seen as implementing ASP's categorization model (without normalization). @sohoglu-davis2016 further propose that changes in category representations reflect changes in predictive coding in the superior temporal gyrus [see also @blank2016; @wang2021]. Exactly how these changes come about remains open, constituting a question of importance for future research.

As in behavioral studies, the integration of computational models such as ASP will increase the specificity of how---through what mechanism---a given brain area or neural network relates to behavioral changes in response to recent exposure. Existing results have suggested that adaptive changes in speech perception involve several different brain regions [@luthra2020a]. These include cerebellum [@guediche2014], early auditory regions [anterior planum temporale, @kilianhutten2011; @bonte2017] and regions responsible for representing phonemes and syllables [e.g., posterior superior temporal gyri/superior temporal sulcus, @bonte2017; @myers-mesite2014; @ullas2020], as well as regions for talker recognition [right temporal regions, @luthra2020b]. The left parietal lobe and the insula, which are implicated in perceptual decision-making [e.g., @dacremont2013; @keuken2014], have also been shown to exhibit distinct activation for different exposure conditions.

While these regions have been found to respond differently to different exposure conditions (e.g., familiar vs. unfamiliar talker accents, see for example, @adank2012neural; @holmes2021speech), most of them are also known to play multiple roles in cognitive processing. The exact interpretation of the results therefore depend largely on the individual researcher's assumptions about the underlying mechanism. For instance, the activation of left parietal lobe might reflect its general role in perceptual decision-making [e.g., @dacremont2013; @keuken2014], or it could be due to a more specific role in phonological processing [e.g., processing abstract category information, @guediche2014]. Similarly, changes in inferior frontal gyri activation in response to accented speech has been interpreted as reflecting greater computational demand [@yi2014neural] or decision-related phonetic categorization of ambiguous stimuli [@myers-mesite2014]. This ambiguity is compounded by univariate analysis methods which test an increase or a decrease of neural activity while largely leaving open the information content represented in the distinct brain regions. Increased specificity in the links between acoustic-phonetic cues and expected changes of recognition, as facilitated by ASP, can help resolve this ambiguity.

Recent research has begun to apply multivariate analyses that can be effectively combined with the model-based approach we have described above. Multivariate analyses can be used to identify the encoding of perceptual experiences across a distinct array of experimental conditions, which helps to distinguish between cognitive models that make different predictions regarding the *type* of information encoded by different brain regions and networks [e.g., @blank2016]. An additional advantage of multivariate analyses is that they are more sensitive in detecting fine-grained patterns *within* regions [e.g., @bonte2017; @luthra2020a]. Representational similarity analysis (RSA) of fMRI data allows researchers to take advantage of specific similarity matrix for a set of stimuli, rather than just the overall activation level for each condition. By selecting test stimuli so as to maximize the contrast between the similarity matrices of different change models, RSA can generate novel insights on the computations performed by different brain regions. Model-guided experimental designs as described in Recommendations 1-4 can facilitate such stimuli selection.

The synergy between behavioral, computational, and neural studies may uncover new knowledge about the relative engagement of the three mechanisms over time. @myers-mesite2014 found that boundary shifts in a perceptual recalibration study were initially driven by changes in the brain regions responsible for decision-making in the absence of changes in acoustic-phonetic representations (as indexed by STG activation). However, over the course of 20 critical exposure trials, evidence of retuned perceptual sensitivities slowly emerged such that the brain activity within STG became more distinguishable for boundary tokens and non-boundary tokens. The authors concluded that this indicates a "transfer of decision-related or lexical-level information to more bottom-up or perceptual processes in the temporal lobes" (p. 91). To extend this finding, future research should track brain responses over a more extended period of exposure. ASP models' abilities to predict incremental changes for multiple change mechanisms make it a tool well-suited to aid the design of such new experiments.

Finally, another promising avenue is to pair temporally-sensitive techniques with imaging methods with good spatial resolution. For instance, recent electrocorticography studies have provided some direct evidence for prelinguistic cue-level normalization within the middle and posterior STG areas [@johnson-sjerps2021; @tang2017; see also @sjerps2019], although it remains an open question how normalization supports the kind of adaptive changes as observed in perceptual recalibration or accent adaptation. Studies employing a combination of EEG and MEG have also been used to tease apart the role of signal properties and prior expectations in regulating perceptual learning of degraded speech: although both properties seem to affect activation of the same region in STG, they operate over different timescales [@sohoglu-davis2016; @sohoglu-davis2020]. Temporally-sensitive methods are also helpful in distinguishing between distinct mechanisms that may underlie adaptive speech perception. Using P200 as an index for acoustic-phonetic processing, @romero2015 found that the extraction of spectral information and other acoustic features was more difficult for foreign-accented speech than for native speech. Critically, this difference in acoustic-phonetic mapping did not attenuate within a single exposure session, compared to faster improvements in lexical-semantic processing, as indexed by N400. To the extent that the P200 and N400 reflect acoustic-phonetic vs. post-lexical processes, respectively, these results are compatible with the possibility that neural changes associated with decision-making criteria [driven by the prediction error experienced during lexical processing, cf. @delaney2019; @kuperberg2016] occur faster than those responsible for acoustic-phonetic remapping.


## Conclusion
We have introduced a theoretical and computational framework for adaptive speech perception (ASP). ASP formalizes three distinct mechanisms of speech perception ('categorization') and adaptive speech perception ('learning'), ranging from low-level auditory (normalization), to linguistic (category representations), and cognitive processes (decision-making). In the present paper, we have presented specific categorization and change models that aimed to be general-purpose, capturing the most common assumptions shared among theories of speech perception. By writing this article in R markdown, we hope to have made it easier for other researchers to revisit any of the assumptions we made, e.g., by substituting alternative models of normalization, category representations, and decision-making. Finally, the simulations in our case studies explored what can (and cannot) be concluded from previous work about the three mechanisms, and how future research can develop behavioral and neuroimaging studies that shed light on the relative engagement of these mechanisms.
