# General discussion

Listeners' ability to adaptively change their interpretation of the speech signal as a function of recent exposure is now understood to play a central role in spoken language comprehension. This insight is in large parts the result of research over the last two decades. In the present study we have focused on two of those lines of research that have been influential in shaping researchers' understanding of adaptive changes in speech perception: perceptual recalibration and accent adaptation.
Our ASP framework and the case studies demonstrated that existing results do not distinguish between adaptive changes caused by updates in pre-linguistic signal normalization, category representations or post-linguistic decisions. Contrary to common beliefs<!--[@bent-baeseberk2021; @quam-creel2021; @hay2019; @kleinschmidt-jaeger2015; @sjerps2019]-->, low-level, pre-linguistic signal
transformation/normalization or changes in response biases during decision-making are sufficient to explain signature results that are often tied to changes of representations. 

We note that the indeterminacy of existing results is even more problematic when we take into account that research often seeks to extrapolate beyond the specific paradigm. For example, one common motivation for perceptual recalibration and other paradigms that employ synthesized or otherwise manipulated speech is that they provide experimenters with increased control over the stimuli and yet can shed light on the mechanisms that underlie adaptation to naturally occurring accents. However, this type of argument—which we have made in our own work—requires certainty that the different paradigms examine the same mechanism(s) [for more discussion, see @zheng-samuel2020]. A main take-home point of our ASP-based simulations is that currently we are not well-equipped to achieve such certainty. In fact we know even less about what mechanisms yield the results of our experiments than seems to have often been assumed. <!--And this means that there is value in extending future applications of these paradigms in ways that allow researchers to determine which mechanism drives their results. The recommendations we present in the next section are meant to move us closer to this goal.--> 

This does not, of course, mean that normalization, representation, and decision-making mechanisms always predict similar response patterns---they do not. Nor does it mean that the three mechanisms cannot be distinguished from each other experimentally. Rather, it suggests that current standards of behavioral experiments and data analyses fall short of generating/testing predictions that enable contrastive tests of the mechanisms. In what follows, we discuss insights emerging from ASP and make concrete recommendations for future experiments on adaptive speech perception. We then close with discussions on how more decisive empirical tests will facilitate theoretical and empirical advances in uncovering mechanisms of human speech perception.

## Methodological advances that can move the field forward 
The case studies highlighted the significant roles that listeners' longterm experience plays in adaptive speech perception. We submit that future experimenters ought to operate under the null hypothesis that speech perception is strongly affected by expectations about the distributional realization of linguistic categories based on long-term experiences. This assumption is now shared by most theories of speech perception. It is also emphasized by recent reviews of the field, which highlight the significance of distributional properties of the speech input for perception [@bent-baeseberk2021; @schertz-clare2020; @quam-creel2021; @kurumada-roettger2021]. However, informal reasoning about the consequences of this null hypothesis can quickly gain in complexity, making it more likely that *ad-hoc* or *post-hoc* reasoning results in misleading conclusions. Consider the highly influential study “The Weckud Wetch of the Wast” [@maye2008]. The study exposed listeners to speech in which the vowel categories had undergone systematic (phonological) shifts. After just 20 minutes of exposure, listeners’ interpretation of subsequent speech input from the same talker had changed substantially [for replications and extensions, see also @weatherholtz2014]. Based on additional control conditions, Maye and colleagues concluded that this finding could not be explained by “general relaxation of the criterion for what constitutes a good exemplar of the accented vowel category” but rather argued that participants had learned that the talker’s categories were systematically shifted. A recent study revisited this result by formalizing the competing accounts [@hitczenko-feldman2016]. They explicitly encoded assumptions about the listeners' prior expectations and tested how these expectations can change in response to the input using the same ideal adaptor models that we have employed here [@kleinschmidt-jaeger2015]. They found that the results of Maye et al. (2008) were just as well, if not better, predicted by a learning model that expanded the categories, rather than shifting them.

An analytical approach like ASP can be a powerful tool for principled hypothesis testing [see also @apfelbaum-mcmurray2015; @sohoglu-davis2016; @tan2021; @theodore-monto2019; @toscano2018; @xie2021cognition]. It provides an alternative to informal reasoning about changes in speech perception. It makes explicit that two types of information affect listeners’ interpretation of speech from an unfamiliar talker: (1) the listener’s prior expectations based on the statistics of previously experienced speech input; and (2) the statistics of the present speech input relative to those prior expectations. In an exposure-test experiment like those discussed in our case studies, (1) is further divided into two sub-components: (1a) expectations based on the speech statistics experienced prior to the experiment; and (1b) the input experienced during the exposure phase. In other words, the input received during an experiment incrementally changes the prior expectations that the listener brings into a subsequent perceptual response. The three change models we described spell out competing hypotheses about *how* (1b) is integrated with (1a). The categorization model (adapted based on exposure input) determines how these changes are expected to affect listeners’ responses during the test phase (see Figure \@ref(fig:overview-change)). In what follows we discuss how this predictive power of ASP can be most effectively used in future experiments.

<!--Beyond providing qualitative predictions, the current approach can be used to derive quantitative predictions based on particular linking hypotheses about mappings between acoustic cues and speech categories. Specifically, we can use the analytical framework to predict *when*, *to what acoustic-phonetic inputs* and *after how much exposure* listeners would show adapted responses under the three mechanistic hypotheses. Below we make four concrete recommendations to achieve this goal. First, experiments should be desgined to densely sample exposure and test items that are expected to yield distinct behavioral responses under each mechanistic model (Recommendation 1). Second, model predictions need to be spelled out in terms of acoustic-phonetic details of the input (Recommendation 2). Third, predictive power-simulations using the analytical framework will ensure the statistical power needed for constrastive tests of the three hypothesized mechanisms (Recommendation 3). Finally, it is critical that listeners' responses are analyzed at the stimulus level (Recommendation 4). --> 

<!-- As another example, consider a recent study by Zheng and Samuel investigates whether perceptual recalibration and accent adaptation involve the same mechanisms. Zheng and Samuel (2020) approach this question through the lens of individual differences between listeners: if perceptual recalibration and accent adaptation involve the same mechanisms, participant-specific differences in perceptual recalibration and accent adaptation should correlate. Zheng and Samuel had participants complete a series of experiments on the perception of the /?/- /s/ contrast (e.g., thin vs. sin), including both perceptual recalibration and accent adaptation paradigms. Zheng and Samuel found no significant correlation between participants’ performance in the two paradigms. Power analyses presented by Zheng and Samuel suggested that their study had more than 80% power to detect a moderate correlation (?? ≥ .4). Based on this, Zheng and Samuel concluded that “the null effect is not plausibly due to insu?icient power” (p. 1281) and cautiously conclude their “results provide no support for the view that recalibration of phonemic boundaries plays a central role in natural accent accommodation.” (p. 1286). 

But do their results really provide evidence against this hypothesis? Taking at face value the assertion that the study provided 80% power to detect a moderate correlation, the real question is what magnitude of correlation researchers should expect if the hypothesis that both perceptual recalibration and accent adaptation share the same mechanism is true. The to-be-expected magnitude of such a correlation depends on many factors that jointly affect the test-retest reliability of the two paradigms (and thus the researchers’ ability to reliable estimate individual differences in these tasks; see also discussion in Zheng and Samuel, 2020). This in turn depends on assumptions about the perceptual noise, attentional lapses, and other factors that affect the trial-to-trial consistency of participants. It also depends on the amount of data collected from participants, and whether it is elicited and analyzed in ways taking into account that repeated testing can reduce the effects of recent exposure, reducing power if not appropriately considered by the analyses (L. Liu and Jaeger 2018, 2019; Luthra et al. 2020; Theodore 2021; see also, discussion in Norris, McQueen, and Cutler 2003). Finally, the actual statistical power of analysis like that by Zheng and Samuel also depends on the statistical properties of the exposure and test stimuli employed in the perceptual recalibration and accent adaptation experiments: how and how much exposure is expected to affect subsequent speech perception depends on the specific placement of exposure and test stimuli relative to listeners’ prior expectations. In short, there are many variables that interact with each other in a complex manner.[JF2] -->

### Recommendation 1: Dense and targeted sampling of the stimulus space
While the ASP change models for all three mechanisms can account for the signature results of perceptual recalibration and accent adaptation paradigms, they do also make different predictions. 
<!-- The predictions of all three of the models are constrained by: 1) the prior state of listeners; 2) the acoustic/phonetic properties as well as category labels of the exposure stimuli; and 3) the acoustic/phonetic properties of the test stimuli. For any combination of 1)-3), each change model predicts a response distribution across test tokens that can be compared to actual human responses.  -->
Given (1) an estimate of listeners state prior to the experiment and (2) a set of exposure and test stimuli, these predictions are mediated only through the parameters of the change model (e.g., $\beta_{\pi}$ for changes in response biases), limiting the range of results a change model can account for. That is, instead of merely comparing the ability in which each mechanism captures the *qualitative* pattern of behavioral changes, we can compare the three mechanisms by assessing their accuracy in *quantitatively* predict and explain human responses across a variety of experimental conditions. 

Figure \@ref(fig:show-model-categorization-3D-plots) illustrates this point for the data from Case Study 2. For each of the three change models, we take the parameters that resulted in the highest post-L2-exposure accuracy during test, and plot the predicted categorization functions for both exposure conditions. These are the categorization functions that a listener would employ while categorizing test stimuli. Despite the fact that the three change models resulted in qualitatively similar categorization accuracy in Case Study 2, the three models differ in *how* they achieve this accuracy. Specifically, they predict different categorization functions over the VOT-f0 space. 

(ref:show-model-categorization-3D-plots) The three change models predict different categorization functions when applied to the data from Case Study 2. From **left to right:** predictions for in changes in representations, decision making, and normalization. **Top:** Predicted categorization functions after L1-accented exposure. **Middle:** Same but after L2-accented exposure. **Bottom:** Differences in predicted posterior log-odds of /d/ between the two exposure conditions. Blue indicates higher predicted posterior log-odds of /d/ in the L2-accented exposure condition, relative to the L1-accented exposure condition. Red indicates the opposite. Gray indicates a difference of 0. The three models make distinct predictions about how exposure affects the perception of specific tokens across the VOT-f0 space.

```{r compare-AA-models-results-across-three-mechanisms, warning=FALSE}
d.AA1 <- 
  d.AA %>%
  filter(category == Item.Intended_category) %>%
  filter(prior_kappa == 4 & prior_nu == 1024) %>%
  droplevels() %>%
  distinct(Condition, Subject, data, prior_kappa, prior_nu, prior, posterior, posterior.categorization)

d.AA2 <- 
  d.AA.bias %>%
  filter(category == Item.Intended_category) %>%
  filter(posterior.lapse_rate == min(d.AA.bias$posterior.lapse_rate) & beta_delta_epsilon == max(d.AA.bias$beta_delta_epsilon)) %>%
   droplevels() %>%
  distinct(Condition, Subject, data, posterior.lapse_rate, beta_delta_epsilon, prior, posterior, posterior.categorization)

d.AA3 <- 
  d.AA.normalization %>%
  filter(category == Item.Intended_category) %>% 
  filter(normalization == "centered based\non exposure") %>%
  filter(prior_kappa.normalization == 4) %>%
  droplevels() %>%
  distinct(Condition, Subject, data, prior_kappa.normalization, mu_inferred, prior, posterior, posterior.categorization)

d.model.compare <- 
  bind_rows(
    d.AA1 %>%
      mutate(model = "Representations"),
    d.AA2 %>%
      mutate(model = "Decision_making"),
    d.AA3 %>%
      mutate(model = "Normalization"))

# Set parameters
limits <- get_plot_limits(p2)
VOT_range = c(0, limits$xmax)
f0_range = c(limits$ymin, limits$ymax)
n_points = 100
cue_names = c("VOT", "f0")
model_names = c("Representations", "Decision_making", "Normalization")
category.contrasts = c("/d/", "/t/")
# Set image size
p_width = 900
p_height = 900

for (j in 1:length(model_names)){
  data = d.model.compare %>%
    filter(model == model_names[j])
  
  for (i in 1:length(conditions.AA)){
    output.AA <- prepare_3D.categorization_from_results(data, VOT_range, f0_range, n_points)
    
    df.resp = output.AA[[i]]
    
    p.3d.categorization <- plot_3D.categorization(df.resp) %>%
      layout(
        margin = margin,
        scene = list(
          camera = list(
            eye = list(x = -0.5, y = -2.5, z = 0.1)) # perspective good for showing categorization curve
        ))
    
    save_figure_or_not(p.3d.categorization, paste0('p.3d.categorization.model_', model_names[j], '_', conditions.AA[i], '.png'))
  }
  
  # show differences between exposure conditions in log odds space
  df.diff = list()
  df.diff$d_diff = qlogis(output.AA[[2]]$d_prop) - qlogis(output.AA[[1]]$d_prop)
  df.diff$x = df.resp$x
  df.diff$y = df.resp$y
  
  p.3d.categorization.diff <- plot_3D.categorization.diff(df.diff) %>%
    layout(
      margin = margin,
      scene = list(
        camera = list(
          eye = list(x = -0.5, y = -2.5, z = 0.1))))
  
  save_figure_or_not(p.3d.categorization.diff, paste0('p.3d.categorization.difference.model_', model_names[j], '.png'))
}
```

```{r show-model-categorization-3D-plots, fig.ncol = 3, fig.show='hold',fig.align='center', out.width="33%", out.height="49%", fig.cap="(ref:show-model-categorization-3D-plots)", fig.subcap=c('Representations: L1-accented', 'Decision making: L1-accented', 'Normalization: L1-accented', 'Representations: L2-accented', 'Decision making: L2-accented', 'Normalization: L2-accented', 'Representations: difference', 'Decision making: difference', 'Normalization: difference')}
filename = c()

for (p in 1:length(model_names)) {
  filename[p] = file.path(get_path('../figures/plotly/'), paste0('p.3d.categorization.model_', model_names[p], '_L1-accented.png'))
}

for (p in 1:length(model_names)) {
  filename[p + length(model_names)] = file.path(get_path('../figures/plotly/'), paste0('p.3d.categorization.model_', model_names[p], '_L2-accented.png'))
}

for (p in 1:length(model_names)) {
  filename[p + 3 + length(model_names)] = file.path(get_path('../figures/plotly/'), paste0('p.3d.categorization.difference.model_', model_names[p], '.png'))
}

knitr::include_graphics(filename)
```

This points the way to approaches that can more clearly distinguish between the three alternative mechanisms. Intuitively, paradigms and analyses should take advantage of the 'richness' of the data. Most immediately, this is achieved by moving beyond comparisons that are limited to changes in overall categorization accuracy or processing speed, towards analyses that directly assess the link between the acoustic/phonetic properties of the speech input and listeners' responses (see Recommendation 3). Such approaches are now increasingly common [e.g., @clare2019; @idemaru-holt2020; @kartushina2015; @kim2020; @liu-holt2015; @schertz2015; @wade2007]. In order to reliably detect changes in categorization functions, it can be beneficial to more densely sample the acoustic/phonetic space during the test phase. Strategic placement of test stimuli can be used to increase the statistical power to test predicted differences between the different change models [@burchill-jaeger2022]. For example, the bottom row of Figure \@ref(fig:show-model-categorization-3D-plots) suggests that denser sampling of certain VOT-f0 combinations should make it possible to distinguish between the three change models. 

Moving beyond simple contrasts between two exposure conditions can further facilitate comparison between change models. The more exposure conditions an experiment employs, the more accurately and reliably the parameters of the competing change models can be estimated, increasing the power of the model comparison. For instance, @kleinschmidt-jaeger2016cogsci and @kleinschmidt2020 created six different between-subject exposure conditions with distinct distributions of VOT. By examining the degree of adaptive changes listeners exhibited in these six conditions, the researchers could estimate their prior expectations more reliably than when only one exposure condition is employed [see also @babel2019; @sumner2011]. Similarly, incremental testing *within* subjects can increase statistical power to contrast the predictions of different change models. Even when all three change models predict the same *outcome* of adaptation, they can differ in the trajectory of changes they predict. While incremental testing paradigms exist [e.g., @bertelson2003; @vroomen2007; @bonte2017], they remain under-utilized, and model comparisons based on such experiments remain lacking [but see @kleinschmidt-jaeger2012]. 

Either approach---denser sampling of exposure conditions or denser sampling of test tokens---is not limited to paradigms that employ synthesized or otherwise manipulated stimuli (e.g., perceptual recalibration or standard distributional learning paradigms). For example, @chodroff-wilson2020 took advantage of naturally occurring variability, and created different exposure conditions by selecting different subsets of natural stimuli. This approach strikes a particularly intriguing balance between ecological validity and experimental control.

Finally, comparison across exposure and test conditions might also be achieved through meta-analyses. For example, an ongoing project in our labs pursues this latter approach by constructing a database of over 100,000 categorization responses from dozens of perceptual recalibration experiments on the English /s/-/`r linguisticsdown::cond_cmpl("ʃ")`/ contrast. Each of these experiments employs slightly different exposure and test stimuli, potentially increasing the statistical power to distinguish between the competing models. Such item-level meta-analyses benefit immensely from open science standards that facilitate (or even require) the sharing of data in well-documented formats. 

<!-- We highlight three considerations for such paradigms. First, it is important to keep in mind computational feasibility. For example, unsupervised distributional learning paradigms that employ unlabeled exposure in randomized order [@clayards2008; @munson2011; @nixon2016] in theory provide extremely informative data, as each individual trial constitutes a test trial with its own unique exposure history. However, in practice the computational costs of fitting hundreds to thousands of different exposure conditions make this approach infeasible. Second, testing---especially, when it involves distribution of test stimuli that deviate from those during exposure [e.g., uniform distributions that result from repeating each test stimulus equally often as is standard in most experiments]---is expected to reduce the effects of exposure [@liu-jaeger2018; liu-jaeger2019; @REFS; for discussion, see @theodore2021]. Third, any factors that might lead participants to be uncertain whether the stimuli during test come from the same source (e.g., talker) as those during exposure should weaken the transfer of exposure effects to test [@kleinschmidt-jaeger2015]. [XX9][JF10]This could be, for example, due to the wording of instructions or simply because test begins with a new block with a different task than exposure (as is almost always the case in perceptual recalibration, and not uncommon in accent adaptation). 
We therefore recommend designs with a small number of within-subject tests, each sufficiently short compared to the length of exposure, and exposure phases that are shared by a sufficiently large number of participants (so that their effect can be reliably estimated). In recent work, we have approached the question of what constitutes “sufficient” through a combination of power analyses and pilot experiments [@burchill-jaeger2021]---an admittedly time- and resource-consuming approach that has, however, yielded a much better understanding of our data. [XX11]Finally, whenever possible, test and exposure phases should transition seamlessly into each other. Alternatively, instructions should use accessible language to highlight whether the upcoming stimuli come from the same source. -->

### Recommendation 2: Advance standards of data annotation, reporting, and sharing
As made clear in the case studies, it is critical to formulate clear linking hypotheses that describe the mapping from the acoustic/phonetic input to perception [see also @apfelbaum-mcmurray2015; @tanenhaus2004]. It is imperative that researchers acknowledge this link, and advance their standards of data reporting accordingly. We recommend that it should become standard for studies on speech perception to report the relevant phonetic properties of their stimuli. For the paradigms like the ones we have discussed here, this would include phonetic annotations of both the exposure and test stimuli. As an added benefit, phonetic annotations also allow more sophisticated and natural stimulus manipulation [see the insightful critique offered in @theodore2021].

In our view, such annotations entail a manageable effort: perception experiments typically employ a small number of speech stimuli that are repeated for each participant. A typical perceptual recalibration experiment would require the annotation of less than 100 isolated word recordings. A large study on accent adaptation like Bradlow and Bent’s (2008) Experiment 2 would require the annotation of about 1000 sentences. Studies on phonetic production regularly annotate data sets many times larger. These efforts towards a more open, collaborative science can further be supported by clear standards for reproducibility and software developments that aid phonetic annotation and data sharing [e.g., @cassidy-schmidt2017; @roettger2019; @picoral2021; @winkelmann2017]. If annotations are not reported and shared---and ideally even if they are---then all audio recordings should be shared in an open and accessible way (e.g., OSF). This will require perception researchers to elicit recordings in ways that gives them the consent to distribute these recordings for the purpose of scientific inquiry. In other words, perception researchers should follow the same standards that adopted by researchers working on language production. 

### Recommendation 3: Simulation and power analysis prior to conducting testing 
Like other analytical frameworks, ASP can be used to estimate expected sizes of effects before conducting an experiment. Unlike commonly used power estimates---which assume expected effect sizes (based on previous work or arbitrarily set to e.g., “moderate”)---a fully specified change model can derive predicted effect sizes under a hypothesized mechanism, based on the distribution of acoustic or phonetic cues in the input. This will make power-analyses less arbitrary and more informative.

In addition to phonetically annotated exposure and test stimuli, such power analyses require estimates of listeners’ prior expectations at the start of the experiment. Because these estimates aim to capture expectations based on input previously experienced throughout listeners’ lives, these estimates entail substantially more effort than the annotation of experimental stimuli. Fortunately, large and phonetically annotated databases can offer good estimates of the speech input of listeners receive. Such databases are now available for an increasing number of phonetic contrasts and languages [e.g., @chodroff-wilson2018; @clopper-pisoni2006; @hillenbrand1995; @newman2001; @theodore2009; @xie-jaeger2020, among many others]. One caveat is that many of these databases either are representative only of a subset of the speech varieties that an average listener of a language has plausibly been exposed to, or contain very little data for each variety. In particular, databases that contain both a large number of talkers and a large number of tokens per talker continue to be the exception. Researchers need to carefully consider these implications when employing databases. In some cases, researchers might find that the best way forward is to collect phonetically annotated data that meets the specific requirements for their study. @xie2021cognition, for instance, collected ~3000 production tokens of prosodic categories and used them to model listeners’ responses in a perception experiment<!--[see also @REFS]-->. We have also found that sometimes phonetically annotated data from a single typical talker can be sufficient to get started—e.g., to gain an initial understanding of one’s results [@tan2021] or to conduct power simulations. 

One alternative option is to eschew phonetically annotated data in favor of computational methods that work with the raw signal or some automatically obtained transformation of the raw data [such as Mel-Frequency Cepstral Coefficents, (MFCC) @Mermelstein1976]. Such models have been developed for automatic speech recognition [for an earlier review, see @jurafsky-martin2000] and have been employed to model human language acquisition [@dupoux2018; @feldman2013; @toscano-mcmurray2010; @vallabha2007]. More recently, they have also been applied to address questions about the effects of recent exposure [@richter2017]. The general framework we have described here can be combined with such models.

### Recommendation 4: Data analysis beyond overall accuracy and speed 
To take advantage of the richer types of data described in the previous sections, it is also necessary to conduct analyses that go beyond changes in the overall accuracy or speed of comprehension. In particular, analyses that directly link relevant acoustic or phonetic properties of the input to listeners’ responses strike us as promising. This should take into account that listeners rely on a multitude of cues [e.g., @mcmurray-jongman2011] even when the experiment manipulates only one of them. Such analyses could employ the type of models we have described in this study. Alternatively, even simple regression can suffice as long as it employs appropriate linking functions [e.g., logistic regression for categorization, @agresti2019; @jaeger2008]. This approach is now increasingly common, taking advantage of stimulus-level variability within and across conditions to test hypotheses. These regression analyses can be expanded to accommodate lapse rates and response biases [e.g., @clayards2008; @kleinschmidt2020], and can be fit in standard statistics software [e.g., through \texttt{brms} @burkner2017 or other libraries in \texttt{R}]. A comprehensive introduction to such models is provided in @wichmann-hill2001.

Indeed, there are direct connections between the model we have employed here and lapsing logistic regression. For example, two Gaussian categories along a phonetic continuum result in linear effect of the continuum on the posterior log-odds of the categories if the two categories have identical variance, but linear and quadratic if the two categories have unequal variance [e.g., @kleinschmidt-jaeger2015; @kronrod2016]. This prediction can only be appreciated if logistic regression is employed for the analysis of 2AFC categorization tasks, rather than ANOVA or similar methods. @schertz-clare2020 provide an excellent overview of how standard regression analyses can be---and have been---used to assess the effects multiple phonetic cues on listeners’ responses.  

## Distinguishing between the mechanisms -- How will that advance theories of speech perception? 
Resolving the empirical indeterminacy in mechanistic theories of speech perception has potentially far-reaching implications. Before closing, we highlight three avenues in which the current approach will help expand the horizon of future studies. 

### Facilitating theory building 
Besides the two classes of experimental paradigms we detailed above, there are a wider range of results cited as evidence for or against a particular mechanism underlying accurate and robust speech perception. In particular, the assumption that recent exposure can shape linguistic representations, or at least their weighting or selection (X. Xie et al., 2018), is central to many accounts of sociophonetics and exemplar-based theories of speech perception more broadly (for review, see Hay et al., 2019; Kleinschmidt & Jaeger, 2015; Sjerps et al., 2019). Multiple non-linguistic factors have been shown to inform and impact the selection, such as the (inferred) physiology [@krauss2002], social identity of a talker [e.g., regional origin, @hay-drager2010; @niedzielski1999; sex, @johnson1999; @strand1999; age, @walker-hay2011; @skoogwaller2015; and individual identity, @nygaard1994; @remez2018], or a situational context [e.g., being in a car, @hay2017]. These effects are thought to draw on episodic memory traces from the listeners' past linguistic experiences and hence inexplicable based soley on lower-level auditory signal normalization. Conversely, a separate line of research has found that non-speech stimuli (e.g., pure tones) can systematically alter the perception of subsequently played speech stimuli [@chodroff-wilson2020; @holt2005; @holt2006; @huang-holt2011], which seems to be harder to explain through changes in, or a selection of, representations. Tested separately on distinct types of stimuli within distinct experimental procedures, these theoretical positions have coexisted for decades by now. 
 
The current results urge us, and hopefully other researchers in the field, to revisit this equilibrium. The empirical indeterminacy we identified for perceptual recalibration and accent adaptation likely applies to *any* form of exposure-test paradigm, regardless of whether the exposure stimuli being researcher curated (in the case of non-speech sound exposure) or natural (in the case of exposure to various talkers throughout lifetime). As shown in Case Studies 1 and 2, depending on the listeners' prior expectations and exact stimulus features in the input, normalization and changes in decision-biases can predict _a wider range of highly complex response patterns than previously recognized_. This means that none of the three mechanisms can be simply dismissed without explicit, empirical tests. In other words, any behavioral experiment that does not annotate/examine stimulus features and analyze link between those features and participants' responses cannot be claimed to support one of the hypotheses over the others.

### Do the mechanisms combine or trade-off? If so, when?
The second avenue in which we anticipate a breakthrough pertains to development of theories that delve into different assumptions about each mechanism. For instance, it has so far been assumed that distributional learning of category statistics [as shown by @clayards2008 and subsequent replications and extensions @bejjanki2011; @nixon2016; @theodore-monto2019] entails representational changes. Specifically, participants that were exposed to /b/ and /p/ categories with larger variance along VOT exhibited more shallow categorization slopes than participants who were exposed to /b/ and /p/ categories with smaller variance (but the same means as the other condition). These changes may not be easily explained by a cue-based normalization mechanism such as C-CuRE. This does not, however, rule out explanations in terms of *other* normalization. For example, normalization that standardizes cues relative to expectations, in addition to centering them---which we might call *S*-CuRE (for "standardizing")--- may be able to account for the variance shifts found by Clayards et al. (2008). Further, it is possible that listeners may learning distributional structures *post* low-level normalization. e.g., They might first normalize/standardize cues [as has been proposed for the perception of vowels @lobanov1971; @monahan-idsardi2010; @johnson2020] and then learn and store residual idiosyncrasies [For a preliminary test of this idea, see @xie2021cognition]. Yet alternatively, the findings of Clayards et al. (2008) might also be accounted for by changes in response biases provided that the lapse rate is not zero: recall that changes in response biases can have non-additive effects when the lapse rate is non-zero (Section \@ref(sec:change-bias)). And non-zero lapse rates were indeed observed by Clayards et al. (2008, Figure 3B and footnote 2). Changes in response biases could thus potentially account for changes in the slope of categorization functions---the result observed in Clayards et al. (2008). 

A related thought-provoking possibility may be that distinct mechanisms can combine or trade off with one another. We have so far attempted to contrast the three mechanisms as contenders. ASP, however, will also help us move beyond this either-or comparisons and assess the relative involvement of the mechanisms as a function of stimulus features and the amount of input. This is important as the three mechanisms could be distinct in terms of cognitive and memory demands and hence differentially costly. For example, to arrive at a precise estimate of a given talker's cue mean (for normalization), listeners must receive at least some amount of exposure to the talker. The minimum amount of exposure required for deriving category means and variances (for learning category representations) is expected to be larger, especially for variance estimates. This makes the representational learning mechanism relatively costly and taxing in terms of memory demands. On the other hand, the decision-bias changes do not require any storage of acoustic-phonetic details, and hence are relatively (computationally) cheap. It is possible that L1 listeners who encounter an unfamiliar accent might initially resort to the decision-making changes to boost their recognition accuracy. As they accrue input to a given talker or accent, they may begin to rely more on the other two mechanisms. Relatedly, listeners might opt to expend more resources to learn distributional properties of categories when doing so would in fact be expected to improve recognition accuracies. They might be less likely to do so when L2 categories are, for instance, phonetically overlapping with one another (e.g., Mandarin-accented talkers tend to pronounce [`r linguisticsdown::cond_cmpl("θ")`] (as in *thick*) as [s] (as in *sick*), Zheng and Samuel (2020)). In such a scenario, the bottom-up signal does not distinguish between intended categories, which makes cue normalization or representational learning rather inefficient as a strategy. Instead, simply increasing the rate of [`r linguisticsdown::cond_cmpl("θ")`] independent of the stimulus could more efficiently improve recognition accuracies. Indeed, a recent MEG study by Blanco-Elorrieta et al., (2021) found strong engagement of a pre-frontal cortex in processing phoneme substitutions (e.g., [s] mispronouced as [`r linguisticsdown::cond_cmpl("ʃ")`]), implicating post-perceptual decision-making processes. Testing these fine-grained hypothesis would require careful parameterization of acoustic-phonetic features, categories/contrasts, and amounts of exposure within subjects. The current analytical framework and the new standards of experimental design and analyses laid out above will be indispensable for this endevour.


<!--there are distinct lines of work that normaliz as episodic  taken to suggest that listeners have implicit representations that encode expectations about talkers and types of talkers. These findings do, however, leave open whether these expectations relate to pre-linguistic normalization or to linguistic categories [see also discussion of "relativization" in @apfelbaum-mcmurray2015, p. 936-938]. They also leave open to what extent recent (e.g., within-experiment) experience affects speech perception through the same mechanisms that underlie the effects of inferred physiology or social identity. (This includes the question of whether and how recent experiences can lead to the learning of new linguistic representations. (e.g., Can listeners *learn* characteristics of a talker within 2 mins of exposure?) The approach and the recommendations provided above will make it possible to explicitly test these subtly distinct possibilities, which have not thus far possible.--> 


<!-- This result is predicted by distributional learning accounts of representational changes [as discussed by @clayards2008; @kleinschmidt-jaeger2015] but is not as easily explained by C-CuRE normalization. It does not, however, rule out explanations in terms of other normalization. For example, normalization that standardizes cues relative to expectations, in addition to centering them---which we might call *S*-CuRE (for "standardizing")---should be able to account for the findings of Clayards et al. (2008). <!-- TO-DO later: confirm through simulation -->	<!-- Such standardization is part of several of the most influential approaches to formant normalization that have been proposed for the perception of vowels [@lobanov1971; @monahan-idsardi2010; @johnson2020]. The findings of Clayards et al. (2008) might also be accounted for by changes in response biases provided that the lapse rate is not zero: recall that changes in response biases can have non-additive effects when the lapse rate is non-zero (Section \@ref(sec:change-bias)). And non-zero lapse rates were indeed observed by Clayards et al. (2008, Figure 3B and footnote 2). Changes in response biases could thus potentially account for changes in the slope of categorization functions---the result observed in Clayards et al. (2008).--> 


<!-- These findings have, however, been challenged [@pitt2016]. The current approach can be used to explicitly model the effects of accumulating exposure to non-linguistic inputs in terms of representational changes or decision-making, for a direct comparison of their fit to human response behaviors. Furthermore, the modeling approach can be straightforwardly extended to test whether a _combination_ of mechanisms could approximate human data. This will help address the intuitive, yet computationally-complex, possibility that human listeners may store and learn distributional characteristics of speech categories in an acoustic-phonetic space *post* normalization [@xie2021cognition]-->

<!---### Neuro-imaging study results--> 

### Neurobiological bases of adaptation
The primary goal of the current investigation has been to make strong inference through behavioral testing. Once achieved, it will facilitate more targeted neuro-imaging research. As discussed in the introduction, shifted categorization boundaries in response to the same physical input have been associated with multiple regions, such as early auditory regions [anterior PT, @kilianhutten2011], regions implicated in phoneme classification [posterior STG/STS, @bonte2017; @myers-mesite2014; @ullas2020], as well as regions for talker identity processing [right temporal regions, @luthra2020a]. The left parietal lobe and the insula, which are implicated in perceptual decision-making [e.g., @dacremont2013; @keuken2014], have also been shown to exhibit distinct activation for different exposure conditions [@bonte2017; @myers-mesite2014; @ullas2020]. As in behavioral experiments, existing results leave open which neural networks are responsible for the behavioral changes observed after recent exposure. Even a significant activation of a given brain region can be associated with functionally-distinct sources. For instance, the activation of left parietal lobe might reflect its general role in perceptual decision making [e.g., @dacremont2013; @keuken2014], or it could be due to a more specific role in phonological processing [e.g., processing abstract category information, @guediche2014]. One limitation of these works is that the conclusion is based primarily on a binary distinction in a brain region’s activation pattern (e.g., activity between categorization trials that a /d/ response is made and those in which a /t/ response is made for the same stimulus). More recently, neuroimaging studies have started to approach questions about the underlying neural mechanisms through multivariate analyses. These analyses are crucial for identifying the encoding of perceptual experiences across a distinct array of experimental conditions. Multivariate analyses can also be more sensitive in detecting fine-grained patterns *within* regions responsible for multiple cognitive demands [e.g., @bonte2017; @luthra2020a]. The methodological recommendations we made above may help generate behavioral responses that enable multivariate analyses required for further dissociation among regions functionally responsible for exposure-driven changes.

Finally, Another promising avenue is to pair temporally-sensitive techniques with imaging methods with good spatial resolution. For instance, studies using a combination of EEG and MEG have identified key regions that regulate perceptual learning of degraded speech by responding to manipulations of either low-level signal clarity or high-level prior knowledge of the speech content [@sohoglu-davis2016; @sohoglu-davis2020]. 



# Conclusion
<!-- 
While there is now a convergence of different approaches---including both behavioral and neuroimaging paradigms---existing findings leave open what neural and cognitive mechanisms underlie adaptive changes in speech perception. In this context, our case studies support two take-home points: i) that far less is known about what mechanisms yield the results of our experiments than seems to be often assumed; and ii) that future research on changes in speech perception stands to benefit greatly from more rigorously defined linking hypotheses. With regard to the former point, we note that the indeterminacy of existing results is even more problematic when we take into account that research often seeks to extrapolate beyond the specific paradigm. For example, one common motivation for perceptual recalibration and other paradigms that employ synthesized or otherwise manipulated speech is that they provide experimenters with increased control over the stimuli and yet can shed light on the mechanisms that underlie adaptation to naturally occurring accents. However, this type of argument---which we have made in our own work---requires certainty that the different paradigms study the same mechanisms. And this means that there is value in extending future applications of these paradigms in ways that allow researchers to determine which mechanism drives their results. The recommendations we presented in General Discussion are meant to move us closer to this goal.--> 

<!-- In our experience, the use of even simple models also facilitate deeper engagement with the results of our experiments, including a clearer understanding of which results ought to surprise us and which ought not to (see also apfelbaum-mcmurray2014?; Hitczenko and Feldman 2016; Kleinschmidt and Jaeger 2011; Schertz and Clare 2020; Tan, Xie, and Jaeger 2021). We have begun to use this approach in several ongoing projects, and have found it to be highly insightful. Simulations based on the hypothetical or actual phonetic properties of stimuli have allowed us to detect flaws in our reasoning prior to conducting the experiment, and to adjust designs accordingly. 
Furthermore, the framework we have introduced here can be used to inform comparisons across experiments, tasks, types of phonetic contrasts, and even languages (see Tan, Xie, and Jaeger 2021). One common approach to gain generalizable insights into a given mechanism is to examine it using multiple accents/languages (e.g., experiments with similar designs using Dutch-accented (Eisner et al., 2013) and Mandarin-accented (Xie et al., 2017) English). Researchers also attempt to contrast multiple paradigms (e.g., selective adaptation vs. perceptual recalibration vs. accent adaptation paradigms) in terms of how long-lasting the effects obtained in each of them may be (Samuel et al., 2021). One major caveat associated with these approaches is that it is not trivial to equate all aspects of experiments in a way so that a fair and meaningful comparison is possible. Even when researchers try to control everything but one factor—be it a particular phoneme contrast or a paradigm—they often come with a host of differences such as the exact acoustic features of the stimuli and their distributions. Given everything that is now known about speech perception, these distributional properties of both prior input and the present input will be critically and jointly affecting the listeners’ behavior. With this in mind, frameworks like the one presented here can also be used predictively prior to conducting experiments. This could include power simulations for existing designs, or more active guidance for the choice between different designs (e.g., the choice of exposure and test stimuli). [JF5]-->

Through formalizing and simulating three distinct mechanisms of adaptive speech perception, we explored how these mechanisms may be empirically distinguished from one another. Even after all the possible steps we envision are implemented, however, our inferences would always be constrained by the assumptions we derive about the data as well as about the underlying cognitive, perceptual and computational architecture. As researchers, we must therefore be aware of our own theoretical biases or dispositions to make them explicit in our theory development. A general conclusion we can draw from the current investigation is that, for processes as complex as human speech perception, it is hard to make strong inference without an aid of analytical tools like the one we put forward here.
