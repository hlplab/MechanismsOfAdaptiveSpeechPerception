\newpage
\setcounter{page}{1}
\renewcommand{\thesection}{\S \arabic{section}}

# Supplementary information for *Xie, Jaeger, & Kurumada (2022). What we do (not) know about the mechanisms underlying adaptive speech perception* {-}
\setcounter{section}{0}

Both the main text and these supplementary information (SI) are derived from the same R markdown document available via OSF at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). It is best viewed using Acrobat Reader. Some links and animations might not work in other viewers. 

# Required software {#sec:SI-software}
The document was compiled using \texttt{knitr} [@xie2021] in RStudio with R:

```{r} 
version
```

You will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or the R library \texttt{tinytex}). The 3D figures require the [\texttt{orca} commandline tool](https://github.com/plotly/orca#installation). It is recommended that you use "Method 4" to install the standalone binaries.

We used the following R packages to create this document: `r cite_r("latex-stuff/r-references.bib", withhold = T, pkgs = c("MVBeliefUpdatr"))`. If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. This includes the \texttt{MVBeliefUpdatr} library that supports working with multivariate Gaussian ideal observers and adaptors (see also Dave Kleinschmidt's \texttt{beliefupdatr} library that this library is based on). The full session information is provided at the end of this document.

# A database of natural productions of word-initial stop voicing in American English [@chodroff-wilson2018] {#sec:SI-chodroff}

All case studies presented in the main text are based on a phonetically annotated database of word-initial stop voicing in American English [@chodroff-wilson2018]. Chodroff and Wilson (2018, p. 3) describe the database:

> The data was extracted from an audited subset of the Mixer 6 corpus (Brandschain et al. 2010, Brandschain
et al. 2013; Chodroff et al. 2016) containing approximately 45 minutes of read speech from 180 native AE
speakers (102 female). Transcripts were aligned to the corresponding WAV files with the Penn Forced Aligner
(Yuan and Liberman 2008), and all word-initial prevocalic stop consonants were further processed with
AutoVOT (Keshet et al. 2014). AutoVOT automatically identifies the stop release and following vowel onset
within a user-specified window of analysis. Further details about the talkers, read sentences, and boundary
alignments can be found in Chodroff and Wilson (2017).³

> COG, positive VOT, and onset f0 in the following vowel were measured for each stop. COG was calculated
from a smoothed spectrum over the initial portion of the release burst. Each spectrum was computed by averaging
FFTs from seven consecutive 3 ms windows, with the first window centered on the burst transient and
a window shift of 1 ms (Hanson and Stevens 2003; Flemming 2007; Chodroff and Wilson 2014). Positive VOT
was defined as the duration from stop release to the onset of periodicity in the vowel; this was automatically
extracted from the AutoVOT boundaries or from manually-corrected boundaries when available. The
f0 value was the first one measured by Praat (Boersma and Weenink 2016) within 50 ms after the following
vowel onset.

<!-- 
# From email communication with Eleanor Chodroff (10/14/2021):
# 
# filename + trial uniquely identify an observation. Trial = the TextGrid interval number for a 
# particular session recording, which each have a unique filename (that includes the subject ID, 
# the date, and some other info). Adding "subj" won't hurt. I wouldn't rely on "word" just in 
# case they happened to produce multiple instances of the same word within the recording. 
#
# vot: 
#   VOT in ms. VOT is reported in the LingVan 2018 paper.
#
# cog, spectral.var, skew, kurtosis: 
#   these are the summary statistics of the power spectral density measures in each of the 33 bins, 
#   where each bin is 250 Hz wide. COG = Center of gravity (Hz), spectral.var = spectral variance 
#   (take the square root to get the SD, a more palatable number and one that will be in Hz), 
#   skew = spectral skewness, kurtosis = spectral kurtosis. COG is reported in the LingVan 2018 paper
#   See Forrest et al. 1988 for a good description of these measures.
#
# usef0:
#   f0 measures were taken every 5 ms after the start of the vowel up to 50 ms into the vowel 
#   (see f0_1 to f0_10 columns). usef0 is the first defined instance of f0. usef0 is reported in the 
#   LingVan 2018 paper.
-->

Following advice from Eleanor Chodroff, we removed tokens with f0 measurements of above `r max.f0` Hz, as those were implausible and likely reflected a measurement error (pitch doubling). We further subset the data talkers for which all three cues (VOT, COG, and f0) were available for at least `r min.observation.n` observations each per stop category. This was done because we detected that the f0 of a good number of talkers exhibited evidence of bimodality. To remove talkers with bimodal f0 measures, we applied a test of multimodality [the dip test, as implemented in the library \texttt{diptest} in \texttt{R}, @maechler2021]. Restricting our data set to talkers with at least `r min.observation.n` observations each per stop category allowed us to more reliably identify bimodal f0 distributions. Talkers for which the null of unimodality was rejected ($p<$ `r max.p`) for the raw f0 or Mel-transformed f0 of *any* stop category were excluded.^[We thank Eleanor Chodroff for help with this issue. Her inspection of some example tokens revealed that both creaky voice and pitch halving contributed to the bimodal pattern. Pitch halving refers to cases in which the f0 detection algorithm wrongly infers the f0 to be half of its true value. One reason for such estimation mistakes can be when the true f0 falls outside the range considered by the algorithm (here: 75-500 Hz, regardless of talker gender, Eleanor Chodroff, p.c.).]

This left `r nrow(d.chodroff_wilson)` observations from `r nlevels(d.chodroff_wilson$Talker)` different talkers and `r nlevels(d.chodroff_wilson$Word)` words. These are the data that we used to derive plausible estimate of the overall and category-specific VOT and Mel distributions that underlie the case studies presented in the main text. Figure \@ref(fig:chodroff-stop-VOT-f0ST) shows three randomly selected talkers from this data set prior to C-CuRE normalization. Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu) further illustrates the degree of cross-talker variability prior to normalization by plotting all talkers' category means relative to the category likelihood.


```{r chodroff-parameters}
n.subj <- 3
seed <- 59876
```

(ref:chodroff-stop-VOT-f0ST) Unormalized VOT and f0 of word-initial stop consonants in American English from `r n.subj` random talkers in the Mixer corpus (Chodroff & Wilson, 2017). Transparent points show individual tokens, which cover a large of different phonotactic, lexical, and utterance contexts. Solid points show talker-specific means over these tokens, connected by a gray line. Ellipses show the 95% probability mass for talker-specific bivariate Gaussian categories. 

```{r chodroff-stop-VOT-f0ST, fig.width=base.width * 3 + 1, fig.height = base.height * 3, fig.cap=c("(ref:chodroff-stop-VOT-f0ST)")}
talkers <- sample(unique(d.chodroff_wilson$Talker), n.subj, replace = F)

p.talkers <- d.chodroff_wilson %>% 
  # Keep random subject of subjects
  ungroup() %>%
  filter(Talker %in% talkers) %>%
  ggplot(aes(x = VOT, y = f0_Mel, color = voicing, shape = gender)) +
  geom_point(alpha = .1) +
  stat_ellipse() +
  scale_x_continuous(expression("VOT (ms)")) +
  scale_y_continuous(expression("f0 (Mel)")) +
  scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) +
  scale_shape_discrete("Gender") +
  scale_size_continuous("n", range = c(.1, 3)) +
  facet_grid(Talker ~ poa) 

p.talkers +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(group = Talker),
    alpha = .9, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    alpha = .9)
```

(ref:chodroff-stop-VOT-f0ST-mu) By-talker means of VOT and f0 for all six stop consonants and all `r nlevels(d.chodroff_wilson$Talker)` talkers in the Mixer corpus (Chodroff & Wilson, 2017) for which all phonetic cues were available. Ellipses show the 95% probability mass for talker-independent bivariate Gaussian categories if the data from all talkers are pooled independent of talker identity.

```{r chodroff-stop-VOT-f0ST-mu, fig.width=base.width * 3 + 1, fig.height = base.height, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-mu)")}
p.means <-
  d.chodroff_wilson %>% 
  ggplot(aes(x = VOT, y = f0_Mel, color = voicing)) +
  stat_ellipse() +
  scale_x_continuous(expression("VOT (ms)")) +
  scale_y_continuous(expression("f0 (Mel)")) +
  scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) +
  scale_shape_discrete("Gender") +
  scale_size_continuous("n", range = c(.1, 3)) +
  facet_grid(. ~ poa)

p.means +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(group = Talker),
    alpha = .1, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(shape = gender),
    alpha = .3)
```

## Applying C-CuRE
We follow @mcmurray-jongman2011 and use linear regression to remove the effects of talker from each observation in the database. In extending normalization accounts to our present goals, we encountered a few decision points that we have not previously seen discussed:

  1. C-CuRE removes the *overall* cue mean from each observation's cue value. Based on this, it would seem most appropriate to include all six stop categories in the estimation of the cue means. However, the experiments we model in the main text involve only two of the categories (/d/ and /t/). We thus decided to only include tokens of /d/ and /t/ in the estimation of cue means. This decision was made because C-CuRE is intended to correct observed cue values for listeners' expectations *for the current context*. Put differently, we assume that listeners expect that the sound will be a /d/ or a /t/ since these are the only two response options provided to participants. This assumption affects some of our results (but not as much as it would if we studied /b/-/p/ or /g/-/k/ since the mean cue values for /d/s and /t/s are closer to the mean cue values across all six categories). 
  
  2. C-CuRE is meant to correct for effects of both talkers and *phonological* contexts. However, in the @chodroff-wilson2018 data /d/ and /t/ occur across different phonological contexts, so that inclusion of phonological contexts as a predictor in the regression model would indirectly capture information about category identity. We thus limited the data further to 9 pairs of /d/- and /t/-items that had identical vowels following the stop consonant (e.g, _*d*ied_ and _*t*ime_). While this does not completely remove the effects of phonological context, it both reduces the variability in cue values associated with phonological contexts (thus providing a more accurate estimate of the expected outcome of normalization) and *balances* the effect of phonological context (deconfounding it from category identity). 
  
  3. Finally, the original data from @chodroff-wilson2018 contained nearly three times as many /d/s as /t/s, and this asymmetry was retained after the data cleaning procedures described above. While this particular imbalance is probably not reflective of natural speech, natural speech will often exhibit *some* form of asymmetry in the relative frequencies of categories. This raises the question of whether listeners somehow correct for these asymmetries when normalizing the speech input for an experiment for which there all categories can be expected to be equally frequent. Following similar considerations as in Point 1 above, we assume this to be the case. We thus randomly subsampled equal numbers of tokens for each /d/ and /t/ from all talkers. This number was determined by the number of tokens of the less frequent category for each talker. For instance, if a talker produced 40 /d/s and 20 /t/s, then 20 /d/ and 20 /t/ observations were randomly pulled from the data. 

The issues described here will only arise whenever the distribution of categories and/or phonological contexts suddenly changes. In those moments, the estimated mean will reflect expectations that do not veridically reflect the statistics of the current input. In previous evaluations of C-CuRE this was never the case: both the distribution of categories and the distribution of phonological contexts during test were identical to the distributions that the C-CuRE model was trained on [@mcmurray-jongman2011]. For the present study, we investigate how listeners might transfer and adapt expectations based on previous long-term exposure to novel input from an unfamiliar talker. We therefore used subsamples that were well-balanced between /d/ and /t/ tokens in terms of their frequency and phonological contexts---a pattern matched to those employed during the exposure and test in typical experiments. That said, none of our core results seem to depend on the assumptions we make here. We obtained qualitatively identical results for all critical comparisons in previous simulations that did not subsample the data to be balanced with regard to /d/ and /t/ or their phonological contexts.

<!-- ^[\label{fn:c-cure-subsampling} Of note, previous applications of C-CuRE (and other normalization models) have been applied to *balanced* data, in which each relevant category occurs equally often. For example, McMurray and Jongman's seminal (2011) study on American English fricatives centered each cue based on a database in which all 8 fricatives occurred equally often. Critically, McMurray and Jongman then also *tested* the C-CuRE model against perceptual data from an experiment in which all eight fricatives occurred equally often. This might mask a computational complexity of normalization accounts when applied to speech perception beyond balanced experiments: in the everyday speech input that listeners' expectations for normalization are derived from, different phonetic categories tend to occur with different frequencies, and these frequencies tend to depend on the current phonological context. Without any correction for this imbalance, listeners' cue expectations will thus be biased by the relative frequency of the different categories in the current context. It is an open question whether (and how) listeners adjust their expectations to correct for such imbalances. Here, we assume that listeners' cue expectations reflect their expectations for the relative frequency of categories in the current context. For details, see SI (\@ref(sec:SI-chodroff)).] -->

Decisions 1-3 left a total of 5632 observations (2816 each for /d/ and /t/). To avoid over-fitting to individual talkers, we use linear mixed effects regression rather than ordinary linear regression. Separate regressions were use to predict VOT and f0 (Mel). Both regressions used the formula:

\begin{equation}\label{eq:c-cure-regression}
\begin{split}
cue \sim 1 + (1 | Talker)
\end{split}
\end{equation}

Rather than to just use the residuals of these regressions, we only subtracted the random effects (BLUPs) from each observation. This removes the talker-specific effects from each token (as intended by C-CuRE) but leaves observations in the original cue space (rather than residual VOTs centered around 0), thus achieving talker normalization without loss in interpretability.

(ref:chodroff-stop-VOT-f0ST-normalized) Same as Figure \@ref(fig:chodroff-stop-VOT-f0ST) but for normalized VOT and f0. Normalization does not affect the relative placement of tokens within the talker's space but it does affect the absolute placement. This is also evident in Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu-normalized)

```{r chodroff-stop-VOT-f0ST-normalized, fig.width=base.width * 3 + 1, fig.height = base.height * 3, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-normalized)")}
p.talkers + 
  aes(x = VOT_centered, y = f0_Mel_centered) +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(group = Talker),
    alpha = .9, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    alpha = .9)
```

(ref:chodroff-stop-VOT-f0ST-mu-normalized) Same as Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu-normalized) but for normalized VOT and f0---both for the means and for the 95% ellipse.

```{r chodroff-stop-VOT-f0ST-mu-normalized, fig.width=base.width * 3 + 1, fig.height = base.height, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-mu-normalized)")}
p.means +
  aes(x = VOT_centered, y = f0_Mel_centered) +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(group = Talker),
    alpha = .1, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(shape = gender),
    alpha = .3)
```

# Graphical models of change models {#sec:SI-models}

## Changes in representations {#sec:SI-models-changes-in-representations}

Figure \ref{fig:model-belief-updating} visualizes the belief-updating process of the $\mathcal{NW^{-1}}$ model in factor graph notation.

\begin{figure}
  \centering
  \tikz{ %
    % exposure
    \node[obs] (cue) {$x_n$} ; %
    \factor[above=of cue] {cuedist} {left:$\mathcal{N}$} {} {}; %
    \node[obs, right=of cuedist] (category) {$c_n$} ; %
    % category parameters
    \node[det, above=of cuedist] (mu) {$\mu_c$} ; %
    \node[det, right=of mu] (Sigma) {$\Sigma_c$} ; %
    \factor[above=of mu] {mudist} {left:$\mathcal{N}$} {} {}; %
    \factor[above=of Sigma] {Sigmadist} {left:$\mathcal{W}^{-1}$} {} {}; %
    % hyperparameters
    \node[det, above=of mudist] (kappa) {$\kappa_{n,c}$} ; %
    \node[det, left=of kappa] (m) {$\mathrm{m}_{n,c}$} ; %
    \node[det, above=of Sigmadist] (S) {$\mathrm{S}_{n,c}$} ; %
    \node[det, right=of S] (nu) {$\nu_{n,c}$} ; %
    % prior
    \factor[above=1 of m] {update_m} {} {} {}; %
    \factor[above=1.05 of kappa] {update_kappa} {} {} {}; %
    \factor[above=.9 of S] {update_S} {} {} {}; %
    \factor[above=1 of nu] {update_nu} {right:update (Equation \ref{eq:niw-updating-parameters}) } {} {}; %
    \node[det, above=of update_m] (prior_m) {$\mathrm{m}_{0,c}$} ; %
    \node[latent, above=5 of update_kappa] (prior_kappa) {$\kappa_{0}$} ; %
    \node[latent, above=5 of update_nu] (prior_nu) {$\nu_{0}$} ; %
    \node[det, above=of update_S] (prior_S) {$\mathrm{S}_{0,c}$} ; %
    % external phonetic database
    \node[obs, above=of prior_m] (expected_mu) {$\mathbf{E}(\mu_{c})$} ; %
    \node[obs, above=of prior_S] (expected_Sigma) {$\mathbf{E}(\Sigma_{c})$} ; %
    % extra nodes to allow connection from cues to updating
    \node[const, left=1.2 of update_m] (update_extra1) {} ; %
    \node[const, left=3 of cue] (update_extra2) {} ; %
    \node[const, right=1 of update_nu] (update_extra3) {} ; %
    % plates
    \plate[inner sep=0.24cm, xshift=-0.06cm, yshift=0.12cm] {plate2} {(mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S) (prior_m) (prior_S) (expected_mu) (expected_Sigma) (update_extra3) } {$\forall c \in categories $}; %
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(cue) (category) (cuedist) (mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S)} {$\forall n \in exposure\ observation$}; %
    \edge {cuedist} {cue} ; %
    \edge {category} {cuedist} ; %
    \edge {mu, Sigma} {cuedist} ; %
    \edge {Sigma} {mudist} ; %
    \edge {kappa,m} {mudist} ; %
    \edge {nu,S} {Sigmadist} ; %
    \edge {mudist} {mu} ; %
    \edge {Sigmadist} {Sigma} ; %
    % updating
    \edge {prior_kappa} {update_kappa} ; %
    \edge {prior_nu} {update_nu} ; %
    \edge {prior_kappa, prior_m} {update_m} ; %
    \edge {prior_kappa, prior_m, prior_S} {update_S} ; %
    \edge {update_kappa} {kappa} ; %
    \edge {update_nu} {nu} ; %
    \edge {update_m} {m} ; %
    \edge {update_S} {S} ; %
    \edge[-] {cue} {update_extra2} ; %
    \edge[-] {update_extra1} {update_extra2} ; %
    \edge {update_extra1} {update_m} ; %
    \edge[-] {update_m} {update_kappa} ; %
    \edge[-] {update_kappa} {update_S} ; %
    \edge[-] {update_S} {update_nu} ; %
    % link to phonetic database
    \edge {expected_mu} {prior_m} ;
    \edge {expected_Sigma} {prior_S} ;
    \edge {prior_nu} {prior_S} ;
  }
  \caption{$\mathcal{NW}^{-1}$ Bayesian belief-updating model, as employed here. We set $\mathrm{m}_{0,c} = \mathbf{E}(\mu_c)$ and $\mathrm{S}_{0,c}=\mathbf{E}(\Sigma_c)$, where $\mathbf{E}(\mu_c)$ and $\mathbf{E}(\Sigma_c)$ are observable from phonetically annotated databases. We further assume that all categories share a common $\kappa_{0,c}$ and $\nu_{0,c}$, shown as $\kappa_{0}$ and $\nu_{0}$. These $\kappa_{0}$ and $\nu_{0}$ are the only two degrees of freedom in this highly simplified model of distributional learning (in the manuscript, we keep the $c$ subscript to avoid confusion with the parameter for changes in normalization in Equation \ref{eq:normalization-change}). All other variables are either observable (filled gray circles) or fully determined by other variables.}\label{fig:model-belief-updating}
\end{figure}

# Creating the stimuli for the (simulated) perceptual recalibration paradigm {#sec:SI-PR}

## Exposure
As described in the main text, the exposure phase of a typical perceptual recalibration experiment employs both typical stimuli and stimuli that are manipulated to be perceptually ambiguous between the two categories. In practice, researchers determine this point of perceptual ambiguity by first generating a continuum from the recording of the typical sound (e.g., _crocodile_) to a recording in which that sound is replaced with the opposite sound (_crocotile_). Procedures to create these continua range from simple blending of the two recordings---mixing the two recordings weighted by different amplitudes (from 100% _crocodile_ and 0% _crocotile_ to 0% "crocodile" and 100% _crocotile_)---to more careful phonetic manipulations (e.g., inserting addition silence to create longer VOTs) or the use of speech synthesis [for an insightful critique of the frequently used blending procedure, see @theodore-cummings2021]. The perceptually most ambiguous point along the resulting continua is then determined either in a separate norming experiment, or simply by experimenter listening to the stimuli.

For the sake of generality, we simulate the general outcome of any of these procedures by imaging an experimenter (or participants) listening to stimuli along a continuum ranging from typical `r categories.PR[1]` to typical `r categories.PR[2]`. We simulate the experimenter with the same perceptual model we assume for general L1-listeners (see Section \@ref(sec:framework)). We assume that the experimenter, who can listen arbitrarily often to any of the stimuli, will make her final decision as to which stimuli should be selected for the shifted tokens without attentional lapses ($\lambda=0)$. We further assume that experimenter's response bias is affected by the lexical context. Specifically, we set $\pi_{intended~category} =$ $p(intended~category | lexical~context) =$ `r my_experimenter.lexical_bias_strength`. This captures the fact that even the experimenter's perception is somewhat affected by the lexical context. <!-- Finally, we assume that the realizations of phonetic contrasts used in perceptual recalibration experiments are less variable than in naturally occurring speech, given that the stimuli for such experiments typically reflect *read* speech recorded in sound-attenuated booths, and experimenters would remove any 'atypical' recordings. Specifically, we arbitrarily assume that the variance of the typical (and shifted) stimuli in the experiment is $\frac{1}{`r my_experimenter.variability_reduction`}$th of that observed in natural productions. --> Figure \@ref(fig:PR-exposure-test-plot)A shows the phonetic properties of an instance of stimuli generated in this way.

## Test
For the test phase, we followed a similar procedure. We used the same continuum from a typical `r categories.PR[1]` to a typical `r categories.PR[2]`, and then selected six stimuli along that continuum that would be expected to yield `r paste(paste0(as.numeric(rev(unique(d.PR.test$Item.Intended_proportion_category1))) * 100, "%"), collapse = ", ")` `r categories.PR[1]`-responses in a norming experiment without any prior exposure to the talker's speech. Since the test tokens present the phonetic contrast in a non-biasing context (e.g., /`r linguisticsdown::cond_cmpl("ɪ")`\_`r linguisticsdown::cond_cmpl("ɪ")`/), we set the effect of lexical context to $p(category | lexical\ context) = .5$. The resulting test tokens shown in Figure \@ref(fig:PR-exposure-test-plot)B closely resemble the placement of test stimuli expected in experiments on perceptual recalibration [e.g., @norris2003; @kraljic-samuel2005; @kraljic-samuel2006]: as is typical for such experiments, test tokens are placed primarily where they are expected to be perceptually most ambiguous prior to exposure (i.e., close to the prior category boundary), with one additional test token towards each end of the continuum (typically somewhere between 10-25% and 75-90%, respectively). Our approach further assumes that the exposure and test stimuli are carefully chosen to have phonological contexts that allow generalization from exposure to test, as differences in the types of phonological context between exposure and test can reduce or even completely block perceptual recalibration [@eisner2013; @mitterer2013]. For example, the original study that we model our procedure on mostly used exposure stimuli in which /d/ or /t/ occurred word-medially at the onset of a syllable, between two vowels [e.g., _crocodile_, _academic_, etc., see Table 1 in @kraljic-samuel2006] or at least between two sonorants (e.g., _legendary_, _secondary_). In the test stimuli, the /d/-/t/ contrast occurred in a similar position (either /a_a/ or /`r linguisticsdown::cond_cmpl("ɪ")`\_`r linguisticsdown::cond_cmpl("ɪ")`/). 


<!-- # Additional considerations for ASP analyses of future experiments {#sec:more-recommendations} -->
<!-- For researchers, who plan to the ASP framework to analyze their experiments, we highlight three considerations. First, it is important to keep in mind computational feasibility. For example, unsupervised distributional learning paradigms that employ unlabeled exposure in randomized order [@clayards2008; @munson2011; @nixon2016] in theory provide extremely informative data, as each individual trial constitutes a test trial with its own unique exposure history. However, in practice the computational costs of fitting ASP to many different exposure conditions make this approach infeasible. Second, testing---especially, when it involves distribution of test stimuli that deviate from those during exposure [e.g., uniform distributions that result from repeating each test stimulus equally often as is standard in most experiments]---is expected to reduce the effects of exposure [@liu-jaeger2018; liu-jaeger2019; @REFS; for discussion, see @theodore2021]. Third, any factors that might lead participants to be uncertain whether the stimuli during test come from the same source (e.g., talker) as those during exposure should weaken the transfer of exposure effects to test [@kleinschmidt-jaeger2015]. This could be, for example, due to the wording of instructions or simply because test begins with a new block with a different task than exposure (as is almost always the case in perceptual recalibration, and not uncommon in accent adaptation).  -->

<!-- We therefore recommend designs with a small number of within-subject tests, each sufficiently short compared to the length of exposure, and exposure phases that are shared by a sufficiently large number of participants (so that their effect can be reliably estimated). In recent work, we have approached the question of what constitutes “sufficient” through a combination of power analyses and pilot experiments [@burchill-jaeger2021]---an admittedly time- and resource-consuming approach that has, however, yielded a much better understanding of our data. [XX11]Finally, whenever possible, test and exposure phases should transition seamlessly into each other. Alternatively, instructions should use accessible language to highlight whether the upcoming stimuli come from the same source. -->

