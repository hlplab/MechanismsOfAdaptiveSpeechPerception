\newpage
\setcounter{page}{1}
\renewcommand{\thesection}{\S \arabic{section}}

# Supplementary information {-}
\setcounter{section}{0}

Both the main text and these supplementary information (SI) are derived from the same R markdown document available via OSF at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). It is best viewed using Acrobat Reader. Some links and animations might not work in other viewers. 

# Required software {#sec:SI-software}
The document was compiled using \texttt{knitr} [@xie2021] in RStudio with R:

```{r} 
version
```

You will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or the R library \texttt{tinytex}). The 3D figures require the [\texttt{orca} commandline tool](https://github.com/plotly/orca#installation). It is recommended that you use "Method 4" to install the standalone binaries.

We used the following R packages to create this document: `r cite_r("latex-stuff/r-references.bib", withhold = T, pkgs = c("MVBeliefUpdatr"))`. If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. This includes the \texttt{MVBeliefUpdatr} library that supports working with multivariate Gaussian ideal observers and adaptors (see also Dave Kleinschmidt's \texttt{beliefupdatr} library that this library is based on). The full session information is provided at the end of this document.

# A database of natural productions of word-initial stop voicing in American English [@chodroff-wilson2018] {#sec:SI-chodroff}

All case studies presented in the main text are based on a phonetically annotated database of word-initial stop voicing in American English [@chodroff-wilson2018]. Chodroff and Wilson (2018, p. 3) describe the database:

> The data was extracted from an audited subset of the Mixer 6 corpus (Brandschain et al. 2010, Brandschain
et al. 2013; Chodroff et al. 2016) containing approximately 45 minutes of read speech from 180 native AE
speakers (102 female). Transcripts were aligned to the corresponding WAV files with the Penn Forced Aligner
(Yuan and Liberman 2008), and all word-initial prevocalic stop consonants were further processed with
AutoVOT (Keshet et al. 2014). AutoVOT automatically identifies the stop release and following vowel onset
within a user-specified window of analysis. Further details about the talkers, read sentences, and boundary
alignments can be found in Chodroff and Wilson (2017).³

> COG, positive VOT, and onset f0 in the following vowel were measured for each stop. COG was calculated
from a smoothed spectrum over the initial portion of the release burst. Each spectrum was computed by averaging
FFTs from seven consecutive 3 ms windows, with the first window centered on the burst transient and
a window shift of 1 ms (Hanson and Stevens 2003; Flemming 2007; Chodroff and Wilson 2014). Positive VOT
was defined as the duration from stop release to the onset of periodicity in the vowel; this was automatically
extracted from the AutoVOT boundaries or from manually-corrected boundaries when available. The
f0 value was the first one measured by Praat (Boersma and Weenink 2016) within 50 ms after the following
vowel onset.

<!-- 
# From email communication with Eleanor Chodroff (10/14/2021):
# 
# filename + trial uniquely identify an observation. Trial = the TextGrid interval number for a 
# particular session recording, which each have a unique filename (that includes the subject ID, 
# the date, and some other info). Adding "subj" won't hurt. I wouldn't rely on "word" just in 
# case they happened to produce multiple instances of the same word within the recording. 
#
# vot: 
#   VOT in ms. VOT is reported in the LingVan 2018 paper.
#
# cog, spectral.var, skew, kurtosis: 
#   these are the summary statistics of the power spectral density measures in each of the 33 bins, 
#   where each bin is 250 Hz wide. COG = Center of gravity (Hz), spectral.var = spectral variance 
#   (take the square root to get the SD, a more palatable number and one that will be in Hz), 
#   skew = spectral skewness, kurtosis = spectral kurtosis. COG is reported in the LingVan 2018 paper
#   See Forrest et al. 1988 for a good description of these measures.
#
# usef0:
#   f0 measures were taken every 5 ms after the start of the vowel up to 50 ms into the vowel 
#   (see f0_1 to f0_10 columns). usef0 is the first defined instance of f0. usef0 is reported in the 
#   LingVan 2018 paper.
-->

Following advice from Eleanor Chodroff, we removed tokens with f0 measurements of above `r max.f0` Hz, as those were implausible and likely reflected a measurement error (pitch doubling). We further subset the data talkers for which all three cues (VOT, COG, and f0) were available for at least `r min.observation.n` observations each per stop category. This was done because we detected that the f0 of a good number of talkers exhibited evidence of bimodality. To remove talkers with bimodal f0 measures, we applied a test of multimodality [the dip test, as implemented in the library \texttt{diptest} in \texttt{R}, @maechler2021]. Restricting our data set to talkers with at least `r min.observation.n` observations each per stop category allowed us to more reliably identify bimodal f0 distributions. Talkers for which the null of unimodality was rejected ($p<$ `r max.p`) for the raw f0 or Mel-transformed f0 of *any* stop category were excluded.^[We thank Eleanor Chodroff for help with this issue. Her inspection of some example tokens revealed that both creaky voice and pitch halving contributed to the bimodal pattern. Pitch halving refers to cases in which the f0 detection algorithm wrongly infers the f0 to be half of its true value. One reason for such estimation mistakes can be when the true f0 falls outside the range considered by the algorithm (here: 75-500 Hz, regardless of talker gender, Eleanor Chodroff, p.c.).]

This left `r nrow(d.chodroff_wilson)` observations from `r nlevels(d.chodroff_wilson$Talker)` different talkers and `r nlevels(d.chodroff_wilson$Word)` words. These are the data that we used to derive plausible estimate of the overall and category-specific VOT and Mel distributions that underlie the case studies presented in the main text. Figure \@ref(fig:chodroff-stop-VOT-f0ST) shows three randomly selected talkers from this data set prior to C-CuRE normalization. Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu) further illustrates the degree of cross-talker variability prior to normalization by plotting all talkers' category means relative to the category likelihood.


```{r chodroff-parameters}
n.subj <- 3
seed <- 59876
```

(ref:chodroff-stop-VOT-f0ST) Unormalized VOT and f0 of word-initial stop consonants in American English from `r n.subj` random talkers in the Mixer corpus (Chodroff & Wilson, 2017). Transparent points show individual tokens, which cover a large of different phonotactic, lexical, and utterance contexts. Solid points show talker-specific means over these tokens, connected by a gray line. Ellipses show the 95% probability mass for talker-specific bivariate Gaussian categories. 

```{r chodroff-stop-VOT-f0ST, fig.width=base.width * 3 + 1, fig.height = base.height * 3, fig.cap=c("(ref:chodroff-stop-VOT-f0ST)")}
talkers <- sample(unique(d.chodroff_wilson$Talker), n.subj, replace = F)

p.talkers <- d.chodroff_wilson %>% 
  # Keep random subject of subjects
  ungroup() %>%
  filter(Talker %in% talkers) %>%
  ggplot(aes(x = VOT, y = f0_Mel, color = voicing, shape = gender)) +
  geom_point(alpha = .1) +
  stat_ellipse() +
  scale_x_continuous(expression("VOT (ms)")) +
  scale_y_continuous(expression("f0 (Mel)")) +
  scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) +
  scale_shape_discrete("Gender") +
  scale_size_continuous("n", range = c(.1, 3)) +
  facet_grid(Talker ~ poa) 

p.talkers +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(group = Talker),
    alpha = .9, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    alpha = .9)
```

(ref:chodroff-stop-VOT-f0ST-mu) By-talker means of VOT and f0 for all six stop consonants and all `r nlevels(d.chodroff_wilson$Talker)` talkers in the Mixer corpus (Chodroff & Wilson, 2017) for which all phonetic cues were available. Ellipses show the 95% probability mass for talker-independent bivariate Gaussian categories if the data from all talkers are pooled independent of talker identity.

```{r chodroff-stop-VOT-f0ST-mu, fig.width=base.width * 3 + 1, fig.height = base.height, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-mu)")}
p.means <-
  d.chodroff_wilson %>% 
  ggplot(aes(x = VOT, y = f0_Mel, color = voicing)) +
  stat_ellipse() +
  scale_x_continuous(expression("VOT (ms)")) +
  scale_y_continuous(expression("f0 (Mel)")) +
  scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) +
  scale_shape_discrete("Gender") +
  scale_size_continuous("n", range = c(.1, 3)) +
  facet_grid(. ~ poa)

p.means +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(group = Talker),
    alpha = .1, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(shape = gender),
    alpha = .3)
```

## Applying C-Cure
We follow @mcmurray-jongman2011 and use linear regression to remove the effects of talker from each observation in the database. We did not include corrections for phonological context because, in this database, even the smallest relevant context (the vowel following the stop consonant) was confounded with the identity of the stop consonant. To avoid over-fitting, we use linear mixed effects regression rather than ordinary linear regression. Separate regressions were use to predict VOT and f0 (Mel). Both regressions used the formula:

\begin{equation}\label{eq:c-cure-regression}
\begin{split}
cue \sim 1 + (1 | Talker)
\end{split}
\end{equation}

Rather than to just use the residuals of these regressions, we only subtracted the random effects (BLUPs) from each observation. This removes the talker- and word-specific effects from each token but leaves observations in the original cue space (e.g. resulting in VOT values that mostly fall between 0 and 80, rather than being centered around 0). 

(ref:chodroff-stop-VOT-f0ST-normalized) Same as Figure \@ref(fig:chodroff-stop-VOT-f0ST) but for normalized VOT and f0. Normalization does not affect the relative placement of tokens within the talker's space but it does affect the absolute placement. This is also evident in Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu-normalized)

```{r chodroff-stop-VOT-f0ST-normalized, fig.width=base.width * 3 + 1, fig.height = base.height * 3, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-normalized)")}
p.talkers + 
  aes(x = VOT_centered, y = f0_Mel_centered) +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(group = Talker),
    alpha = .9, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    alpha = .9)
```

(ref:chodroff-stop-VOT-f0ST-mu-normalized) Same as Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu-normalized) but for normalized VOT and f0---both for the means and for the 95% ellipse.

```{r chodroff-stop-VOT-f0ST-mu-normalized, fig.width=base.width * 3 + 1, fig.height = base.height, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-mu-normalized)")}
p.means +
  aes(x = VOT_centered, y = f0_Mel_centered) +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(group = Talker),
    alpha = .1, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(shape = gender),
    alpha = .3)
```

# Graphical models of change models {#sec:SI-models}

## Changes in representations {#sec:SI-models-changes-in-representations}

Figure \ref{fig:model-belief-updating} visualizes the belief-updating process of the $\mathcal{NW^{-1}}$ model in factor graph notation.

\begin{figure}
  \centering
  \tikz{ %
    % exposure
    \node[obs] (cue) {$x_n$} ; %
    \factor[above=of cue] {cuedist} {left:$\mathcal{N}$} {} {}; %
    \node[obs, right=of cuedist] (category) {$c_n$} ; %
    % category parameters
    \node[det, above=of cuedist] (mu) {$\mu_c$} ; %
    \node[det, right=of mu] (Sigma) {$\Sigma_c$} ; %
    \factor[above=of mu] {mudist} {left:$\mathcal{N}$} {} {}; %
    \factor[above=of Sigma] {Sigmadist} {left:$\mathcal{W}^{-1}$} {} {}; %
    % hyperparameters
    \node[det, above=of mudist] (kappa) {$\kappa_{n,c}$} ; %
    \node[det, left=of kappa] (m) {$\mathrm{m}_{n,c}$} ; %
    \node[det, above=of Sigmadist] (S) {$\mathrm{S}_{n,c}$} ; %
    \node[det, right=of S] (nu) {$\nu_{n,c}$} ; %
    % prior
    \factor[above=1 of m] {update_m} {} {} {}; %
    \factor[above=1.05 of kappa] {update_kappa} {} {} {}; %
    \factor[above=.9 of S] {update_S} {} {} {}; %
    \factor[above=1 of nu] {update_nu} {right:update (Equation \ref{eq:niw-updating-parameters}) } {} {}; %
    \node[det, above=of update_m] (prior_m) {$\mathrm{m}_{0,c}$} ; %
    \node[latent, above=5 of update_kappa] (prior_kappa) {$\kappa_{0}$} ; %
    \node[latent, above=5 of update_nu] (prior_nu) {$\nu_{0}$} ; %
    \node[det, above=of update_S] (prior_S) {$\mathrm{S}_{0,c}$} ; %
    % external phonetic database
    \node[obs, above=of prior_m] (expected_mu) {$\mathbf{E}(\mu_{c})$} ; %
    \node[obs, above=of prior_S] (expected_Sigma) {$\mathbf{E}(\Sigma_{c})$} ; %
    % extra nodes to allow connection from cues to updating
    \node[const, left=1.2 of update_m] (update_extra1) {} ; %
    \node[const, left=3 of cue] (update_extra2) {} ; %
    \node[const, right=1 of update_nu] (update_extra3) {} ; %
    % plates
    \plate[inner sep=0.24cm, xshift=-0.06cm, yshift=0.12cm] {plate2} {(mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S) (prior_m) (prior_S) (expected_mu) (expected_Sigma) (update_extra3) } {$\forall c \in categories $}; %
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(cue) (category) (cuedist) (mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S)} {$\forall n \in exposure\ observation$}; %
    \edge {cuedist} {cue} ; %
    \edge {category} {cuedist} ; %
    \edge {mu, Sigma} {cuedist} ; %
    \edge {Sigma} {mudist} ; %
    \edge {kappa,m} {mudist} ; %
    \edge {nu,S} {Sigmadist} ; %
    \edge {mudist} {mu} ; %
    \edge {Sigmadist} {Sigma} ; %
    % updating
    \edge {prior_kappa} {update_kappa} ; %
    \edge {prior_nu} {update_nu} ; %
    \edge {prior_kappa, prior_m} {update_m} ; %
    \edge {prior_kappa, prior_m, prior_S} {update_S} ; %
    \edge {update_kappa} {kappa} ; %
    \edge {update_nu} {nu} ; %
    \edge {update_m} {m} ; %
    \edge {update_S} {S} ; %
    \edge[-] {cue} {update_extra2} ; %
    \edge[-] {update_extra1} {update_extra2} ; %
    \edge {update_extra1} {update_m} ; %
    \edge[-] {update_m} {update_kappa} ; %
    \edge[-] {update_kappa} {update_S} ; %
    \edge[-] {update_S} {update_nu} ; %
    % link to phonetic database
    \edge {expected_mu} {prior_m} ;
    \edge {expected_Sigma} {prior_S} ;
    \edge {prior_nu} {prior_S} ;
  }
  \caption{$\mathcal{NW}^{-1}$ Bayesian belief-updating model, as employed here. We set $\mathrm{m}_{0,c} = \mathbf{E}(\mu_c)$ and $\mathrm{S}_{0,c}=\mathbf{E}(\Sigma_c)$, where $\mathbf{E}(\mu_c)$ and $\mathbf{E}(\Sigma_c)$ are observable from phonetically annotated databases. We further assume that all categories share a common $\kappa_{0,c}$ and $\nu_{0,c}$, shown as $\kappa_{0}$ and $\nu_{0}$. These $\kappa_{0}$ and $\nu_{0}$ are the only two degrees of freedom in this highly simplified model of distributional learning (in the manuscript, we keep the $c$ subscript to avoid confusion with the parameter for changes in normalization in Equation \ref{eq:normalization-change}). All other variables are either observable (filled gray circles) or fully determined by other variables.}\label{fig:model-belief-updating}
\end{figure}

# Creating the stimuli for the (simulated) perceptual recalibration paradigm {#sec:SI-PR}

## Exposure
As described in the main text, the exposure phase of a typical perceptual recalibration experiment employs both typical stimuli and stimuli that are manipulated to be perceptually ambiguous between the two categories. In practice, researchers determine this point of perceptual ambiguity by first generating a continuum from the recording of the typical sound (e.g., _crocodile_) to a recording in which that sound is replaced with the opposite sound (_crocotile_). Procedures to create these continua range from simple blending of the two recordings---mixing the two recordings weighted by different amplitudes (from 100% _crocodile_ and 0% _crocotile_ to 0% "crocodile" and 100% _crocotile_)---to more careful phonetic manipulations (e.g., inserting addition silence to create longer VOTs) or the use of speech synthesis [for an insightful critique of the frequently used blending procedure, see @theodore-cummings2021]. The perceptually most ambiguous point along the resulting continua is then obtained either in a separate perception experiment, or simply by experimenter listening to the stimuli.

For the sake of generality, we simulate the general outcome of any of these procedures by imaging an experimenter (or subjects) listening to stimuli along a continuum ranging from typical `r categories.PR[1]` to typical `r categories.PR[2]`. We simulate the experimenter with the same perceptual model we assume for the listeners (see Section \@ref(sec:framework)). We assume that the experimenter, who can listen arbitrarily often to any of the stimuli, will make her final decision as to which stimuli select for the shifted tokens without attentional lapses ($\lambda=0)$. We further assume that experimenter's response bias are affected by the lexical context. Specifically, we set $\pi_{intended~category} = p(intended~category | lexical~context) =$ `r my_experimenter.lexical_bias_strength`. This captures that even the experimenter's perception is somewhat affected by the lexical context. Finally, we assume that the realizations of phonetic contrasts used in perceptual recalibration experiments are less variable than in naturally occurring speech, given that the stimuli for such experiments typically reflect *read* speech recorded in sound-attenuated booths, and experimenters would remove any 'atypical' recordings. Specifically, we arbitrarily assume that the variance of the typical (and shifted) stimuli in the experiment is $\frac{1}{`r my_experimenter.variability_reduction`}$th of that observed in natural productions. Figure \@ref(fig:PR-exposure-test-plot)A shows the phonetic properties of an instance of stimuli generated in this way.

## Test
For the test phase, we followed a similar procedure. We used the same continuum from a typical `r categories.PR[1]` to a typical `r categories.PR[2]`, and then selected six stimuli along that continuum that would be expected to yield `r paste(paste0(as.numeric(rev(unique(d.PR.test$Item.Intended_proportion_category1))) * 100, "%"), collapse = ", ")` `r categories.PR[1]`-responses in a norming experiment without any prior exposure to the talker's speech. Since the test tokens present the phonetic contrast in a non-biasing context (e.g., /`r linguisticsdown::cond_cmpl("ɪ")`\_`r linguisticsdown::cond_cmpl("ɪ")`/), we set the effect of lexical context to $p(category | lexical\ context) = .5$. The resulting test tokens shown in Figure \@ref(fig:PR-exposure-test-plot)B closely resemble the placement of test stimuli expected in experiments on perceptual recalibration [e.g., @norris2003; @kraljic-samuel2005; @kraljic-samuel2006]: as is typical for such experiments, test tokens are placed primarily where they are expected to be perceptually most ambiguous prior to exposure (i.e., close to the prior category boundary), with one additional test token towards each end of the continuum (typically somewhere between 10-25% and 75-90%, respectively). Our approach further assumes that the exposure and test stimuli are carefully chosen to have phonological contexts that allow generalization from exposure to test, as differences in the types of phonological context between exposure and test can reduce or even completely block perceptual recalibration [@eisner2013; @mitterer2013]. For example, the original study that we model our procedure on mostly used exposure stimuli in which /d/ or /t/ occurred word-medially at the onset of a syllable, between two vowels [e.g., _crocodile_, _academic_, etc., see Table 1 in @kraljic-samuel2006] or at least between two sonorants (e.g., _legendary_, _secondary_). In the test stimuli, the /d/-/t/ contrast occurred in a similar position (either /a_a/ or /`r linguisticsdown::cond_cmpl("ɪ")`\_`r linguisticsdown::cond_cmpl("ɪ")`/). 


<!-- ## Session Info -->

<!-- ```{r session_info, echo=FALSE, results='markup'} -->
<!-- devtools::session_info() -->
<!-- ``` -->

