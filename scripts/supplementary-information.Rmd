\newpage
\setcounter{page}{1}
\renewcommand{\thesection}{\S \arabic{section}}
\renewcommand{\theHsection}{sisection.\S \arabic{section}}


# Supplementary information for *Xie, Jaeger, & Kurumada (2022). What we do (not) know about the mechanisms underlying adaptive speech perception* {-}
\setcounter{section}{0}

Both the main text and these supplementary information (SI) are derived from the same R markdown document available via OSF at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). It is best viewed using Acrobat Reader. Some links and animations might not work in other viewers. 

# Required software {#sec:SI-software}
The document was compiled using \texttt{knitr} [@xie2021] in RStudio with R:

```{r} 
version
```

You will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or the R library \texttt{tinytex}). The 3D figures require the [\texttt{orca} commandline tool](https://github.com/plotly/orca#installation). It is recommended that you use "Method 4" to install the standalone binaries.

We used the following R packages to create this document: `r cite_r("latex-stuff/r-references.bib", withhold = T, pkgs = c("MVBeliefUpdatr"))`. If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. This includes the \texttt{MVBeliefUpdatr} library that supports working with multivariate Gaussian ideal observers and adaptors (see also Dave Kleinschmidt's \texttt{beliefupdatr} library that this library is based on). The full session information is provided at the end of this document.

# A database of natural productions of word-initial stop voicing in American English [@chodroff-wilson2018] {#sec:SI-chodroff}

All case studies presented in the main text are based on a phonetically annotated database of word-initial stop voicing in American English [@chodroff-wilson2018]. Chodroff and Wilson (2018, p. 3) describe the database:

> The data was extracted from an audited subset of the Mixer 6 corpus (Brandschain et al. 2010, Brandschain
et al. 2013; Chodroff et al. 2016) containing approximately 45 minutes of read speech from 180 native AE
speakers (102 female). Transcripts were aligned to the corresponding WAV files with the Penn Forced Aligner
(Yuan and Liberman 2008), and all word-initial prevocalic stop consonants were further processed with
AutoVOT (Keshet et al. 2014). AutoVOT automatically identifies the stop release and following vowel onset
within a user-specified window of analysis. Further details about the talkers, read sentences, and boundary
alignments can be found in Chodroff and Wilson (2017).³

> COG, positive VOT, and onset f0 in the following vowel were measured for each stop. COG was calculated
from a smoothed spectrum over the initial portion of the release burst. Each spectrum was computed by averaging
FFTs from seven consecutive 3 ms windows, with the first window centered on the burst transient and
a window shift of 1 ms (Hanson and Stevens 2003; Flemming 2007; Chodroff and Wilson 2014). Positive VOT
was defined as the duration from stop release to the onset of periodicity in the vowel; this was automatically
extracted from the AutoVOT boundaries or from manually-corrected boundaries when available. The
f0 value was the first one measured by Praat (Boersma and Weenink 2016) within 50 ms after the following
vowel onset.

<!-- 
# From email communication with Eleanor Chodroff (10/14/2021):
# 
# filename + trial uniquely identify an observation. Trial = the TextGrid interval number for a 
# particular session recording, which each have a unique filename (that includes the subject ID, 
# the date, and some other info). Adding "subj" won't hurt. I wouldn't rely on "word" just in 
# case they happened to produce multiple instances of the same word within the recording. 
#
# vot: 
#   VOT in ms. VOT is reported in the LingVan 2018 paper.
#
# cog, spectral.var, skew, kurtosis: 
#   these are the summary statistics of the power spectral density measures in each of the 33 bins, 
#   where each bin is 250 Hz wide. COG = Center of gravity (Hz), spectral.var = spectral variance 
#   (take the square root to get the SD, a more palatable number and one that will be in Hz), 
#   skew = spectral skewness, kurtosis = spectral kurtosis. COG is reported in the LingVan 2018 paper
#   See Forrest et al. 1988 for a good description of these measures.
#
# usef0:
#   f0 measures were taken every 5 ms after the start of the vowel up to 50 ms into the vowel 
#   (see f0_1 to f0_10 columns). usef0 is the first defined instance of f0. usef0 is reported in the 
#   LingVan 2018 paper.
-->

Following advice from Eleanor Chodroff, we removed tokens with f0 measurements of above `r max.f0` Hz, as those were implausible and likely reflected a measurement error (pitch doubling). We further subset the data talkers for which all three cues (VOT, COG, and f0) were available for at least `r min.observation.n` observations each per stop category. This was done because we detected that the f0 of a good number of talkers exhibited evidence of bimodality. To remove talkers with bimodal f0 measures, we applied a test of multimodality [the dip test, as implemented in the library \texttt{diptest} in \texttt{R}, @maechler2021]. Restricting our data set to talkers with at least `r min.observation.n` observations each per stop category allowed us to more reliably identify bimodal f0 distributions. Talkers for which the null of unimodality was rejected ($p<$ `r max.p`) for the raw f0 or Mel-transformed f0 of *any* stop category were excluded.^[We thank Eleanor Chodroff for help with this issue. Her inspection of some example tokens revealed that both creaky voice and pitch halving contributed to the bimodal pattern. Pitch halving refers to cases in which the f0 detection algorithm wrongly infers the f0 to be half of its true value. One reason for such estimation mistakes can be when the true f0 falls outside the range considered by the algorithm (here: 75-500 Hz, regardless of talker gender, Eleanor Chodroff, p.c.).]

This left `r nrow(d.chodroff_wilson)` observations from `r nlevels(d.chodroff_wilson$Talker)` different talkers and `r nlevels(d.chodroff_wilson$Word)` words. These are the data that we used to derive plausible estimate of the overall and category-specific VOT and Mel distributions that underlie the case studies presented in the main text. Figure \@ref(fig:chodroff-stop-VOT-f0ST) shows three randomly selected talkers from this data set prior to C-CuRE normalization. Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu) further illustrates the degree of cross-talker variability prior to normalization by plotting all talkers' category means relative to the category likelihood.


```{r chodroff-parameters}
n.subj <- 3
seed <- 59876
```

(ref:chodroff-stop-VOT-f0ST) Unormalized VOT and f0 of word-initial stop consonants in American English from `r n.subj` random talkers in the Mixer corpus (Chodroff & Wilson, 2017). Transparent points show individual tokens, which cover a large of different phonotactic, lexical, and utterance contexts. Solid points show talker-specific means over these tokens, connected by a gray line. Ellipses show the 95% probability mass for talker-specific bivariate Gaussian categories. 

```{r chodroff-stop-VOT-f0ST, fig.width=base.width * 3 + 1, fig.height = base.height * 3, fig.cap=c("(ref:chodroff-stop-VOT-f0ST)")}
talkers <- sample(unique(d.chodroff_wilson$Talker), n.subj, replace = F)

p.talkers <- d.chodroff_wilson %>% 
  # Keep random subject of subjects
  ungroup() %>%
  filter(Talker %in% talkers) %>%
  ggplot(aes(x = VOT, y = f0_Mel, color = voicing, shape = gender)) +
  geom_point(alpha = .1) +
  stat_ellipse() +
  scale_x_continuous(expression("VOT (ms)")) +
  scale_y_continuous(expression("f0 (Mel)")) +
  scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) +
  scale_shape_discrete("Gender") +
  scale_size_continuous("n", range = c(.1, 3)) +
  facet_grid(Talker ~ poa) 

p.talkers +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(group = Talker),
    alpha = .9, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    alpha = .9)
```

(ref:chodroff-stop-VOT-f0ST-mu) By-talker means of VOT and f0 for all six stop consonants and all `r nlevels(d.chodroff_wilson$Talker)` talkers in the Mixer corpus (Chodroff & Wilson, 2017) for which all phonetic cues were available. Ellipses show the 95% probability mass for talker-independent bivariate Gaussian categories if the data from all talkers are pooled independent of talker identity.

```{r chodroff-stop-VOT-f0ST-mu, fig.width=base.width * 3 + 1, fig.height = base.height, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-mu)")}
p.means <-
  d.chodroff_wilson %>% 
  ggplot(aes(x = VOT, y = f0_Mel, color = voicing)) +
  stat_ellipse() +
  scale_x_continuous(expression("VOT (ms)")) +
  scale_y_continuous(expression("f0 (Mel)")) +
  scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) +
  scale_shape_discrete("Gender") +
  scale_size_continuous("n", range = c(.1, 3)) +
  facet_grid(. ~ poa)

p.means +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(group = Talker),
    alpha = .1, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(shape = gender),
    alpha = .3)
```

## Applying C-CuRE
We follow @mcmurray-jongman2011 and use linear regression to remove the effects of talker from each observation in the database. In extending normalization accounts to our present goals, we encountered three decision points that we have not previously seen discussed:

  1. C-CuRE removes the *overall* cue mean from each observation's cue value. Based on this, it would seem most appropriate to include all six stop categories in the estimation of the cue means. However, the experiments we model in the main text involve only two of the categories (/d/ and /t/). We thus decided to only include tokens of /d/ and /t/ in the estimation of cue means. This decision was made because C-CuRE is intended to correct observed cue values for listeners' expectations *for the current context*. Put differently, we assume that listeners expect that the sound will be a /d/ or a /t/ since these are the only two response options provided to participants. This assumption affects some of our results (but not as much as it would if we studied /b/-/p/ or /g/-/k/ since the mean cue values for /d/s and /t/s are closer to the mean cue values across all six categories). 
  
  2. C-CuRE is meant to correct for effects of both talkers and *phonological* contexts. However, in the @chodroff-wilson2018 data /d/ and /t/ occur across different phonological contexts, so that inclusion of phonological contexts as a predictor in the regression model would indirectly capture information about category identity. We thus limited the data further to 9 pairs of /d/- and /t/-items that had identical vowels following the stop consonant (e.g, _*d*ied_ and _*t*ime_). While this does not completely remove the effects of phonological context, it both reduces the variability in cue values associated with phonological contexts (thus providing a more accurate estimate of the expected outcome of normalization) and *balances* the effect of phonological context (deconfounding it from category identity). 
  
  3. Finally, the original data from @chodroff-wilson2018 contained nearly three times as many /d/s as /t/s, and this asymmetry was retained after the data cleaning procedures described above. While this particular imbalance is probably not reflective of natural speech, natural speech will often exhibit *some* form of asymmetry in the relative frequencies of categories. This raises the question of whether listeners somehow correct for these asymmetries when normalizing the speech input for an experiment for which there all categories can be expected to be equally frequent. Following similar considerations as in Point 1 above, we assume this to be the case. We thus randomly subsampled equal numbers of tokens for each /d/ and /t/ from all talkers. This number was determined by the number of tokens of the less frequent category for each talker. For instance, if a talker produced 40 /d/s and 20 /t/s, then 20 /d/ and 20 /t/ observations were randomly pulled from the data. 

For experimenters, the issues described here will arise whenever the distribution of categories and/or phonological contexts differs between exposure and test: the estimated mean will then reflect expectations that do not veridically reflect the statistics of the current input. In previous evaluations of C-CuRE this was never the case: both the distribution of categories and the distribution of phonological contexts during test were identical to the distributions that the C-CuRE model was trained on [@mcmurray-jongman2011]. For the present study, we investigate how listeners might transfer and adapt expectations based on previous long-term exposure to novel input from an unfamiliar talker. We therefore used subsamples that were well-balanced between /d/ and /t/ tokens in terms of their frequency and phonological contexts---a pattern matched to those employed during the exposure and test in typical experiments. We note, however, that none of our core results seem to depend on the assumptions we make here. We obtained qualitatively identical results for all critical comparisons in previous simulations that did not subsample the data to be balanced with regard to /d/ and /t/ or their phonological contexts.

The three issues raises above also raise questions about C-CuRE that go beyond practical concerns for experimenters. For any of the three decisions we described above, it is an empirical question whether listeners do something similar. Put differently, there are complexities in applying approaches like C-CuRE to scenarios that are likely to occur in everyday speech perception that need to be investigated in future research.

<!-- TO-DO commented out for now but should go into SI or GD?: It is important to keep in mind that while the updates in normalization in this case study seem computationally simple---requiring only the computation of cue mean of exposure tokens, the computational processes required for the normalization of talker differences in real-life situations can be more complex than first meets the eye. As simulated here, empirical tests of normalization accounts often employ balanced samples across categories within a contrast. However, in real encounters with an unfamiliar talker, human listeners may be dealing with unbalanced data or data in which the token frequency of categories does not follow the pattern of regularities in one’s long-term input. Both will create artificial shifts in normalized cue space that should not be attributed to cross-talker differences. For instance, in the data employed here, the mean VOTs for /d/ and /t/ are 16ms and 72ms respectively, yielding a cue-level mean of 39ms. Consider two scenarios. In one case, a listener is exposed to a talker who produces equal number of shifted /d/ productions with a mean of 34ms and typical /t/s. In another case, the listener hears a talker who produces twice as many as /t/s than /d/s. In both cases, the cue mean of the newly experienced talker is 53ms. If listeners do not explicitly encode labels for the exposure tokens, then the same boundary shift is predicted in both cases. Yet only the shift in the first case is desirable. In other words, in order to effectively compensate for talker differences, the expectations about the mean of previous input would need to take into account both the previous input itself and its labels (e.g., whether it should be a /d/ or /t/). Listener therefore need to either infer and store category labels along with specific instances or keep track of the frequency by which each category occurs, in addition to inferring the cue mean. Regardless, the computational demand of normalization-based changes is higher than recognized in previous discussions of normalization accounts.  -->


Decisions 1-3 left a total of 5632 observations (2816 each for /d/ and /t/). To avoid over-fitting to individual talkers, we use linear mixed-effects regression rather than ordinary linear regression. This 'shrinks' talker-specific estimates of the cue means towards the overall (population-level) mean, and does so more when less data is available for the particular talker. Separate regressions were use to predict VOT and f0 (Mel). Both regressions used the formula:

\begin{equation}\label{eq:c-cure-regression}
\begin{split}
cue \sim 1 + (1 | Talker)
\end{split}
\end{equation}

Rather than to just use the residuals of these regressions, we only subtracted the random effects (BLUPs) from each observation. This removes the talker-specific effects from each token (as intended by C-CuRE) but leaves observations in the original cue space (rather than residual VOTs centered around 0), thus achieving talker normalization without loss in interpretability. The R code for the steps described here is found in R markdown document for the main text, at the end of Section \@ref(sec:framework).

(ref:chodroff-stop-VOT-f0ST-normalized) Same as Figure \@ref(fig:chodroff-stop-VOT-f0ST) but for normalized VOT and f0. Normalization does not affect the relative placement of tokens within the talker's space but it does affect the absolute placement. This is also evident in Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu-normalized)

```{r chodroff-stop-VOT-f0ST-normalized, fig.width=base.width * 3 + 1, fig.height = base.height * 3, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-normalized)")}
p.talkers + 
  aes(x = VOT_centered, y = f0_Mel_centered) +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(group = Talker),
    alpha = .9, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    alpha = .9)
```

(ref:chodroff-stop-VOT-f0ST-mu-normalized) Same as Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu-normalized) but for normalized VOT and f0---both for the means and for the 95% ellipse.

```{r chodroff-stop-VOT-f0ST-mu-normalized, fig.width=base.width * 3 + 1, fig.height = base.height, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-mu-normalized)")}
p.means +
  aes(x = VOT_centered, y = f0_Mel_centered) +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(group = Talker),
    alpha = .1, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(shape = gender),
    alpha = .3)
```

# Additional details about the change models {#sec:SI-models}
This section contains additional details about the change models.

## Changes in category representations {#sec:SI-models-changes-in-representations}
The updating of the four $\mathcal{NW^{-1}}$ parameters after $N$ observations of a category $c$ from the talker is described by the equations in \@ref(eq:niw-updating-parameters), and is deterministic [for details and derivation, see @murphy2012, p. 134]. $\kappa_c$ and $\nu_c$ simply increase by 1 with each observation, capturing the fact that each observation adds additional information about the talker's category mean and covariances. $\mathrm{m}_{N,c}$ is a weighted combination of its prior value $\mathrm{m}_{0,c}$ and the category mean of the $N$ observations $\bar{x}$---following the same logic that we applied to the inference of the overall cue mean in Equation \@ref(eq:normalization-change). Similarly, $\mathrm{S}_{N,c}$ is a weighted combination of its prior value $\mathrm{S}_{0,c}$ and the category variability of the $N$ observations,^[$\mathrm{S}_c \triangleq \Sigma_{i=1}^N x_{i,c} x_{i,c}^T$ is the uncentered sum of squares matrix of the $N$ observations [@murphy2012].] plus an additional term that captures the uncertainty about the category mean. Figure \ref{fig:model-belief-updating} visualizes the belief-updating process in Equations \@ref(eq:niw-updating)-\@ref(eq:niw-updating-parameters) as a graphical model.

\begin{equation}\label{eq:niw-updating-parameters}
\begin{split}
\mathrm{m}_{N,c} & = \frac{\kappa_{0,c} \mathrm{m}_{0,c} + N_c \bar{x}}{\kappa_{N,c}} = \frac{\kappa_{0,c}}{\kappa_{0,c} + N_c} \mathrm{m}_{0,c} + \frac{N_c}{\kappa_{0,c} + N_c}\bar{x}_c \\
\kappa_{N,c} & = \kappa_{0,c} + N_c \\
\nu_{N,c} & = \nu_{0,c} + N_c \\
\mathrm{S}_{N,c} & = \mathrm{S}_{0,c} + \mathrm{S}_{\bar{x}_c} + \frac{\kappa_{0,c} N_c}{\kappa_{0,c} + N_c}\left( \bar{x}_c-\mathrm{m}_{0,c} \right) \left( \bar{x}_c-\mathrm{m}_{0,c} \right)^T \\
 & = \mathrm{S}_{0,c} + \mathrm{S}_c + \kappa_{0,c} \mathrm{m}_{0,c} \mathrm{m}_{0,c}^T - \kappa_{N,c} \mathrm{m}_{N,c} \mathrm{m}_{N,c}^T
\end{split}
\end{equation}

\begin{figure}
  \centering
  \tikz{ %
    % exposure
    \node[obs] (cue) {$x_n$} ; %
    \factor[above=of cue] {cuedist} {left:$\mathcal{N}$} {} {}; %
    \node[obs, right=of cuedist] (category) {$c_n$} ; %
    % category parameters
    \node[det, above=of cuedist] (mu) {$\mu_c$} ; %
    \node[det, right=of mu] (Sigma) {$\Sigma_c$} ; %
    \factor[above=of mu] {mudist} {left:$\mathcal{N}$} {} {}; %
    \factor[above=of Sigma] {Sigmadist} {left:$\mathcal{W}^{-1}$} {} {}; %
    % hyperparameters
    \node[det, above=of mudist] (kappa) {$\kappa_{n,c}$} ; %
    \node[det, left=of kappa] (m) {$\mathrm{m}_{n,c}$} ; %
    \node[det, above=of Sigmadist] (S) {$\mathrm{S}_{n,c}$} ; %
    \node[det, right=of S] (nu) {$\nu_{n,c}$} ; %
    % prior
    \factor[above=1 of m] {update_m} {} {} {}; %
    \factor[above=1.05 of kappa] {update_kappa} {} {} {}; %
    \factor[above=.9 of S] {update_S} {} {} {}; %
    \factor[above=1 of nu] {update_nu} {right:update (Equation \ref{eq:niw-updating-parameters}) } {} {}; %
    \node[det, above=of update_m] (prior_m) {$\mathrm{m}_{0,c}$} ; %
    \node[latent, above=5 of update_kappa] (prior_kappa) {$\kappa_{0}$} ; %
    \node[latent, above=5 of update_nu] (prior_nu) {$\nu_{0}$} ; %
    \node[det, above=of update_S] (prior_S) {$\mathrm{S}_{0,c}$} ; %
    % external phonetic database
    \node[obs, above=of prior_m] (expected_mu) {$\mathbf{E}(\mu_{c})$} ; %
    \node[obs, above=of prior_S] (expected_Sigma) {$\mathbf{E}(\Sigma_{c})$} ; %
    % extra nodes to allow connection from cues to updating
    \node[const, left=1.2 of update_m] (update_extra1) {} ; %
    \node[const, left=3 of cue] (update_extra2) {} ; %
    \node[const, right=1 of update_nu] (update_extra3) {} ; %
    % plates
    \plate[inner sep=0.24cm, xshift=-0.06cm, yshift=0.12cm] {plate2} {(mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S) (prior_m) (prior_S) (expected_mu) (expected_Sigma) (update_extra3) } {$\forall c \in categories $}; %
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(cue) (category) (cuedist) (mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S)} {$\forall n \in exposure\ observation$}; %
    \edge {cuedist} {cue} ; %
    \edge {category} {cuedist} ; %
    \edge {mu, Sigma} {cuedist} ; %
    \edge {Sigma} {mudist} ; %
    \edge {kappa,m} {mudist} ; %
    \edge {nu,S} {Sigmadist} ; %
    \edge {mudist} {mu} ; %
    \edge {Sigmadist} {Sigma} ; %
    % updating
    \edge {prior_kappa} {update_kappa} ; %
    \edge {prior_nu} {update_nu} ; %
    \edge {prior_kappa, prior_m} {update_m} ; %
    \edge {prior_kappa, prior_m, prior_S} {update_S} ; %
    \edge {update_kappa} {kappa} ; %
    \edge {update_nu} {nu} ; %
    \edge {update_m} {m} ; %
    \edge {update_S} {S} ; %
    \edge[-] {cue} {update_extra2} ; %
    \edge[-] {update_extra1} {update_extra2} ; %
    \edge {update_extra1} {update_m} ; %
    \edge[-] {update_m} {update_kappa} ; %
    \edge[-] {update_kappa} {update_S} ; %
    \edge[-] {update_S} {update_nu} ; %
    % link to phonetic database
    \edge {expected_mu} {prior_m} ;
    \edge {expected_Sigma} {prior_S} ;
    \edge {prior_nu} {prior_S} ;
  }
  \caption{$\mathcal{NW}^{-1}$ Bayesian belief-updating model, as employed here. We set $\mathrm{m}_{0,c} = \mathbf{E}(\mu_c)$ and $\mathrm{S}_{0,c}=\mathbf{E}(\Sigma_c)$, where $\mathbf{E}(\mu_c)$ and $\mathbf{E}(\Sigma_c)$ are observable from phonetically annotated databases. We further assume that all categories share a common $\kappa_{0,c}$ and $\nu_{0,c}$, shown as $\kappa_{0}$ and $\nu_{0}$. These $\kappa_{0}$ and $\nu_{0}$ are the only two degrees of freedom in this highly simplified model of distributional learning (in the manuscript, we keep the $c$ subscript to avoid confusion with the parameter for changes in normalization in Equation \ref{eq:normalization-change}). All other variables are either observable (filled gray circles) or fully determined by other variables.}\label{fig:model-belief-updating}
\end{figure}

The $\kappa_{c}$s and $\nu_{c}$s are also sometimes called "pseudocounts" because they have a rather intuitive interpretation: the value of these parameters can be seen as describing the number of observations of this category that the listener assumes to have observed from the talker. For example, a listener with $\kappa_{c,0} = 100$ updates her beliefs about an unfamiliar talker's category mean as if she has already seen 100 observations of that category from the talker *prior to having received any input from that talker*. After 900 observations of that category from the unfamiliar talker, this listener's belief about the talker's category mean would be a weighted mixture, made up to 10% by the prior $\mathrm{m}_{c,0}$ and to 90% of the mean of the 900 observed category instances $\bar{x}_c$.^[The extent to which listeners transfer prior expectations to an unfamiliar talker or context (i.e., the values of $\kappa_{c,0}$ and $\nu_{c,0}$) is expected to differ across phonological contrasts [see discussion in @kleinschmidt-jaeger2015].]

### Setting the $m_{0,c}$ and $S_{0,c}$ parameters
It is possible to treat $m_{0,c}$ and $S_{0,c}$ as free parameters of the Normal-Inverse-Wishart belief-updating model, and to infer them from participants' responses in perceptual experiments [@kleinschmidt-jaeger2016cogsci]. This makes it possible to compare whether estimates of these parameters that are purely based on *listeners'* behavior correspond to distributions of category means $\mu_c$ and category covariances $\Sigma_c$ that match those observed in the input that listeners have received throughout their lives---i.e., the joint distribution of $\mu_c$ and $\Sigma_c$ observed in phonetically annotated databases that are are representative of the input listeners have previously received. If such a match is indeed observed, this would lend support to the hypothesis that listeners learn and store knowledge about the statistics of cue-to-category mappings in the input [for initial tests of this type, and further discussion, see @kleinschmidt-jaeger2016cogsci; @tan2022].

However, it is also possible to instead set $\mathrm{m}_{0,c}$ and $\mathrm{S}_{0,c}$ based on phonetically annotated data. This fixes all $J$ DFs for $\mathrm{m}_{0,c}$ and $\frac{J}{2}(K^2+K)$ DFs for $\mathrm{S}_{0,c}$ prior to predicting listeners' behavior but *assumes* (rather than tests) that listeners learn and store knowledge about the statistics of cue-to-category mappings in the input. This latter approach is the one that we took in our case studies. The core idea is to select $\mathrm{m}_{0,c}$ and $\mathrm{S}_{0,c}$ so that they yield a prior joint distribution of $\mu_c$ and $\Sigma_c$ that match those observed in a sufficiently large phonetic database that can reasonably assumed to approximate the type of input an average participant has received throughout their life. 

For the present purpose, we simplify this estimation problem further by setting $\mathrm{m}_{0,c}$ and $\mathrm{S}_{0,c}$ so that the expected values of the *marginal* distributions of $\mu_c$ and $\Sigma_c$ match the maximum likelihood estimates of the category mean and covariance matrix, respectively, in the phonetic database [@chodroff-wilson2018]. This means that we set $\mathrm{m_{0,c}}$ to the empirical mean (i.e., maximum likelihood estimate) of the category's cue distribution, $\bar{x}_c$, estimated from the phonetic database. Since the Normal-Inverse-Wishart belief-updating model describes the prior distribution of the category mean $\mu_c$ as a Normal distribution with mean $\mathrm{m}_{0,c}$ and covariance matrix $\frac{1}{\kappa_{0,c}}\Sigma_c$, this yields an expected prior category mean $\mathbf{E}(\mu_c) = \mathrm{m_{0,c}} = \bar{x}_c$.

The prior distribution of the category covariance matrix $\Sigma_c$ is described by the Inverse-Wishart distribution with scale parameter $\mathrm{S_{0,c}}$ and degree of freedom $\nu_{0,c}$. For our case studies, we thus again set $\mathrm{S_{0,c}}$, such that the expected prior category covariance matrix matches the empirical covariance matrix $\frac{\bar{S}_c}{N}$ (i.e., the empirical sum of squares matrix divided by the number of observations) estimated from the phonetic database *given $\nu_{0,c}$*. This yields $\mathrm{S_{0,c}} = \mathbf{E}(\Sigma_c)(\nu_{0,c}-D-1)$ = $\bar{S}_c(\nu_{0,c}-D-1)$, where $D$ is the dimensionality of input [i.e., the number of phonetic cues considered, cf. @murphy2012, p. 134-5].

We note that the approach employed in our case studies does not take into account that category means and covariance matrices can be correlated. In that case, the expected values of the marginal distributions of $\mu_c$ and $\Sigma_c$ will not be identical to the expected value of the joint distribution of $\mu_c$ and $\Sigma_c$.^[This might also reveal that some of the assumptions of the Normal-Inverse-Wishart belief-updating model are problematic, either for researchers or for listeners.] For future work, it would thus be more appropriate to estimate the *joint* distribution of $\mu_c$ and $\Sigma_c$ from phonetic data, and to find the values for $\mathrm{m}_{0,c}$ and $\mathrm{S}_{0,c}$ that best approximate this distribution (e.g., through moment-matching). For the present purpose, however, this change is not expected to qualitatively affect the conclusions reached in our case studies.


## Changes in decision-making {#sec:SI-models-changes-in-decision-making}
Here, we model the prediction error as the surprisal experienced when observing the category label, $I(c_{observed})$. This surprisal is a log-inverse function of the posterior probability of the category label given the beliefs held by the listener prior to observing the category label. 

\begin{equation}\label{eq:bias-updating}
\begin{split}
\mathrm{logit}(\pi_{n,c_{observed}}) & = \mathrm{logit}(\pi_{n-1,c}) + \beta_{\pi} I(c_{observed}) \\
                          & = \mathrm{logit}(\pi_{n-1,c}) - \beta_{\pi} \log_2 p_{n-1}(c_{observed}) \\
\end{split}
\end{equation}

The same amount that is added to the bias of the observed (labeled) category is subtracted from the response biases for all other categories (uniformly distributed across those categories). <!-- For the simple case of a 2AFC task, which involves only two categories, changes in the bias after $n$ observations are thus described by a logistic regression with intercept $\mathrm{logit}(\pi_{n-1,c})$ and surprisal slope $\beta_{\pi}$:

\begin{equation}\label{eq:bias-probability}
\begin{split}
\pi_{n,c} = \frac{1}{1+ e^{\mathrm{logit}(\pi_{n-1,c}) + \beta_{\pi} I(c_{observed}) }}
\end{split}
\end{equation}

-->
Like the representational change model, the change model for response biases is sensitive to the mismatch between listeners' expectations based on the input and labeling information provided by the context. Unlike the representational change model, however, the change model for response biases uses the new observations to update response biases rather than beliefs about category likelihoods. As such, the change for response biases does not *directly* change the mapping from stimulus properties to categorization responses. The change model for response biases can, however, affect this mapping indirectly. This means that even simple changes in response biases can change listeners' categorization function in complex ways.

We also briefly considered another simple change model for decision-making but never implemented it since it could not possibly explain the findings we present in Sections \@ref(sec:PR) and \@ref(sec:AA). This alternative model holds that listeners aim to infer the relative probability of each category based on recent input, and that response biases reflect these estimates. For example, listeners might enter an experiment with expectations about the relative probability of each category based on their relative frequency in previously experienced input, and then update their expectations based on the relative frequency of the categories within the experiment [similar to belief-updating models of syntactic adaptation, @fine-jaeger2013; @jaeger2019; @prasad2021]. However, such a model could not possibly explain exposure effects in, for example, a typical perceptual recalibration experiment since the relative frequency of the two categories does not differ between exposure conditions.

### (Non)-additivity of changes as a function of $\lambda$
In Figures \@ref(fig:demonstrate-lapse-bias-change) and \@ref(fig:demonstrate-lapse-bias-change-nonzero-lapse) in the main text, we demonstrate that the (non)-additivity of changes in decision-making depends on $\lambda$. Specifically, changes are additive in the log-odds of the posterior probability of categories if and only if $\lambda = 0$. To see how this limitation arises, consider how the *log-odds* of a category---e.g., /d/---in Equation \@ref(eq:posterior-probability-lapse) depend on the response biases $\pi_c$ when $\lambda=0$: 

\begin{equation}
\begin{split}
p(/d/ | input) & = \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/}}{\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} \Rightarrow \\
\log \frac{p(/d/ | input)}{1 - p(/d/ | input)} & = \log \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/}}{\mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right) \pi_{/t/}} \\
 & = \log \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right)}{\mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right)} + \log\frac{\pi_{/d/}}{\pi_{/t/}} \\
\end{split}
\end{equation}

In other words, adding some arbitrary amount to $\pi_{/d/}$ and subtracting same amount from $\pi_{/t/}$ (since $\Sigma_i \pi_{c_i} = 1$) will lift/lower the entire categorization function by a constant amount in log-odds, independent of the stimulus.

However, if $\lambda \neq 0$, then:

\begin{equation}
\begin{split}
\log \frac{p(/d/ | input)}{p(/t/ | input)} = \log \frac{(1-\lambda)\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) + \lambda \left(\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i} \right)\pi_{c_i}\right)}{(1-\lambda)\mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right) + \lambda \left(\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i} \right)\pi_{c_i}\right)} + \log\frac{\pi_{/d/}}{\pi_{/t/}}
\end{split}
\end{equation}

# Creating the stimuli for the (simulated) perceptual recalibration paradigm {#sec:SI-PR}
As also mentioned in the main text, many studies on perceptual recalibration do not report the acoustic properties of the exposure and test stimuli,^[Some studies provide aggregate information [e.g., @kraljic-samuel2006; @kraljic-samuel2007] and very few provide detailed information and visualization [e.g., @drouin2016].] and analyses that relate the acoustic properties of exposure and test stimuli to participants' responses during test remain the exception. In our experience, it is also not uncommon that original recordings are no longer available or only partially available (see one of our recommendations in the general discussions). Phonetic annotations that aid the extraction of acoustic information about these recordings are available for only a very small number of studies.^[Some notable exceptions for research on American English include the labs of Drs. Babel (UBC), Myer (UConn), and Theodore (UConn). In an ongoing project at Rochester, we have collected a database of phonetically annotated perceptual recalibration experiments on American English /s/-/`r linguisticsdown::cond_cmpl("ʃ")`/ that links stimulus recordings, phonetic annotations, and over 20 phonetic extracted phonetic cues to categorization responses from several thousand participants across a dozen experiments from multiple labs. Researchers interested in the database can contact any of the authors.] For that reason, we use simulated data in our case study on perceptual recalibration. Our stimulus generation procedure aims to capture the qualitative properties of the most common approaches to stimulus selection in perceptual recalibration experiments. 

## Exposure
As described in the main text, the exposure phase of a typical perceptual recalibration experiment employs both typical stimuli and stimuli that are manipulated to be perceptually ambiguous between the two categories. In practice, researchers determine this point of perceptual ambiguity by first generating a continuum from the recording of the typical sound (e.g., _crocodile_) to a recording in which that sound is replaced with the opposite sound (_crocotile_). Procedures to create these continua range from simple blending of the two recordings---mixing the two recordings weighted by different amplitudes (from 100% _crocodile_ and 0% _crocotile_ to 0% "crocodile" and 100% _crocotile_)---to more careful phonetic manipulations (e.g., inserting addition silence to create longer VOTs) or the use of speech synthesis [for an insightful critique of the frequently used blending procedure, see @theodore-cummings2021]. The former is the by far more common approach. The perceptually most ambiguous point along the resulting continua is then determined either by the experimenter(s) or in a separate norming experiment, with the former approach being far more common. In short, perceptual recalibration experiments do neither carefully select the tokens within each category based on phonetic properties, nor is there any form of counter-balancing of the average phonetic or perceptual shifts across the two bias conditions. 

For the sake of generality, we simulate the general outcome of any of these procedures by imaging an experimenter (or participants) listening to stimuli along a continuum ranging from typical `r categories.PR[1]` to typical `r categories.PR[2]`. We simulate the experimenter with the same perceptual model we assume for general L1-listeners (see Section \@ref(sec:framework)). We assume that the experimenter, who can listen arbitrarily often to any of the stimuli, will make her final decision as to which stimuli should be selected for the shifted tokens without attentional lapses ($\lambda=0)$. We further assume that experimenter's response bias is affected by the lexical context. Specifically, we set $\pi_{intended~category} =$ $p(intended~category | lexical~context) =$ `r my_experimenter.lexical_bias_strength`. This captures the fact that even the experimenter's perception is somewhat affected by the lexical context. <!-- Finally, we assume that the realizations of phonetic contrasts used in perceptual recalibration experiments are less variable than in naturally occurring speech, given that the stimuli for such experiments typically reflect *read* speech recorded in sound-attenuated booths, and experimenters would remove any 'atypical' recordings. Specifically, we arbitrarily assume that the variance of the typical (and shifted) stimuli in the experiment is $\frac{1}{`r my_experimenter.variability_reduction`}$th of that observed in natural productions. --> Figure \@ref(fig:PR-exposure-test-plot)A shows the phonetic properties of an instance of stimuli generated in this way.

## Test
For the test phase, we followed a similar procedure. We used the same continuum from a typical `r categories.PR[1]` to a typical `r categories.PR[2]`, and then selected six stimuli along that continuum that would be expected to yield `r paste(paste0(as.numeric(rev(unique(d.PR.test$Item.Intended_proportion_category1))) * 100, "%"), collapse = ", ")` `r categories.PR[1]`-responses in a norming experiment without any prior exposure to the talker's speech. Since the test tokens present the phonetic contrast in a non-biasing context (e.g., /`r linguisticsdown::cond_cmpl("ɪ")`\_`r linguisticsdown::cond_cmpl("ɪ")`/), we set the effect of lexical context to $p(category | lexical\ context) = .5$. The resulting test tokens shown in Figure \@ref(fig:PR-exposure-test-plot)B closely resemble the placement of test stimuli expected in experiments on perceptual recalibration [e.g., @norris2003; @kraljic-samuel2005; @kraljic-samuel2006]: as is typical for such experiments, test tokens are placed primarily where they are expected to be perceptually most ambiguous prior to exposure (i.e., close to the prior category boundary), with one additional test token towards each end of the continuum (typically somewhere between 10-25% and 75-90%, respectively). Our approach further assumes that the exposure and test stimuli are carefully chosen to have phonological contexts that allow generalization from exposure to test, as differences in the types of phonological context between exposure and test can reduce or even completely block perceptual recalibration [@eisner2013; @mitterer2013]. For example, the original study that we model our procedure on mostly used exposure stimuli in which /d/ or /t/ occurred word-medially at the onset of a syllable, between two vowels [e.g., _crocodile_, _academic_, etc., see Table 1 in @kraljic-samuel2006] or at least between two sonorants (e.g., _legendary_, _secondary_). In the test stimuli, the /d/-/t/ contrast occurred in a similar position (either /a_a/ or /`r linguisticsdown::cond_cmpl("ɪ")`\_`r linguisticsdown::cond_cmpl("ɪ")`/). 

# Creating the stimuli for the (simulated) accent adaptation paradigm {#sec:SI-AA}
As is the case for experiments on perceptual recalibration, the majority of studies on accent adaptation do not report the acoustic properties of the exposure and test stimuli, or present analyses that relate those properties to participants' responses.^[Notable exceptions include, for example, the labs of Drs. Kartushina (UOslo) and Schertz (UToronto). In past work, we have analyzed to what extent the results of experiments on accent adaptation (including differences between experiments) can be explained by the specific phonetic properties of the stimuli [e.g., @tan2021; @xie2017; @xie2021jep]. These studies find that the results of experiments on accent adaptation can strongly depend on the choice of stimuli. On the one hand, this should not be surprising. But it is often not considered in the interpretation of seemingly unexpected results.] The stimulus generation procedure described below mimics a scenario that has been reported in previous work [e.g., @schertz2015].

## Exposure
We simulate a scenario where the /t/ category remains unchanged between the L1- and L2-accents while the /d/ category is altered in the L2 accent. In the specific simulations presented in the main text, the cue weighting was changed so that the primary cue for L1 listeners (VOT) becomes the secondary cue in the L2-accented speech. This was achieved by adjusting the category mean of /d/ so that its distance from the category mean of /t/ is reduced along VOT but increased along f0, while preserving the relative ordering (i.e., the category mean of /d/ has smaller f0 and VOT than that of /t/). Additionally, we assume no difference in category covariance between the L1 and the L2 accents. This was done to focus our analysis on the influence of relative locations of the two categories between L1 and L2, keeping the effects of category dispersion constant. We note, however, that L1 and L2 categories also often differ both the location (category means) *and* their dispersion [category variance-covariance, e.g., @schertz2015; @xie-jaeger2020]. The code in this R markdown document can be straightforwardly extended to simulate such cases as well.

## Test
For the test phase, we randomly sample 60 tokens per category from the L2-accented categories. This procedure matches the approach typically taken by studies on accent adaptation: researchers record L2-accented speakers’ productions of the target categories without making specific selection based on acoustic-phonetic features. 

<!-- # Additional considerations for ASP analyses of future experiments {#sec:more-recommendations} -->
<!-- For researchers, who plan to the ASP framework to analyze their experiments, we highlight three considerations. First, it is important to keep in mind computational feasibility. For example, unsupervised distributional learning paradigms that employ unlabeled exposure in randomized order [@clayards2008; @munson2011; @nixon2016] in theory provide extremely informative data, as each individual trial constitutes a test trial with its own unique exposure history. However, in practice the computational costs of fitting ASP to many different exposure conditions make this approach infeasible. Second, testing---especially, when it involves distribution of test stimuli that deviate from those during exposure [e.g., uniform distributions that result from repeating each test stimulus equally often as is standard in most experiments]---is expected to reduce the effects of exposure [@liu-jaeger2018; liu-jaeger2019; @REFS; for discussion, see @theodore2021]. Third, any factors that might lead participants to be uncertain whether the stimuli during test come from the same source (e.g., talker) as those during exposure should weaken the transfer of exposure effects to test [@kleinschmidt-jaeger2015]. This could be, for example, due to the wording of instructions or simply because test begins with a new block with a different task than exposure (as is almost always the case in perceptual recalibration, and not uncommon in accent adaptation).  -->

<!-- We therefore recommend designs with a small number of within-subject tests, each sufficiently short compared to the length of exposure, and exposure phases that are shared by a sufficiently large number of participants (so that their effect can be reliably estimated). In recent work, we have approached the question of what constitutes “sufficient” through a combination of power analyses and pilot experiments [@burchill-jaeger2021]---an admittedly time- and resource-consuming approach that has, however, yielded a much better understanding of our data. [XX11]Finally, whenever possible, test and exposure phases should transition seamlessly into each other. Alternatively, instructions should use accessible language to highlight whether the upcoming stimuli come from the same source. -->

