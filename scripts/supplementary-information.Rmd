\newpage
\setcounter{page}{1}
\renewcommand{\thesection}{\S \arabic{section}}
\renewcommand{\theHsection}{sisection. \arabic{section}}


# Supplementary information for *Xie, Jaeger, & Kurumada (2022). What we do (not) know about the mechanisms underlying adaptive speech perception* {-}
Both the main text and these supplementary information (SI) are derived from the same R markdown document available via OSF at [https://osf.io/q7gjp/](https://osf.io/q7gjp/). **The PDFs for both the main text and the SI are best viewed using Acrobat Reader.** Some links and animations might not work in other PDF viewers.

\setcounter{section}{0}
# Required software {#sec:SI-software}
The document was compiled using \texttt{knitr} [@xie2021] in RStudio with R:

```{r} 
version
```

We used the following R packages to create this document: `r cite_r("latex-stuff/r-references.bib", withhold = T, pkgs = c("MVBeliefUpdatr"))`. If opened in RStudio, the top of the R markdown document should alert you to any libraries you will need to download, if you have not already installed them. This includes the \texttt{MVBeliefUpdatr} library that supports working with multivariate Gaussian ideal observers and adaptors (see also Dave Kleinschmidt's \texttt{beliefupdatr} library that this library is based on). The full session information is provided at the end of this document.

**Beyond R**, you will also need to download the IPA font [SIL Doulos](https://software.sil.org/doulos/download/) and a Latex environment like (e.g., [MacTex](https://tug.org/mactex/mactex-download.html) or use \texttt{install\_tinytex} from the R library \texttt{tinytex}). The 3D figures require the [\texttt{orca} commandline tool](https://github.com/plotly/orca#installation). It is recommended that you use "Method 4" to install the standalone binaries. Some of the R libraries evoked in the R markdown code might require additional freely available non-R software (e.g., the R library \texttt{sf} requires non-R *gdal*). If installation of any R library fails, follow the prompts in the error message.

# A database of natural productions of word-initial stop voicing in L1 US English [@chodroff-wilson2018] {#sec:SI-chodroff}
All case studies presented in the main text are based on a phonetically annotated database of word-initial stop voicing in L1 US English [@chodroff-wilson2018]. Chodroff and Wilson (2018, p. 3) describe the database:

> The data was extracted from an audited subset of the Mixer 6 corpus (Brandschain et al. 2010, Brandschain
et al. 2013; Chodroff et al. 2016) containing approximately 45 minutes of read speech from 180 native AE
speakers (102 female). Transcripts were aligned to the corresponding WAV files with the Penn Forced Aligner
(Yuan and Liberman 2008), and all word-initial prevocalic stop consonants were further processed with
AutoVOT (Keshet et al. 2014). AutoVOT automatically identifies the stop release and following vowel onset
within a user-specified window of analysis. Further details about the talkers, read sentences, and boundary
alignments can be found in Chodroff and Wilson (2017).Â³

> COG, positive VOT, and onset f0 in the following vowel were measured for each stop. COG was calculated
from a smoothed spectrum over the initial portion of the release burst. Each spectrum was computed by averaging
FFTs from seven consecutive 3 ms windows, with the first window centered on the burst transient and
a window shift of 1 ms (Hanson and Stevens 2003; Flemming 2007; Chodroff and Wilson 2014). Positive VOT
was defined as the duration from stop release to the onset of periodicity in the vowel; this was automatically
extracted from the AutoVOT boundaries or from manually-corrected boundaries when available. The
f0 value was the first one measured by Praat (Boersma and Weenink 2016) within 50 ms after the following
vowel onset.

<!-- 
# From email communication with Eleanor Chodroff (10/14/2021):
# 
# filename + trial uniquely identify an observation. Trial = the TextGrid interval number for a 
# particular session recording, which each have a unique filename (that includes the subject ID, 
# the date, and some other info). Adding "subj" won't hurt. I wouldn't rely on "word" just in 
# case they happened to produce multiple instances of the same word within the recording. 
#
# vot: 
#   VOT in ms. VOT is reported in the LingVan 2018 paper.
#
# cog, spectral.var, skew, kurtosis: 
#   these are the summary statistics of the power spectral density measures in each of the 33 bins, 
#   where each bin is 250 Hz wide. COG = Center of gravity (Hz), spectral.var = spectral variance 
#   (take the square root to get the SD, a more palatable number and one that will be in Hz), 
#   skew = spectral skewness, kurtosis = spectral kurtosis. COG is reported in the LingVan 2018 paper
#   See Forrest et al. 1988 for a good description of these measures.
#
# usef0:
#   f0 measures were taken every 5 ms after the start of the vowel up to 50 ms into the vowel 
#   (see f0_1 to f0_10 columns). usef0 is the first defined instance of f0. usef0 is reported in the 
#   LingVan 2018 paper.
-->

Following advice from Eleanor Chodroff, we removed tokens with f0 measurements of above `r max.f0` Hz, as those were implausible and likely reflected a measurement error (pitch doubling). We further subset the data talkers for which all three cues (VOT, COG, and f0) were available for at least `r min.observation.n` observations each per stop category. This was done because we detected that the f0 of a good number of talkers exhibited evidence of bimodality. To remove talkers with bimodal f0 measures, we applied a test of multimodality [the dip test, as implemented in the library \texttt{diptest} in \texttt{R}, @maechler2021]. Restricting our data set to talkers with at least `r min.observation.n` observations each per stop category allowed us to more reliably identify bimodal f0 distributions. Talkers for which the null of unimodality was rejected ($p<$ `r max.p`) for the raw f0 or Mel-transformed f0 of *any* stop category were excluded.^[We thank Eleanor Chodroff for help with this issue. Her inspection of some example tokens revealed that both creaky voice and pitch halving contributed to the bimodal pattern. Pitch halving refers to cases in which the f0 detection algorithm wrongly infers the f0 to be half of its true value. One reason for such estimation mistakes can be when the true f0 falls outside the range considered by the algorithm (here: 75-500 Hz, regardless of talker gender, Eleanor Chodroff, p.c.).]

This left `r nrow(d.chodroff_wilson)` observations from `r nlevels(d.chodroff_wilson$Talker)` different talkers and `r nlevels(d.chodroff_wilson$Word)` words. These are the data that we used to derive plausible estimate of the overall and category-specific VOT and Mel distributions that underlie the case studies presented in the main text. Figure \@ref(fig:chodroff-stop-VOT-f0ST) shows the individual tokens and fitted bivariate Gaussian category likelihood of three randomly selected talkers from this data set prior to C-CuRE normalization. Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu) further illustrates the degree of cross-talker variability by plotting all talkers' category means relative to the category likelihood across all talkers prior to normalization.

```{r chodroff-parameters}
n.subj <- 3
set.seed(59876)
```

(ref:chodroff-stop-VOT-f0ST) Unormalized VOT and f0 of word-initial stop consonants in L1 US English from `r n.subj` random talkers from the database [@chodroff-wilson2018]. Transparent points show individual tokens, which cover a range of different phonotactic, lexical, and utterance contexts. Solid points show talker-specific means over these tokens, connected by a gray line. Ellipses show the 95% probability mass boundary for the talker-specific bivariate Gaussian category likelihoods. 

```{r chodroff-stop-VOT-f0ST, fig.width=base.width * 3 + 1, fig.height = base.height * 3, fig.cap=c("(ref:chodroff-stop-VOT-f0ST)")}
talkers <- sample(unique(d.chodroff_wilson$Talker), n.subj, replace = F)

p.talkers <- d.chodroff_wilson %>% 
  # Keep random subject of subjects
  ungroup() %>%
  filter(Talker %in% talkers) %>%
  ggplot(aes(x = VOT, y = f0_Mel, color = voicing, shape = gender)) +
  geom_point(alpha = .1) +
  stat_ellipse() +
  scale_x_continuous(expression("VOT (ms)")) +
  scale_y_continuous(expression("f0 (Mel)")) +
  scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) +
  scale_shape_discrete("Gender") +
  scale_size_continuous("n", range = c(.1, 3)) +
  facet_grid(Talker ~ poa) 

p.talkers +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(group = Talker),
    alpha = .9, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    alpha = .9)
```

(ref:chodroff-stop-VOT-f0ST-mu) By-talker means of VOT and f0 for all six stop consonants and all `r nlevels(d.chodroff_wilson$Talker)` talkers in the database [@chodroff-wilson2018] for which all phonetic cues were available. Ellipses show the 95% probability mass for talker-independent bivariate Gaussian categories if the data from all talkers are pooled independent of talker identity.

```{r chodroff-stop-VOT-f0ST-mu, fig.width=base.width * 3 + 1, fig.height = base.height, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-mu)")}
p.means <-
  d.chodroff_wilson %>% 
  ggplot(aes(x = VOT, y = f0_Mel, color = voicing)) +
  stat_ellipse() +
  scale_x_continuous(expression("VOT (ms)")) +
  scale_y_continuous(expression("f0 (Mel)")) +
  scale_color_manual("Voiced", breaks = c("yes", "no"), values = colors.voicing) +
  scale_shape_discrete("Gender") +
  scale_size_continuous("n", range = c(.1, 3)) +
  facet_grid(. ~ poa)

p.means +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(group = Talker),
    alpha = .1, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT, f0_Mel),
        mean),
    mapping = aes(shape = gender),
    alpha = .3)
```

## Applying C-CuRE
We follow @mcmurray-jongman2011 and use linear regression to remove the effects of talker from each observation in the database. In extending normalization accounts to our present goals, we encountered three decision points that we have not previously seen discussed:

  1. C-CuRE removes the *overall* cue mean from each observation's cue value. Based on this, it would seem most appropriate to include all six stop categories in the estimation of the cue means. However, the experiments we model in the main text involve only two of the categories (/d/ and /t/). We thus decided to only include tokens of /d/ and /t/ in the estimation of cue means. This decision was made because C-CuRE is intended to correct observed cue values for listeners' expectations *for the current context*. Put differently, we assume that listeners expect that the sound will be a /d/ or a /t/ since these are the only two response options provided to participants. This assumption affects some of our results (but not as much as it would if we studied /b/-/p/ or /g/-/k/ since the mean cue values for /d/s and /t/s are closer to the mean cue values across all six categories). 
  
  2. C-CuRE is meant to correct for effects of both talkers and *phonological* contexts. However, in the @chodroff-wilson2018 data /d/ and /t/ occur across different phonological contexts, so that inclusion of phonological contexts as a predictor in the regression model would indirectly capture information about category identity. We thus limited the data further to 9 pairs of /d/- and /t/-items that had identical vowels following the stop consonant (e.g, _*d*ied_ and _*t*ime_). While this does not completely remove the effects of phonological context, it both reduces the variability in cue values associated with phonological contexts (thus providing a more accurate estimate of the expected outcome of normalization) and *balances* the effect of phonological context (de-confounding it from category identity). 
  
  3. Finally, the original data from @chodroff-wilson2018 contained nearly three times as many /d/s as /t/s, and this asymmetry was retained after the data cleaning procedures described above. While this particular imbalance is probably not reflective of natural speech, natural speech will often exhibit *some* form of asymmetry in the relative frequencies of categories. This raises the question of whether listeners somehow correct for these asymmetries when normalizing the speech input for an experiment for which there all categories can be expected to be equally frequent. Following similar considerations as in Point 1 above, we assume this to be the case. We thus randomly subsampled equal numbers of tokens for each /d/ and /t/ from all talkers. This number was determined by the number of tokens of the less frequent category for each talker. For instance, if a talker produced 40 /d/s and 20 /t/s, then 20 /d/ and 20 /t/ observations were randomly pulled from the data. 

For experimenters, the issues described here will arise whenever the distribution of categories and/or phonological contexts differ between prior exposure and the exposure in the experiment, or between exposure and test in the experiment: the estimated mean will then reflect expectations that do not veridically reflect the statistics of the current input. In previous evaluations of C-CuRE this was never the case, since those evaluations made a number of unrealistic assumptions: the data previously used to test C-CuRE typically were balanced across combinations of (1) categories, (2) phonological contexts, and (3) talkers, and (4) were so equally for both C-CuRE was 'fit' on and the data that it was tested on [see, e.g., @mcmurray-jongman2011; for a critique and demonstration that these assumptions can affect the conclusions to be drawn about the plausibility of different normalization approaches, see @barreda-nearey2018]. For the present study, we investigate how listeners might transfer and adapt expectations based on previous long-term exposure to novel input from an unfamiliar talker. We therefore used subsamples that were well-balanced between /d/ and /t/ tokens in terms of their frequency and phonological contexts---a pattern matched to those employed during the exposure and test in typical experiments. We note, however, that none of our core results seem to depend on the assumptions we make here. We obtained qualitatively identical results for all critical comparisons in previous simulations that did not subsample the data to be balanced with regard to /d/ and /t/ or their phonological contexts.

The three issues raised above also raise questions about C-CuRE that go beyond practical concerns for experimenters. For any of the three decisions we described above, it is an empirical question whether listeners do something similar. Put differently, there are complexities in applying approaches like C-CuRE to scenarios that are likely to occur in everyday speech perception that need to be investigated in future research. 

For example, our third assumption essentially means that normalization is sensitive to which category tokens are inferred to originate from (unlike any normalization accounts we know of), and this can result in some counter-intuitive predictions. Consider a scenario, in which a listener hears 100 typical /d/ tokens that match the listener's prior expectations for /d/. As formulated in @mcmurray-jongman2011, C-CuRE would predict that the listener's estimate for the cue mean will move towards the category mean of /d/ (since normalization is assumed to be insensitive to which category tokens originate from).^[Intriguingly, this would predict that the listener will categorize fewer tokens along the prototypical /d/ to /t/ continuum as /d/---in line with findings that repeated exposure to prototypical tokens of a category leads to selective adaptation [e.g., @samuel1986; @samuel2021; @vroomen2007; for discussion, see also @kleinschmidt-jaeger2016pbr].] Alternatively, if normalization *is* sensitive to which category a token is inferred to originate from, then the listener's estimate of the cue mean is predicted to not change much at all (since all tokens were expected given that they were a /d/). Which of these two rather different conceptualizations of normalization is empirically more adequate is, to the best of our knowledge, unknown.

<!-- TO-DO commented out for now but should go into SI or GD?: It is important to keep in mind that while the updates in normalization in this case study seem computationally simple---requiring only the computation of cue mean of exposure tokens, the computational processes required for the normalization of talker differences in real-life situations can be more complex than first meets the eye. As simulated here, empirical tests of normalization accounts often employ balanced samples across categories within a contrast. However, in real encounters with an unfamiliar talker, human listeners may be dealing with unbalanced data or data in which the token frequency of categories does not follow the pattern of regularities in oneâs long-term input. Both will create artificial shifts in normalized cue space that should not be attributed to cross-talker differences. For instance, in the data employed here, the mean VOTs for /d/ and /t/ are 16ms and 72ms respectively, yielding a cue-level mean of 39ms. Consider two scenarios. In one case, a listener is exposed to a talker who produces equal number of shifted /d/ productions with a mean of 34ms and typical /t/s. In another case, the listener hears a talker who produces twice as many as /t/s than /d/s. In both cases, the cue mean of the newly experienced talker is 53ms. If listeners do not explicitly encode labels for the exposure tokens, then the same boundary shift is predicted in both cases. Yet only the shift in the first case is desirable. In other words, in order to effectively compensate for talker differences, the expectations about the mean of previous input would need to take into account both the previous input itself and its labels (e.g., whether it should be a /d/ or /t/). Listener therefore need to either infer and store category labels along with specific instances or keep track of the frequency by which each category occurs, in addition to inferring the cue mean. Regardless, the computational demand of normalization-based changes is higher than recognized in previous discussions of normalization accounts.  -->

Decisions 1-3 left a total of 5632 observations (2816 each for /d/ and /t/). To avoid over-fitting to individual talkers, we use linear mixed-effects regression rather than ordinary linear regression. This 'shrinks' talker-specific estimates of the cue means towards the overall (population-level) mean, and does so more when less data is available for the particular talker. Separate regressions were use to predict VOT and f0 (Mel). Both regressions used the formula:

\begin{equation}\label{eq:c-cure-regression}
\begin{split}
cue \sim 1 + (1 | Talker)
\end{split}
\end{equation}

Rather than to just use the residuals of these regressions, we only subtracted the random effects (BLUPs) from each observation. This removes the talker-specific effects from each token (as intended by C-CuRE) but leaves observations in the original cue space (rather than residual VOTs centered around 0), thus achieving talker normalization without loss in interpretability. The R code for the steps described here is found in R markdown document for the main text, at the end of Section \@ref(sec:framework). Figures \@ref(fig:chodroff-stop-VOT-f0ST-normalized) and \@ref(fig:chodroff-stop-VOT-f0ST-mu-normalized) replot Figures \@ref(fig:chodroff-stop-VOT-f0ST) and \@ref(fig:chodroff-stop-VOT-f0ST-mu) after C-CuRE normalization has been applied to the data.

(ref:chodroff-stop-VOT-f0ST-normalized) Same as Figure \@ref(fig:chodroff-stop-VOT-f0ST) but for normalized VOT and f0. Normalization does not affect the relative placement of tokens within the talker's space but it does affect the absolute placement. This is also evident in Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu-normalized)

```{r chodroff-stop-VOT-f0ST-normalized, fig.width=base.width * 3 + 1, fig.height = base.height * 3, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-normalized)")}
p.talkers + 
  aes(x = VOT_centered, y = f0_Mel_centered) +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(group = Talker),
    alpha = .9, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    alpha = .9)
```

(ref:chodroff-stop-VOT-f0ST-mu-normalized) Same as Figure \@ref(fig:chodroff-stop-VOT-f0ST-mu) but for normalized VOT and f0---both for the means and for the 95% ellipse.

```{r chodroff-stop-VOT-f0ST-mu-normalized, fig.width=base.width * 3 + 1, fig.height = base.height, fig.cap=c("(ref:chodroff-stop-VOT-f0ST-mu-normalized)")}
p.means +
  aes(x = VOT_centered, y = f0_Mel_centered) +
  geom_line(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(group = Talker),
    alpha = .1, size = .3, color = "gray") +
  geom_point(
    data = . %>% 
      group_by(Talker, gender, category, poa, voicing) %>% 
      summarise_at(
        vars(VOT_centered, f0_Mel_centered),
        mean),
    mapping = aes(shape = gender),
    alpha = .3)
```

# Additional details about the change models {#sec:SI-models}
This section contains additional details about the change models.

## Changes in category representations {#sec:SI-models-changes-in-representations}
The updating of the four $\mathcal{NW^{-1}}$ parameters after $N$ observations of a category $c$ from the talker is described by the equations in \@ref(eq:niw-updating-parameters), and is deterministic [for details and derivation, see @murphy2012, p. 134]. $\kappa_c$ and $\nu_c$ simply increase by 1 with each observation, capturing the fact that each observation adds additional information about the talker's category mean and covariances. $\mathrm{m}_{N,c}$ is a weighted combination of its prior value $\mathrm{m}_{0,c}$ and the category mean of the $N$ observations $\bar{x}$---following the same logic that we applied to the inference of the overall cue mean in Equation \@ref(eq:normalization-change). Similarly, $\mathrm{S}_{N,c}$ is a weighted combination of its prior value $\mathrm{S}_{0,c}$ and the category variability of the $N$ observations,^[$\mathrm{S}_c \triangleq \Sigma_{i=1}^N x_{i,c} x_{i,c}^T$ is the uncentered sum of squares matrix of the $N$ observations [@murphy2012].] plus an additional term that captures the uncertainty about the category mean. Figure \ref{fig:model-belief-updating} visualizes the belief-updating process in Equations \@ref(eq:niw-updating)-\@ref(eq:niw-updating-parameters) as a graphical model.

\begin{equation}\label{eq:niw-updating-parameters}
\begin{split}
\mathrm{m}_{N,c} & = \frac{\kappa_{0,c} \mathrm{m}_{0,c} + N_c \bar{x}}{\kappa_{N,c}} = \frac{\kappa_{0,c}}{\kappa_{0,c} + N_c} \mathrm{m}_{0,c} + \frac{N_c}{\kappa_{0,c} + N_c}\bar{x}_c \\
\kappa_{N,c} & = \kappa_{0,c} + N_c \\
\nu_{N,c} & = \nu_{0,c} + N_c \\
\mathrm{S}_{N,c} & = \mathrm{S}_{0,c} + \mathrm{S}_{\bar{x}_c} + \frac{\kappa_{0,c} N_c}{\kappa_{0,c} + N_c}\left( \bar{x}_c-\mathrm{m}_{0,c} \right) \left( \bar{x}_c-\mathrm{m}_{0,c} \right)^T \\
 & = \mathrm{S}_{0,c} + \mathrm{S}_c + \kappa_{0,c} \mathrm{m}_{0,c} \mathrm{m}_{0,c}^T - \kappa_{N,c} \mathrm{m}_{N,c} \mathrm{m}_{N,c}^T
\end{split}
\end{equation}

\begin{figure}
  \centering
  \tikz{ %
    % exposure
    \node[obs] (cue) {$x_n$} ; %
    \factor[above=of cue] {cuedist} {left:$\mathcal{N}$} {} {}; %
    \node[obs, right=of cuedist] (category) {$c_n$} ; %
    % category parameters
    \node[det, above=of cuedist] (mu) {$\mu_c$} ; %
    \node[det, right=of mu] (Sigma) {$\Sigma_c$} ; %
    \factor[above=of mu] {mudist} {left:$\mathcal{N}$} {} {}; %
    \factor[above=of Sigma] {Sigmadist} {left:$\mathcal{W}^{-1}$} {} {}; %
    % hyperparameters
    \node[det, above=of mudist] (kappa) {$\kappa_{n,c}$} ; %
    \node[det, left=of kappa] (m) {$\mathrm{m}_{n,c}$} ; %
    \node[det, above=of Sigmadist] (S) {$\mathrm{S}_{n,c}$} ; %
    \node[det, right=of S] (nu) {$\nu_{n,c}$} ; %
    % prior
    \factor[above=1 of m] {update_m} {} {} {}; %
    \factor[above=1.05 of kappa] {update_kappa} {} {} {}; %
    \factor[above=.9 of S] {update_S} {} {} {}; %
    \factor[above=1 of nu] {update_nu} {right:update (Equation \ref{eq:niw-updating-parameters}) } {} {}; %
    \node[det, above=of update_m] (prior_m) {$\mathrm{m}_{0,c}$} ; %
    \node[latent, above=5 of update_kappa] (prior_kappa) {$\kappa_{0}$} ; %
    \node[latent, above=5 of update_nu] (prior_nu) {$\nu_{0}$} ; %
    \node[det, above=of update_S] (prior_S) {$\mathrm{S}_{0,c}$} ; %
    % external phonetic database
    \node[obs, above=of prior_m] (expected_mu) {$\mathbf{E}(\mu_{c})$} ; %
    \node[obs, above=of prior_S] (expected_Sigma) {$\mathbf{E}(\Sigma_{c})$} ; %
    % extra nodes to allow connection from cues to updating
    \node[const, left=1.2 of update_m] (update_extra1) {} ; %
    \node[const, left=3 of cue] (update_extra2) {} ; %
    \node[const, right=1 of update_nu] (update_extra3) {} ; %
    % plates
    \plate[inner sep=0.24cm, xshift=-0.06cm, yshift=0.12cm] {plate2} {(mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S) (prior_m) (prior_S) (expected_mu) (expected_Sigma) (update_extra3) } {$\forall c \in categories $}; %
    \plate[inner sep=0.12cm, xshift=-0.06cm, yshift=0.06cm] {plate1} {(cue) (category) (cuedist) (mu) (Sigma) (mudist) (Sigmadist) (kappa) (m) (nu) (S)} {$\forall n \in exposure\ observation$}; %
    \edge {cuedist} {cue} ; %
    \edge {category} {cuedist} ; %
    \edge {mu, Sigma} {cuedist} ; %
    \edge {Sigma} {mudist} ; %
    \edge {kappa,m} {mudist} ; %
    \edge {nu,S} {Sigmadist} ; %
    \edge {mudist} {mu} ; %
    \edge {Sigmadist} {Sigma} ; %
    % updating
    \edge {prior_kappa} {update_kappa} ; %
    \edge {prior_nu} {update_nu} ; %
    \edge {prior_kappa, prior_m} {update_m} ; %
    \edge {prior_kappa, prior_m, prior_S} {update_S} ; %
    \edge {update_kappa} {kappa} ; %
    \edge {update_nu} {nu} ; %
    \edge {update_m} {m} ; %
    \edge {update_S} {S} ; %
    \edge[-] {cue} {update_extra2} ; %
    \edge[-] {update_extra1} {update_extra2} ; %
    \edge {update_extra1} {update_m} ; %
    \edge[-] {update_m} {update_kappa} ; %
    \edge[-] {update_kappa} {update_S} ; %
    \edge[-] {update_S} {update_nu} ; %
    % link to phonetic database
    \edge {expected_mu} {prior_m} ;
    \edge {expected_Sigma} {prior_S} ;
    \edge {prior_nu} {prior_S} ;
  }
  \caption{$\mathcal{NW}^{-1}$ Bayesian belief-updating model, as employed here. We set $\mathrm{m}_{0,c} = \mathbf{E}(\mu_c)$ and $\mathrm{S}_{0,c}=\mathbf{E}(\Sigma_c)$, where $\mathbf{E}(\mu_c)$ and $\mathbf{E}(\Sigma_c)$ are observable from phonetically annotated databases. We further assume that all categories share a common $\kappa_{0,c}$ and $\nu_{0,c}$, shown as $\kappa_{0}$ and $\nu_{0}$. These $\kappa_{0}$ and $\nu_{0}$ are the only two degrees of freedom in this highly simplified model of distributional learning (in the manuscript, we keep the $c$ subscript to avoid confusion with the parameter for changes in normalization in Equation \ref{eq:normalization-change}). All other variables are either observable (filled gray circles) or fully determined by other variables.}\label{fig:model-belief-updating}
\end{figure}

The $\kappa_{c}$s and $\nu_{c}$s are also sometimes called "pseudocounts" because they have a rather intuitive interpretation: the value of these parameters can be seen as describing the number of observations of this category that the listener assumes to have observed from the talker. For example, a listener with $\kappa_{c,0} = 100$ updates her beliefs about an unfamiliar talker's category mean as if she has already seen 100 observations of that category from the talker *prior to having received any input from that talker*. After 900 observations of that category from the unfamiliar talker, this listener's belief about the talker's category mean would be a weighted mixture, made up to 10% by the prior $\mathrm{m}_{c,0}$ and to 90% of the mean of the 900 observed category instances $\bar{x}_c$.^[The extent to which listeners transfer prior expectations to an unfamiliar talker or context (i.e., the values of $\kappa_{c,0}$ and $\nu_{c,0}$) is expected to differ across phonological contrasts [see discussion in @kleinschmidt-jaeger2015].]

### Setting the $m_{0,c}$ and $S_{0,c}$ parameters
It is possible to treat $m_{0,c}$ and $S_{0,c}$ as free parameters of the Normal-Inverse-Wishart belief-updating model, and to infer them from participants' responses in perceptual experiments [@kleinschmidt-jaeger2016cogsci]. This makes it possible to compare whether estimates of these parameters that are purely based on *listeners'* behavior correspond to distributions of category means $\mu_c$ and category covariances $\Sigma_c$ that match those observed in the input that listeners have received throughout their lives---i.e., the joint distribution of $\mu_c$ and $\Sigma_c$ observed in phonetically annotated databases that are are representative of the input listeners have previously received. If such a match is indeed observed, this would lend support to the hypothesis that listeners learn and store knowledge about the statistics of cue-to-category mappings in the input [for initial tests of this type, and further discussion, see @kleinschmidt-jaeger2016cogsci; @tan2022].

However, it is also possible to instead set $\mathrm{m}_{0,c}$ and $\mathrm{S}_{0,c}$ based on phonetically annotated data. This fixes all $J$ DFs for $\mathrm{m}_{0,c}$ and $\frac{J}{2}(K^2+K)$ DFs for $\mathrm{S}_{0,c}$ prior to predicting listeners' behavior but *assumes* (rather than tests) that listeners learn and store knowledge about the statistics of cue-to-category mappings in the input. This latter approach is the one that we took in our case studies. The core idea is to select $\mathrm{m}_{0,c}$ and $\mathrm{S}_{0,c}$ so that they yield a prior joint distribution of $\mu_c$ and $\Sigma_c$ that match those observed in a sufficiently large phonetic database that can reasonably assumed to approximate the type of input an average participant has received throughout their life. 

For the present purpose, we simplify this estimation problem further by setting $\mathrm{m}_{0,c}$ and $\mathrm{S}_{0,c}$ so that the expected values of the *marginal* distributions of $\mu_c$ and $\Sigma_c$ match the maximum likelihood estimates of the category mean and covariance matrix, respectively, in the phonetic database [@chodroff-wilson2018]. This means that we set $\mathrm{m_{0,c}}$ to the empirical mean (i.e., maximum likelihood estimate) of the category's cue distribution, $\bar{x}_c$, estimated from the phonetic database. Since the Normal-Inverse-Wishart belief-updating model describes the prior distribution of the category mean $\mu_c$ as a Normal distribution with mean $\mathrm{m}_{0,c}$ and covariance matrix $\frac{1}{\kappa_{0,c}}\Sigma_c$, this yields an expected prior category mean $\mathbf{E}(\mu_c) = \mathrm{m_{0,c}} = \bar{x}_c$.

The prior distribution of the category covariance matrix $\Sigma_c$ is described by the Inverse-Wishart distribution with scale parameter $\mathrm{S_{0,c}}$ and degree of freedom $\nu_{0,c}$. For our case studies, we thus again set $\mathrm{S_{0,c}}$, such that the expected prior category covariance matrix matches the empirical covariance matrix $\frac{\bar{S}_c}{N}$ (i.e., the empirical sum of squares matrix divided by the number of observations) estimated from the phonetic database *given $\nu_{0,c}$*. This yields $\mathrm{S_{0,c}} = \mathbf{E}(\Sigma_c)(\nu_{0,c}-D-1)$ = $\bar{S}_c(\nu_{0,c}-D-1)$, where $D$ is the dimensionality of input [i.e., the number of phonetic cues considered, cf. @murphy2012, p. 134-5].

We note that the approach employed in our case studies does not take into account that category means and covariance matrices can be correlated. In that case, the expected values of the marginal distributions of $\mu_c$ and $\Sigma_c$ will not be identical to the expected value of the joint distribution of $\mu_c$ and $\Sigma_c$.^[This might also reveal that some of the assumptions of the Normal-Inverse-Wishart belief-updating model are problematic, either for researchers or for listeners.] For future work, it would thus be more appropriate to estimate the *joint* distribution of $\mu_c$ and $\Sigma_c$ from phonetic data, and to find the values for $\mathrm{m}_{0,c}$ and $\mathrm{S}_{0,c}$ that best approximate this distribution (e.g., through moment-matching). For the present purpose, however, this change is not expected to qualitatively affect the conclusions reached in our case studies.

## Changes in decision-making {#sec:SI-models-changes-in-decision-making}
Here, we model the prediction error as the surprisal experienced when observing the category label, $I(c_{observed})$. This surprisal is a log-inverse function of the posterior probability of the category label given the beliefs held by the listener prior to observing the category label. 

\begin{equation}\label{eq:bias-updating}
\begin{split}
\mathrm{logit}(\pi_{n,c_{observed}}) & = \mathrm{logit}(\pi_{n-1,c}) + \beta_{\pi} I(c_{observed}) \\
                          & = \mathrm{logit}(\pi_{n-1,c}) - \beta_{\pi} \log_2 p_{n-1}(c_{observed}) \\
\end{split}
\end{equation}

The same amount that is added to the bias of the observed (labeled) category is subtracted from the response biases for all other categories (uniformly distributed across those categories). <!-- For the simple case of a 2AFC task, which involves only two categories, changes in the bias after $n$ observations are thus described by a logistic regression with intercept $\mathrm{logit}(\pi_{n-1,c})$ and surprisal slope $\beta_{\pi}$:

\begin{equation}\label{eq:bias-probability}
\begin{split}
\pi_{n,c} = \frac{1}{1+ e^{\mathrm{logit}(\pi_{n-1,c}) + \beta_{\pi} I(c_{observed}) }}
\end{split}
\end{equation}

-->
Like the representational change model, the change model for response biases is sensitive to the mismatch between listeners' expectations based on the input and labeling information provided by the context. Unlike the representational change model, however, the change model for response biases uses the new observations to update response biases rather than beliefs about category likelihoods. As such, the change for response biases does not *directly* change the mapping from stimulus properties to categorization responses. The change model for response biases can, however, affect this mapping indirectly. This means that even simple changes in response biases can change listeners' categorization function in complex ways.

We also briefly considered another simple change model for decision-making but never implemented it since it could not possibly explain the findings we present in Sections \@ref(sec:PR) and \@ref(sec:AA). This alternative model holds that listeners aim to infer the relative probability of each category based on recent input, and that response biases reflect these estimates. For example, listeners might enter an experiment with expectations about the relative probability of each category based on their relative frequency in previously experienced input, and then update their expectations based on the relative frequency of the categories within the experiment [similar to belief-updating models of syntactic adaptation, @fine-jaeger2013; @jaeger2019; @prasad2021]. However, such a model could not possibly explain exposure effects in, for example, a typical perceptual recalibration experiment since the relative frequency of the two categories does not differ between exposure conditions.

### (Non)-additivity of changes as a function of $\lambda$ {#sec:consequences-of-lambda}
In Figures \@ref(fig:demonstrate-lapse-bias-change) and \@ref(fig:demonstrate-lapse-bias-change-nonzero-lapse) in the main text, we demonstrate that the (non)-additivity of changes in decision-making depends on $\lambda$. Specifically, changes are additive in the log-odds of the posterior probability of categories if and only if $\lambda \in \{0, 1\}$. To see how this limitation arises, consider how the *log-odds* of a category---e.g., /d/---in Equation \@ref(eq:posterior-probability-lapse) depend on the response biases $\pi_c$ when $\lambda=0$: 

\begin{equation}\label{eq:change-bias}
\begin{split}
p(/d/ | input) & = (1-\lambda) \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/}}{\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} + \lambda \frac{\pi_{/d/}}{\Sigma_i \pi_{c_i}} \\
 & = (1-\lambda) \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/}}{\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} + \lambda \pi_{/d/} \\
 & = \frac{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/} + \lambda \pi_{/d/} \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}}{\Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} \Rightarrow \\
\log \frac{p(/d/ | input)}{p(/t/ | input)}  & = \log \frac{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) \pi_{/d/} + \lambda \pi_{/d/} \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}}{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right) \pi_{/t/} + \lambda \pi_{/t/} \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} \\
 & = \log \frac{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right) + \lambda \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}}{(1-\lambda) \mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right)  + \lambda  \Sigma_i \mathcal{N}\!\left( input | \mu_{c_i}, \Sigma_{c_i}\right) \pi_{c_i}} + \log\frac{\pi_{/d/}}{\pi_{/t/}} \\
\end{split}
\end{equation}

When $\lambda=1$, this simplifies to:

\begin{equation}
\begin{split}
\log \frac{p(/d/ | input)}{p(/t/ | input)} & = \log\frac{\pi_{/d/}}{\pi_{/t/}}
\end{split}
\end{equation}

And for $\lambda=0$:

\begin{equation}
\begin{split}
\log \frac{p(/d/ | input)}{p(/t/ | input)} & = \log \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right)}{\mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right)} + \log\frac{\pi_{/d/}}{\pi_{/t/}} 
\end{split}
\end{equation}

Either way, a change in responses ($\pi$) will result in a change in predicted categorization behavior that is independent of the $input$, leading to changes in the log-odds of categorization responses that are constant across the acoustic-phonetic space. 

Even when $\lambda \not\in \{0,1\}$, equation \@ref(eq:change-bias) suggests that the type of changes in posterior log-odds that can be explained through changes in decision-making are constrained. For example, when the two categories exhibit equal variance (so that $\log \frac{\mathcal{N}\!\left( input | \mu_{/d/}, \Sigma_{/d/} \right)}{\mathcal{N}\!\left( input | \mu_{/t/}, \Sigma_{/t/} \right)}$ describes a line), changes in decision-biases are predicted to lead to changes in categorization that are most extreme between the two category means and converge against two different constants 'outside' of the two category means (see Figure \@ref(fig:lapse-rate-consequences)). The shape of the predicted changes further gains in complexity if the variances of the two categories differ (Figure \@ref(fig:lapse-rate-consequences-unequal)). Further study of this constraint and similar limitations of the other changes models is needed to characterize the range of adaptive behaviors each model can predict (see Recommendation 5 in the general discussion).

(ref:lapse-rate-consequences) Predicted difference in posterior log-odds of /d/ as a function of the lapse rate ($\lambda$), prior response bias for /d/ ($\pi_{/d/}$) and the change in that response bias after exposure ($\Delta_{\pi_{/d/}}$). For this example, only VOT is considered. The mean of /d/ and /t/ were set to that observed in Chodroff & Wilson (2018) and the variance was held equal across the two categories (at the average of the variances observed in Chodroff & Wilson, 2018).

```{r lapse-rate-consequences, fig.width=base.width * 4 + .5, fig.height=base.height * 3 + 1.5, fig.cap="(ref:lapse-rate-consequences)"}
categories <- c("/d/", "/t/")
x <- seq(-30, 150, .5)

logodds <- function(x, pi, lambda) {
  c1 <- dnorm(x, m.VOT_f0_MVG$mu[[1]][1], sqrt((m.VOT_f0_MVG$Sigma[[1]][1] + m.VOT_f0_MVG$Sigma[[2]][1]) / 2))
  c2 <- dnorm(x, m.VOT_f0_MVG$mu[[2]][1], sqrt((m.VOT_f0_MVG$Sigma[[1]][1] + m.VOT_f0_MVG$Sigma[[2]][1]) / 2))

  log((1-lambda) * c1 + lambda * (c1 * pi[1] + c2 * pi[2])) - 
    log((1-lambda) * c2 + lambda * (c1 * pi[1] + c2 * pi[2])) + 
    log(pi[1]/pi[2]) 
}

diff_in_logodds <- function(x, pi_c1, diff_in_pi_c1, lambda = 0) {
  logodds(x, c(pi_c1 + diff_in_pi_c1, 1 - (pi_c1 + diff_in_pi_c1)), lambda) - 
    logodds(x, c(pi_c1, 1 - pi_c1), lambda)
}

crossing(x = list(x), lambda = c(0, .001, .01, .1, .5), pi_c1 = c(.3, .5, .7), diff_in_pi_c1 = c(.025, .05, .1, .2)) %>%
  mutate(
    change_in_logodds = pmap(
    .l = list(x, pi_c1, diff_in_pi_c1, lambda),
    .f = function(x, pi_c1, diff_in_pi_c1, lambda) diff_in_logodds(x, pi_c1, diff_in_pi_c1, lambda))) %>%
  unnest(c(x, change_in_logodds)) %>%
  ggplot(aes(x = x, y = change_in_logodds, color = factor(lambda))) +
  geom_path() +
  scale_x_continuous("VOT (in msec)") +
  scale_y_continuous("Difference in log-odds of /d/") +
  scale_color_discrete(expression(lambda)) +
  facet_grid(pi_c1 ~ diff_in_pi_c1, 
             labeller = label_bquote(
               rows = { pi[.(categories[1])] == ~.(as.character(pi_c1)) },
               cols = { Delta[pi[.(categories[1])]] == ~.(as.character(diff_in_pi_c1)) } )) +
  theme(legend.position = "top")
```

(ref:lapse-rate-consequences-unequal) Same as in Figure \@ref(fig:lapse-rate-consequences) but while setting the variances of the two categories to the unequal values observed in Chodroff & Wilson (2018).

```{r lapse-rate-consequences-unequal, fig.width=base.width * 4 + .5, fig.height=base.height * 3 + 1.5, fig.cap="(ref:lapse-rate-consequences-unequal)"}
logodds <- function(x, pi, lambda) {
  c1 <- dnorm(x, m.VOT_f0_MVG$mu[[1]][1], sqrt(m.VOT_f0_MVG$Sigma[[1]][1]))
  c2 <- dnorm(x, m.VOT_f0_MVG$mu[[2]][1], sqrt(m.VOT_f0_MVG$Sigma[[2]][1]))

  log((1-lambda) * c1 + lambda * (c1 * pi[1] + c2 * pi[2])) - 
    log((1-lambda) * c2 + lambda * (c1 * pi[1] + c2 * pi[2])) + 
    log(pi[1]/pi[2]) 
}

crossing(x = list(x), lambda = c(0, .001, .01, .1, .5), pi_c1 = c(.3, .5, .7), diff_in_pi_c1 = c(.025, .05, .1, .2)) %>%
  mutate(
    change_in_logodds = pmap(
    .l = list(x, pi_c1, diff_in_pi_c1, lambda),
    .f = function(x, pi_c1, diff_in_pi_c1, lambda) diff_in_logodds(x, pi_c1, diff_in_pi_c1, lambda))) %>%
  unnest(c(x, change_in_logodds)) %>%
  ggplot(aes(x = x, y = change_in_logodds, color = factor(lambda))) +
  geom_path() +
  scale_x_continuous("VOT (in msec)") +
  scale_y_continuous("Difference in log-odds of /d/") +
  scale_color_discrete(expression(lambda)) +
  facet_grid(pi_c1 ~ diff_in_pi_c1, 
             labeller = label_bquote(
               rows = { pi[.(categories[1])] == ~.(as.character(pi_c1)) },
               cols = { Delta[pi[.(categories[1])]] == ~.(as.character(diff_in_pi_c1)) } )) +
  theme(legend.position = "top")
```

# Creating the stimuli for the (simulated) perceptual recalibration paradigm {#sec:SI-PR}
As also mentioned in the main text, many studies on perceptual recalibration do not report the acoustic properties of the exposure and test stimuli,^[Some studies provide aggregate information [e.g., @kraljic-samuel2006; @kraljic-samuel2007] and very few provide detailed information and visualization [e.g., @drouin2016].] and analyses that relate the acoustic properties of exposure and test stimuli to participants' responses during test remain the exception. In our experience, it is also not uncommon that original recordings are no longer available or only partially available (see one of our recommendations in the general discussions). Phonetic annotations that aid the extraction of acoustic information about these recordings are available for only a very small number of studies.^[Some notable exceptions for research on US English include the labs of Drs. Babel (UBC), Myer (UConn), and Theodore (UConn). In an ongoing project at Rochester, we have collected a database of phonetically annotated perceptual recalibration experiments on L1 US English /s/-/`r linguisticsdown::cond_cmpl("Ê")`/ that links stimulus recordings, phonetic annotations, and over 20 phonetic extracted phonetic cues to categorization responses from several thousand participants across a dozen experiments from multiple labs. Researchers interested in the database can contact any of the authors.] For that reason, we use simulated data in our case study on perceptual recalibration. Our stimulus generation procedure aims to capture the qualitative properties of the most common approaches to stimulus selection in perceptual recalibration experiments. 

## Exposure
As described in the main text, the exposure phase of a typical perceptual recalibration experiment employs both typical stimuli and stimuli that are manipulated to be perceptually ambiguous between the two categories. In practice, researchers determine this point of perceptual ambiguity by first generating a continuum from the recording of the typical sound (e.g., _crocodile_) to a recording in which that sound is replaced with the opposite sound (_crocotile_). Procedures to create these continua range from simple blending of the two recordings---mixing the two recordings weighted by different amplitudes (from 100% _crocodile_ and 0% _crocotile_ to 0% "crocodile" and 100% _crocotile_)---to more careful phonetic manipulations (e.g., inserting addition silence to create longer VOTs) or the use of speech synthesis [for an insightful critique of the frequently used blending procedure, see @theodore-cummings2021]. The former is the by far more common approach. The perceptually most ambiguous point along the resulting continua is then determined either by the experimenter(s) or in a separate norming experiment, with the former approach being far more common. In short, perceptual recalibration experiments do neither carefully select the tokens within each category based on phonetic properties, nor is there any form of counter-balancing of the average phonetic or perceptual shifts across the two bias conditions. 

For the sake of generality, we simulate the general outcome of any of these procedures by imaging an experimenter (or participants) listening to stimuli along a continuum ranging from typical `r categories.PR[1]` to typical `r categories.PR[2]`. We simulate the experimenter with the same perceptual model we assume for general L1-listeners (see Section \@ref(sec:framework)). We assume that the experimenter, who can listen arbitrarily often to any of the stimuli, will make her final decision as to which stimuli should be selected for the shifted tokens without attentional lapses ($\lambda=0)$. We further assume that experimenter's response bias is affected by the lexical context. Specifically, we set $\pi_{intended~category} =$ $p(intended~category | lexical~context) =$ `r my_experimenter.lexical_bias_strength`. This captures the fact that even the experimenter's perception is somewhat affected by the lexical context. <!-- Finally, we assume that the realizations of phonetic contrasts used in perceptual recalibration experiments are less variable than in naturally occurring speech, given that the stimuli for such experiments typically reflect *read* speech recorded in sound-attenuated booths, and experimenters would remove any 'atypical' recordings. Specifically, we arbitrarily assume that the variance of the typical (and shifted) stimuli in the experiment is $\frac{1}{`r my_experimenter.variability_reduction`}$th of that observed in natural productions. --> Figure \@ref(fig:PR-exposure-test-plot)A shows the phonetic properties of an instance of stimuli generated in this way.

## Test
For the test phase, we followed a similar procedure. We used the same continuum from a typical `r categories.PR[1]` to a typical `r categories.PR[2]`, and then selected six stimuli along that continuum that would be expected to yield `r paste(paste0(as.numeric(rev(unique(d.PR.test$Item.Intended_proportion_category1))) * 100, "%"), collapse = ", ")` `r categories.PR[1]`-responses in a norming experiment without any prior exposure to the talker's speech. Since the test tokens present the phonetic contrast in a non-biasing context (e.g., /`r linguisticsdown::cond_cmpl("Éª")`\_`r linguisticsdown::cond_cmpl("Éª")`/), we set the effect of lexical context to $p(category | lexical\ context) = .5$. The resulting test tokens shown in Figure \@ref(fig:PR-exposure-test-plot)B closely resemble the placement of test stimuli expected in experiments on perceptual recalibration [e.g., @norris2003; @kraljic-samuel2005; @kraljic-samuel2006]: as is typical for such experiments, test tokens are placed primarily where they are expected to be perceptually most ambiguous prior to exposure (i.e., close to the prior category boundary), with one additional test token towards each end of the continuum (typically somewhere between 10-25% and 75-90%, respectively). Our approach further assumes that the exposure and test stimuli are carefully chosen to have phonological contexts that allow generalization from exposure to test, as differences in the types of phonological context between exposure and test can reduce or even completely block perceptual recalibration [@eisner2013; @mitterer2013]. For example, the original study that we model our procedure on mostly used exposure stimuli in which /d/ or /t/ occurred word-medially at the onset of a syllable, between two vowels [e.g., _crocodile_, _academic_, etc., see Table 1 in @kraljic-samuel2006] or at least between two sonorants (e.g., _legendary_, _secondary_). In the test stimuli, the /d/-/t/ contrast occurred in a similar position (either /a_a/ or /`r linguisticsdown::cond_cmpl("Éª")`\_`r linguisticsdown::cond_cmpl("Éª")`/). 

# Creating the stimuli for the (simulated) accent adaptation paradigm {#sec:SI-AA}
As is the case for experiments on perceptual recalibration, the majority of studies on accent adaptation do not report the acoustic properties of the exposure and test stimuli, or present analyses that relate those properties to participants' responses.^[Notable exceptions include, for example, the labs of Drs. Chodroff (UZurich), Kartushina (UOslo) and Schertz (UToronto). In past work, we have analyzed to what extent the results of experiments on accent adaptation (including differences between experiments) can be explained by the specific phonetic properties of the stimuli [e.g., @tan2021; @xie2017; @xie2021jep]. These studies find that the results of experiments on accent adaptation can strongly depend on the choice of stimuli. On the one hand, this should not be surprising. But it is often not considered in the interpretation of seemingly unexpected results.] The stimulus generation procedure described below mimics a scenario that has been reported in previous work [e.g., @schertz2015].

## Exposure
We simulate a scenario where the /t/ category remains unchanged between the L1- and L2-accents while the /d/ category is altered in the L2 accent. In the specific simulations presented in the main text, the cue weighting was changed so that the primary cue for L1 listeners (VOT) becomes the secondary cue in the L2-accented speech. This was achieved by adjusting the category mean of /d/ so that its distance from the category mean of /t/ is reduced along VOT but increased along f0, while preserving the relative ordering (i.e., the category mean of /d/ has smaller f0 and VOT than that of /t/). Additionally, we assume no difference in category covariance between the L1 and the L2 accents. This was done to focus our analysis on the influence of relative locations of the two categories between L1 and L2, keeping the effects of category dispersion constant. We note, however, that L1 and L2 categories also often differ both the location (category means) *and* their dispersion [category variance-covariance, e.g., @schertz2015; @xie-jaeger2020]. The code in this R markdown document can be straightforwardly extended to simulate such cases as well.

## Test
For the test phase, we randomly sample 60 tokens per category from the L2-accented categories. This procedure matches the approach typically taken by studies on accent adaptation: researchers record L2-accented speakersâ productions of the target categories without making specific selection based on acoustic-phonetic features. 

<!-- # Additional considerations for ASP analyses of future experiments {#sec:more-recommendations} -->
<!-- For researchers, who plan to the ASP framework to analyze their experiments, we highlight three considerations. First, it is important to keep in mind computational feasibility. For example, unsupervised distributional learning paradigms that employ unlabeled exposure in randomized order [@clayards2008; @munson2011; @nixon2016] in theory provide extremely informative data, as each individual trial constitutes a test trial with its own unique exposure history. However, in practice the computational costs of fitting ASP to many different exposure conditions make this approach infeasible. Second, testing---especially, when it involves distribution of test stimuli that deviate from those during exposure [e.g., uniform distributions that result from repeating each test stimulus equally often as is standard in most experiments]---is expected to reduce the effects of exposure [@liu-jaeger2018; liu-jaeger2019; @REFS; for discussion, see @theodore2021]. Third, any factors that might lead participants to be uncertain whether the stimuli during test come from the same source (e.g., talker) as those during exposure should weaken the transfer of exposure effects to test [@kleinschmidt-jaeger2015]. This could be, for example, due to the wording of instructions or simply because test begins with a new block with a different task than exposure (as is almost always the case in perceptual recalibration, and not uncommon in accent adaptation).  -->

<!-- We therefore recommend designs with a small number of within-subject tests, each sufficiently short compared to the length of exposure, and exposure phases that are shared by a sufficiently large number of participants (so that their effect can be reliably estimated). In recent work, we have approached the question of what constitutes âsufficientâ through a combination of power analyses and pilot experiments [@burchill-jaeger2021]---an admittedly time- and resource-consuming approach that has, however, yielded a much better understanding of our data. [XX11]Finally, whenever possible, test and exposure phases should transition seamlessly into each other. Alternatively, instructions should use accessible language to highlight whether the upcoming stimuli come from the same source. -->

# Computational limitations of change models afford qualitative tests of their *sufficiency* {#sec:SI-sufficiency}
The three change models---as well as the three general hypotheses that they aim to implement---differ in their computational assumptions. Given these assumptions, it is possible to conduct experiments that can decisively test the *(in)sufficient* of any of the three mechanisms. Indeed, though not necessarily framed as such, some previous findings already speak to the (in)sufficiency of the three mechanisms. Here we summarize some of those key findings, and describe possible extensions to them. We note, however, that---as we outlined in the general discussion---we do not believe that such (in)sufficiency tests are themselves sufficient to advance research on adaptive speech perception.

## Normalization vs. changes in category representations
While both normalization and the representational change model assume that listeners are sensitive to talker-specific changes in the usage of phonetic cues, only the latter tracks those differences separately for each category (e.g., /d/ vs /t/). This makes normalization computationally more parsimonious than representational changes for both researchers and listeners: for the normalization model, the number of parameters that researchers need to determine (e.g., fit from the behavioral data of perception experiments), and the number of estimates that listeners need to infer and store from the speech input does not increase with the number of categories [for related discussion, see also @apfelbaum-mcmurray2015]. For example, for the C-CuRE-based normalization model employed in our case studies, researchers need to determine only one parameter ($\kappa_0$) and listeners are assumed to infer and store only the *overall* means of all cues ($K$ values for $K$ cues). In contrast, the representational change model employed in our case studies requires researchers to determine two parameters *per category*, and listeners are assumed to infer and store the cue means and covariance matrices of *each category* ($JK + \frac{J}{2}(K^2+K)$ values for $K$ cues and $J$ categories).^[Even if C-CuRE normalization, which only centers cues, is made more comparable to the representational change model by extending normalization to include cue standardization [also known as z-scoring, as used in, e.g., Lobanov normalization, @lobanov1971], this would introduce just one additional parameter for researchers (the equivalent for $\kappa_0$ but for the estimation of the cue variances), and listeners would be assumed to learn and store $2K$ values for $K$ cues.] 

While the parsimony of normalization offers a computational advantage in terms of efficiency, it also comes with limitations. We discuss three such limitations that future work can exploit to evaluate the sufficiency of this mechanism. First, normalization accounts predict that the effects of exposure on subsequent perception do not depend on the category membership inferred by listeners.<!--^[To be precise, some normalization accounts allow the category identities of *surrounding* segments of speech to affect the normalization of cues on the target segment. This is, for example, how C-CuRE normalizes for effects of surrounding phonetic context [@mcmurray-jongman2011].<!-- This makes me wonder how they explain the Holt findings that a sine tone can affect subsequent perception. Do McMurray & Jongman even discuss this finding? [CK] They do. They actually say that C-CuRE builds on the type of "auditory contrast accounts" driven by any surrounding speech. They suggest that the phonetic context can be used but are not always needed. e.g., "C-CuRE also builds on auditory contrast accounts (Lotto &Kluender, 1998, Holt, 2006; Kluender, Coady & Kiefte, 2003) by proposing that cues are interpreted relative to expectations, though these expectations can be driven by categories (perhaps in addition to lower-level expectations)"(Section 5.5)This does not, however, affect the point we make here.]--> For example, for C-CuRE, only the overall cue mean of the input can affect subsequent perception, regardless of whether the lexical context labels the input as one category or another. Although not originally discussed in the context of normalization, there is evidence that challenges this prediction. In their seminal study on perceptual recalibration, @norris2003 exposed participants to /f/- or /s/-biased inputs using the same general design that we discussed for Case Study 1. The critical conditions exposed participants either to words with typical /f/ and words with atypical (shifted) /s/ or to words with typical /s/ and words with atypical (shifted) /f/. This resulted in the signature perceptual recalibration effect.

Importantly though, no boundary shift was observed in control conditions where the shifted tokens were embedded in non-words instead of real words, everything else being identical. In short, the control conditions differed from the experimental conditions almost exclusively in whether the atypical inputs were lexically labeled to be of a particular category.<!--Importantly though, Norris and colleagues also included several control conditions. Two control conditions of relevance to the present discussion exposed participants either to words with typical /f/ and *non*-words with atypical (shifted) /s/ or to words with typical /s/ and *non*-words with atypical (shifted) /f/. The atypical /f/ and /s/ sounds in these control conditions were acoustically identical to those in the experimental conditions, and the non-word contexts in the control conditions matched the word contexts in terms of the phonetic context surrounding the critical /f/ and /s/ sounds (specifically, in terms of lexical stress and the vowel immediately /f/ or /s/). In short, the two control conditions differed from the experimental conditions almost exclusively in whether the atypical inputs were lexically labeled to be of a particular category. Unlike the experimental conditions, however, the control conditions did *not* elicit the signature boundary shift [@norris2003, Experiment 2].--><!-- As Norris and colleagues concluded, "[...] perceptual learning depended on exposure to an ambiguous speech sound in lexically biased contexts" (p. 227).--> The perceptual recalibration effect thus seems to depend on the category that the shifted atypical tokens are attributed to [@norris2003, p. 227]---contrary to what would be expected if adaptive speech perception was solely achieved through cue-level normalization.<!-- ^[Later work found that the label (biasing information) does not have to be provided by the *lexical* context. For example, visual information about lip position [@vroomen2007] can induce perceptual recalibration, too. Neither do the shifted atypical tokens have to be precisely "ambiguous" (i.e., located at the prior category boundary). For example, smaller shifts from the prior category mean towards the category boundary can also elicit (smaller) boundary shifts [@babel2019]. But this does not affect the take-home message of Norris et al.'s (2003) finding.]--> The fact that this finding was obtained for (Dutch) fricatives is of particular interest since C-CuRE normalization has been found to provide a good fit against the recognition of (American English) fricatives. While most of the tests of C-CuRE were non-contrastive [e.g., @mcmurray-jongman2011; @mcmurray-jongman2016], @apfelbaum-mcmurray2015 compared C-CuRE to a representational change model (specifically, exemplar models) and found very similar performance, with the latter providing a slightly better fit against human responses. We consider the question of how labeling information affects adaptive speech perception as a productive venue for future research. 

A second computational limitation of normalization accounts is specific to accounts that only correct for differences in the overall mean of cues but not differences in cue variability (like C-CuRE). <!--Such normalization accounts seem to be in conflict with existing findings, although these findings have to the best of our knowledge not previously been discussed in these terms. -->In a ground-breaking study that was targeted at a separate question, @clayards2008 exposed participants to distributions of synthesized /b/ and /p/ tokens (as in, e..g, *beach*-*peach* continuum). Between participants, the VOT of these tokens had been manipulated to either form wide or narrow VOT distributions for both /b/ and /p/. The VOT means of /b/ and /p/ were identical in both conditions<!-- (0 and 50 msecs, respectively)-->. Since acoustic cues are encoded relative to the cue mean only in C-CuRE and similar normalization accounts, these accounts thus do *not* predict any differences in the effects of these exposure conditions. Clayards and colleagues, however, found that participants in the wide variance condition exhibited shallower categorization functions along the VOT continuum [conceptually replicated in @nixon2016]---as predicted by representational change models [see @clayards2008; @kleinschmidt-jaeger2015; @theodore-monto2019]. Future research should test whether this finding replicates for more natural-sounding (rather than resynthesized, robotic-sounding) stimuli and in situations where task demands more closely resemble those of everyday speech perception (less repetition, more lexical heterogeneity, words presented in sentential contexts rather than in isolation, etc.). <!-- TO DO: was Theodore & Monto not natural sounding? I think it was. So then the first part of this has been done. Let's discuss. --> If such replications are obtained, this would argue that normalization would at least have to include standardization or similar corrections for the variability of cues [as proposed in @johnson2020; @lobanov1971; @monahan-idsardi2010].

A third limitation of normalization accounts that can be productively explored in future research is the lack of category specificity. Consider for example, a possible extension of the study by Clayards and colleagues. In the study by @clayards2008, both /b/ and /p/ either had narrow variance along VOT (SD = 8 msecs) or wide variance (SD = 14 msecs). This confounds the category-specific variance with the overall variance of the cues along VOT. It is, however, possible to manipulate the variance of the two categories while keeping both the means of the two categories (and thus the overall cue mean) and the overall cue variance constant. Figure \@ref(fig:proposed-experiment-asymmetric-variance) depicts two exposure conditions that achieve this, along with changes in the categorization function predicted by normalization and representational change models. This demonstrates how researchers might use the lack of category-specificity to disentangle normalization and representational change accounts. Other possible manipulations in this vein could include category-specific changes in the *co*variance of cues.<!--^[This is not to be confused with another experimental manipulation that is sometimes describes in terms of covariance [@REF]: changes in which of two or more cues exhibits the largest relative distance between two category means [which changes the relative reliability of cues, as discussed in @schertz-clare2020]. ASP-based simulations would be required to assess whether such manipulations cannot also be explained by normalization. We suspect that they can.]--> Basically, any manipulation that leaves the *overall* cue mean and variance unaffected while predicting differences in the categorization function under the representational change model can test whether the human listeners exhibit more flexibility than expected by normalization accounts. We note, however, that any such test should take into account that listeners can have strong prior beliefs based on the speech input they have received previously, and that this might include strong prior beliefs about *how* the realization of categories varies across talkers [@kleinschmidt-jaeger2015]. If the manipulations employed by researchers strongly violate those expectations, this needs to be carefully considered in the derivation of predictions. 

(ref:proposed-experiment-asymmetric-variance) A possible way to contrast normalization and representational change accounts. Panel A: two exposure conditions with identical overall means and variances along VOT, but different category-specific variances. Panel B: the predictions of the best-performing normalization and representational change models. For this purpose, the normalization model was extended to both center and standardized the cues.^[We note that the specific predictions for the representational change model shown here assume that listeners have equally strong prior beliefs about the variance of both of the two categories (i.e., one $nu_{0,c}$ for both categories). This is what we assumed in our case studies---which aimed to show that even simplified versions of each change model can explain a wide range of findings in the literature---but it is not an inalienable assumption for representational change models [for discussion, see also @kleinschmidt-jaeger2015]. The general prediction of representational change models that listeners are sensitive to the category-specific variance should hold even if this assumption is removed. How easy to detect this predicted difference is, however, expected to depend on this assumption.] <!-- TO DO: Xin, any chance you could add such a figure. for this we can just perfectly center and standardize (i.e., kappa_0 and the newly introduced nu_0 are both 0); the best performing representational change model should be nu_0,c = 4 and kappa_0,c = 1024, assuming you put the category means on the a priori expected locations. -->

```{r proposed-experiment-asymmetric-variance, fig.cap="(ref:proposed-experiment-asymmetric-variance)"}
# Put figure here.
```

## Changes in decision-making vs. changes in category representations
In Section \@ref(sec:framework), we showed that the limitations of change models for decision-making are less well understood than sometimes assumed. One recommendation for future research is thus to further explore the mathematical limitations of decision-making change models. Designs that limit attentional lapses to basically zero [e.g., by employing more engaging tasks, as in gamified paradigms, @wade-holt2005; @lim-holt2011] would emphasize the computational limitations of decision-making change models. In such scenarios, changes in response biases can only explain changes that are additive in the posterior log-odds of categories (Section \@ref(sec:change-bias)). That is, changes in response biases cannot account for changes in the *slope* of categorization functions. Future studies should use exposure conditions for which such changes are predicted by representational or normalization models to test whether changes in response biases are sufficient to explain adaptive speech perception<!-- [e.g., if zero lapses had been observed in @clayards2008, which was, however, not the case]-->. Since it can be difficult to detect changes in categorization slopes---especially without making strong linearity assumptions, we suspect that this question is better explored by manipulating the relative reliability of two cues (see also Figures \@ref(fig:show-model-categorization-3D-plots-similar-accuracy) and \@ref(fig:show-model-categorization-3D-plots-best-performing)). Such manipulations are routinely used in a paradigm known as "dimension-based statistical learning" [@idemaru-holt2011; @lehet-holt2020; @liu-holt2015]. With recent trends towards analyses that more transparently link phonetic cues to changes in participants' responses [@idemaru-holt2020; @schertz-clare2020, see also Section \@ref(sec:methodological-advances)]  this field of study is in a good position to test the sufficiency of changes in decision-making accounts. An ongoing project from one of our labs, uses ASP-like simulations in combination with experimental designs that are intended to directly address this question [@burchill-jaeger2022].

Conversely, there are ways to assess whether representational changes alone are sufficient to explain all forms of adaptive speech perception. For example, if auditory input that arguably carries no information about category statistics affects subsequent speech perception, this provides evidence that changes in representations alone cannot explain all aspects of adaptive speech perception. Perhaps one of the most convincing demonstrations of this type comes from the findings of auditory enhancement effects, wherein non-speech stimuli (e.g., pure tones) can systematically alter the perception of subsequently played speech stimuli [@chodroff-wilson2020; @holt2001; @holt2005; @holt2006; @huang-holt2011; for review, see also @weatherholtz-jaeger2016]. While these demonstrations might be challenged for lack of ecological validity (potentially inviting meta-reasoning about experimenters' intentions that is unlikely to be present during everyday speech perception), it is unclear how representational changes---or, for that matter, changes in response biases---can explain such findings. At the very least then, these findings suggest that normalization *can* be involved in adaptive speech perception.

In sum, by focusing on the computational assumptions of the different change models, it is possible to conduct behavioral experiments that can decisively determine whether either of the two computationally more parsimonious change models is *sufficient* to explain adaptive speech perception, or whether changes in representations are necessary to explain adaptive speech perception. 


